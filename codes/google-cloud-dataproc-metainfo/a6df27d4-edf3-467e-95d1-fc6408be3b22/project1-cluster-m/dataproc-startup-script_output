+ run_with_logger --tag startup-script
+ local tag=
+ local pid=1178
+ [[ --tag == \-\-\t\a\g ]]
+ tag=startup-script
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'startup-script[1178]'
<13>Feb 20 20:44:16 startup-script[1178]: + cd /tmp
<13>Feb 20 20:44:16 startup-script[1178]: + trap logstacktrace ERR
<13>Feb 20 20:44:16 startup-script[1178]: + loginfo 'Starting Dataproc startup script'
<13>Feb 20 20:44:16 startup-script[1178]: + echo 'Starting Dataproc startup script'
<13>Feb 20 20:44:16 startup-script[1178]: Starting Dataproc startup script
<13>Feb 20 20:44:16 startup-script[1178]: + set -a
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_project_id
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + PROJECT=gcp-bigquery-project1
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_dataproc_region
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + REGION=europe-west3
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_zone
<13>Feb 20 20:44:16 startup-script[1178]: ++ local zone_uri
<13>Feb 20 20:44:16 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_ZONE zone
<13>Feb 20 20:44:16 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ zone_uri=projects/671384469824/zones/europe-west3-c
<13>Feb 20 20:44:16 startup-script[1178]: ++ echo europe-west3-c
<13>Feb 20 20:44:16 startup-script[1178]: + ZONE=europe-west3-c
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_bucket
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + CONFIGBUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_temp_bucket
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_TEMP_BUCKET attributes/dataproc-temp-bucket
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + TEMP_BUCKET=dataproc-temp-europe-west3-671384469824-mghpndlt
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_role
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + ROLE=Master
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_cluster_name
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_cluster_uuid
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + CLUSTER_UUID=a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_worker_count
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + WORKER_COUNT=0
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_master
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_metadata_master_additional
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:16 startup-script[1178]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:16 startup-script[1178]: + MASTER_COUNT=1
<13>Feb 20 20:44:16 startup-script[1178]: ++ hostname -s
<13>Feb 20 20:44:16 startup-script[1178]: + MY_HOSTNAME=project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: ++ hostname -f
<13>Feb 20 20:44:16 startup-script[1178]: + MY_FULL_HOSTNAME=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:16 startup-script[1178]: ++ dnsdomainname
<13>Feb 20 20:44:16 startup-script[1178]: + DOMAIN=europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:16 startup-script[1178]: + DATAPROC_MASTER_FQDN=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:16 startup-script[1178]: ++ echo project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Feb 20 20:44:16 startup-script[1178]: + PREFIX=project1-cluster
<13>Feb 20 20:44:16 startup-script[1178]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Feb 20 20:44:16 startup-script[1178]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Feb 20 20:44:16 startup-script[1178]: + UNINSTALL_TMP_DIR=/tmp/dataproc/uninstall
<13>Feb 20 20:44:16 startup-script[1178]: + UNINSTALL_PRE_ACTIVATE_TMP_DIR=/tmp/dataproc/uninstall-pre-activate
<13>Feb 20 20:44:16 startup-script[1178]: + KEYTAB_DIR=/etc/security/keytab
<13>Feb 20 20:44:16 startup-script[1178]: + CLUSTER_STAGING_FOLDER=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:16 startup-script[1178]: + CLUSTER_TEMP_FOLDER=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:16 startup-script[1178]: + COMPONENT_SERVICES=(hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server)
<13>Feb 20 20:44:16 startup-script[1178]: + readonly COMPONENT_SERVICES
<13>Feb 20 20:44:16 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:16 startup-script[1178]: + set +a
<13>Feb 20 20:44:16 startup-script[1178]: + mkdir -p /tmp/dataproc
<13>Feb 20 20:44:16 startup-script[1178]: + mkdir -p /tmp/dataproc/commands
<13>Feb 20 20:44:16 startup-script[1178]: + mkdir -p /tmp/dataproc/components
<13>Feb 20 20:44:16 startup-script[1178]: + mkdir -p /tmp/dataproc/uninstall
<13>Feb 20 20:44:16 startup-script[1178]: + mkdir -p /tmp/dataproc/uninstall-pre-activate
<13>Feb 20 20:44:16 startup-script[1178]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + local -r src=/tmp/cluster/properties/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Feb 20 20:44:16 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:16 startup-script[1178]: + cat /tmp/cluster/properties/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Feb 20 20:44:16 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Feb 20 20:44:16 startup-script[1178]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Feb 20 20:44:16 startup-script[1178]: + merge_java_properties /etc/google-dataproc/dataproc.custom.properties /etc/google-dataproc/dataproc.properties '\n# Custom image supplied properties'
<13>Feb 20 20:44:16 startup-script[1178]: + local -r src=/etc/google-dataproc/dataproc.custom.properties
<13>Feb 20 20:44:16 startup-script[1178]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + local -r 'header=\n# Custom image supplied properties'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ ! -f /etc/google-dataproc/dataproc.custom.properties ]]
<13>Feb 20 20:44:16 startup-script[1178]: + loginfo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Feb 20 20:44:16 startup-script[1178]: + echo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Feb 20 20:44:16 startup-script[1178]: Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.
<13>Feb 20 20:44:16 startup-script[1178]: + return 0
<13>Feb 20 20:44:16 startup-script[1178]: + determine_selected_components
<13>Feb 20 20:44:16 startup-script[1178]: + local -a default_components
<13>Feb 20 20:44:16 startup-script[1178]: + default_components=($(get_default_components))
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_default_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property_or_default dataproc.components.default 'hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd'
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE='jupyter knox proxy-agent'
<13>Feb 20 20:44:16 startup-script[1178]: + add_default_components hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd
<13>Feb 20 20:44:16 startup-script[1178]: + default_components=("$@")
<13>Feb 20 20:44:16 startup-script[1178]: + local default_components
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *hdfs* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent == *hdfs* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' hdfs'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected yarn
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=yarn
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *yarn* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component yarn
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=yarn
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs == *yarn* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' yarn'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected mapreduce
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=mapreduce
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *mapreduce* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component mapreduce
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=mapreduce
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn == *mapreduce* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' mapreduce'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected mysql
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=mysql
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *mysql* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component mysql
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=mysql
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce == *mysql* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' mysql'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected pig
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=pig
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *pig* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component pig
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=pig
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql == *pig* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' pig'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected tez
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=tez
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *tez* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component tez
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=tez
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig == *tez* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' tez'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected hive-metastore
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hive-metastore
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *hive-metastore* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component hive-metastore
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hive-metastore
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez == *hive-metastore* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' hive-metastore'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected hive-server2
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hive-server2
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *hive-server2* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component hive-server2
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hive-server2
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore == *hive-server2* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' hive-server2'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected spark
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=spark
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *spark* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component spark
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=spark
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 == *spark* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' spark'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected earlyoom
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=earlyoom
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *earlyoom* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component earlyoom
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=earlyoom
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark == *earlyoom* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' earlyoom'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + for component in "${default_components[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_explicitly_unselected npd
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=npd
<13>Feb 20 20:44:16 startup-script[1178]: + local deactivated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.deactivate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + deactivated_components=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == *npd* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component npd
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=npd
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom == *npd* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' npd'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:16 startup-script[1178]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:16 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:16 startup-script[1178]: + set +x
<13>Feb 20 20:44:16 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:16 startup-script[1178]: + return 0
<13>Feb 20 20:44:16 startup-script[1178]: + add_optional_component miniconda3
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=miniconda3
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd == *miniconda3* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd ]]
<13>Feb 20 20:44:16 startup-script[1178]: + OPTIONAL_COMPONENTS_VALUE+=' miniconda3'
<13>Feb 20 20:44:16 startup-script[1178]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3/g' /etc/google-dataproc/dataproc.properties
<13>Feb 20 20:44:16 startup-script[1178]: + (( 1 > 1 ))
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_selected kafka-server
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=kafka-server
<13>Feb 20 20:44:16 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kafka-server* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + local caching_enabled
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property_or_default dataproc.cluster.caching.enabled false
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + caching_enabled=false
<13>Feb 20 20:44:16 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:16 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:16 startup-script[1178]: + SELECTED_COMPONENTS=(${OPTIONAL_COMPONENTS_VALUE})
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_selected kerberos
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:16 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Feb 20 20:44:16 startup-script[1178]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Feb 20 20:44:16 startup-script[1178]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Feb 20 20:44:16 startup-script[1178]: + export_hcfs_root_uri
<13>Feb 20 20:44:16 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_property_in_xml /tmp/cluster/properties/core.xml fs.defaultFS
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + HCFS_ROOT_URI=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -z '' ]]
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_selected hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=hdfs
<13>Feb 20 20:44:16 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hdfs* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + (( 1 > 1 ))
<13>Feb 20 20:44:16 startup-script[1178]: + HCFS_ROOT_URI=hdfs://project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: + readonly HCFS_ROOT_URI
<13>Feb 20 20:44:16 startup-script[1178]: + export HCFS_ROOT_URI
<13>Feb 20 20:44:16 startup-script[1178]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:16 startup-script[1178]: + hostname=project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: + is_component_selected kerberos
<13>Feb 20 20:44:16 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:16 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:16 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:16 startup-script[1178]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Feb 20 20:44:16 startup-script[1178]: + [[ project1-cluster-m == \p\r\o\j\e\c\t\1\-\c\l\u\s\t\e\r\-\m ]]
<13>Feb 20 20:44:16 startup-script[1178]: + export MASTER_INDEX=0
<13>Feb 20 20:44:16 startup-script[1178]: + MASTER_INDEX=0
<13>Feb 20 20:44:16 startup-script[1178]: + break
<13>Feb 20 20:44:16 startup-script[1178]: + (( 0 == 0 ))
<13>Feb 20 20:44:16 startup-script[1178]: + ARTIFACTS_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES})
<13>Feb 20 20:44:16 startup-script[1178]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Feb 20 20:44:16 startup-script[1178]: + is_ubuntu
<13>Feb 20 20:44:16 startup-script[1178]: ++ os_id
<13>Feb 20 20:44:16 startup-script[1178]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:16 startup-script[1178]: ++ cut -d= -f2
<13>Feb 20 20:44:16 startup-script[1178]: ++ xargs
<13>Feb 20 20:44:16 startup-script[1178]: + [[ debian == \u\b\u\n\t\u ]]
<13>Feb 20 20:44:16 startup-script[1178]: + loginfo 'Generating helper scripts'
<13>Feb 20 20:44:16 startup-script[1178]: + echo 'Generating helper scripts'
<13>Feb 20 20:44:16 startup-script[1178]: Generating helper scripts
<13>Feb 20 20:44:16 startup-script[1178]: + cat
<13>Feb 20 20:44:16 startup-script[1178]: ++ (( i = 0 ))
<13>Feb 20 20:44:16 startup-script[1178]: ++ (( i < 1 ))
<13>Feb 20 20:44:16 startup-script[1178]: ++ echo MASTER_HOSTNAME_0=project1-cluster-m
<13>Feb 20 20:44:16 startup-script[1178]: ++ (( i++ ))
<13>Feb 20 20:44:16 startup-script[1178]: ++ (( i < 1 ))
<13>Feb 20 20:44:16 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:16 startup-script[1178]: + cat
<13>Feb 20 20:44:16 startup-script[1178]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_keys.sh /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_docker.sh /usr/local/share/google/dataproc/bdutil/configure_metadata_proxy.sh
<13>Feb 20 20:44:16 startup-script[1178]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/collectd /usr/local/share/google/dataproc/bdutil/conf/collectd_default_filtered_write.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_flink_statsd_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hdfs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hivemetastore_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hiveserver2_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_load_jmx_plugin.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_processes_default_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_shs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_default_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd
<13>Feb 20 20:44:16 startup-script[1178]: _spark_yarn_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_yarn_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /
<13>Feb 20 20:44:16 startup-script[1178]: usr/local/share/google/dataproc/bdutil/conf/otel_spark_default_metrics.yaml /usr/local/share/google/dataproc/bdutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Feb 20 20:44:16 startup-script[1178]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Feb 20 20:44:16 startup-script[1178]: + chmod +x configure_mrv2_mem.py
<13>Feb 20 20:44:16 startup-script[1178]: + loginfo 'Running helper scripts'
<13>Feb 20 20:44:16 startup-script[1178]: + echo 'Running helper scripts'
<13>Feb 20 20:44:16 startup-script[1178]: Running helper scripts
<13>Feb 20 20:44:16 startup-script[1178]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Feb 20 20:44:16 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:16 startup-script[1178]: + MOUNT_DISKS_ENABLED=
<13>Feb 20 20:44:16 startup-script[1178]: + [[ '' == \f\a\l\s\e ]]
<13>Feb 20 20:44:16 startup-script[1178]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Feb 20 20:44:16 startup-script[1178]: + cat
<13>Feb 20 20:44:16 startup-script[1178]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Feb 20 20:44:16 startup-script[1178]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Feb 20 20:44:16 startup-script[1178]: + enable_and_start_service google-dataproc-disk-mount
<13>Feb 20 20:44:16 startup-script[1178]: + local -r service=google-dataproc-disk-mount
<13>Feb 20 20:44:16 startup-script[1178]: + enable_service google-dataproc-disk-mount
<13>Feb 20 20:44:16 startup-script[1178]: + local -r service=google-dataproc-disk-mount
<13>Feb 20 20:44:16 startup-script[1178]: + local -r unit=google-dataproc-disk-mount.service
<13>Feb 20 20:44:16 startup-script[1178]: + retry_constant_short systemctl enable google-dataproc-disk-mount.service
<13>Feb 20 20:44:16 startup-script[1178]: + retry_constant_custom 30 1 systemctl enable google-dataproc-disk-mount.service
<13>Feb 20 20:44:16 startup-script[1178]: + local -r max_retry_time=30
<13>Feb 20 20:44:16 startup-script[1178]: + local -r retry_delay=1
<13>Feb 20 20:44:16 startup-script[1178]: + cmd=("${@:3}")
<13>Feb 20 20:44:16 startup-script[1178]: + local -r cmd
<13>Feb 20 20:44:16 startup-script[1178]: + local -r max_retries=30
<13>Feb 20 20:44:16 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:16 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:16 startup-script[1178]: + set +x
<13>Feb 20 20:44:16 startup-script[1178]: About to run 'systemctl enable google-dataproc-disk-mount.service' with retries...
<13>Feb 20 20:44:16 startup-script[1178]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Feb 20 20:44:16 startup-script[1178]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Feb 20 20:44:16 startup-script[1178]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Feb 20 20:44:16 startup-script[1178]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Feb 20 20:44:16 startup-script[1178]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Feb 20 20:44:17 startup-script[1178]: 'systemctl enable google-dataproc-disk-mount.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:17 startup-script[1178]: + return 0
<13>Feb 20 20:44:17 startup-script[1178]: + local -r drop_in_dir=/etc/systemd/system/google-dataproc-disk-mount.service.d
<13>Feb 20 20:44:17 startup-script[1178]: + mkdir -p /etc/systemd/system/google-dataproc-disk-mount.service.d
<13>Feb 20 20:44:17 startup-script[1178]: + local props
<13>Feb 20 20:44:17 startup-script[1178]: ++ retry_constant_short systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:17 startup-script[1178]: ++ retry_constant_custom 30 1 systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -r retry_delay=1
<13>Feb 20 20:44:17 startup-script[1178]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -r cmd
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -r max_retries=30
<13>Feb 20 20:44:17 startup-script[1178]: ++ local reenable_x=false
<13>Feb 20 20:44:17 startup-script[1178]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:17 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:17 startup-script[1178]: About to run 'systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:17 startup-script[1178]: 'systemctl show google-dataproc-disk-mount.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:17 startup-script[1178]: ++ return 0
<13>Feb 20 20:44:17 startup-script[1178]: + props='Restart=no
<13>Feb 20 20:44:17 startup-script[1178]: RemainAfterExit=yes'
<13>Feb 20 20:44:17 startup-script[1178]: + [[ google-dataproc-disk-mount != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:17 startup-script[1178]: + [[ google-dataproc-disk-mount != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:17 startup-script[1178]: + [[ Restart=no
<13>Feb 20 20:44:17 startup-script[1178]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:17 startup-script[1178]: + [[ Restart=no
<13>Feb 20 20:44:17 startup-script[1178]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:17 startup-script[1178]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:17 startup-script[1178]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:17 startup-script[1178]: ++ dirname /etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:17 startup-script[1178]: + mkdir -p /etc/systemd/system/common
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + [[ google-dataproc-disk-mount == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:17 startup-script[1178]: + [[ google-dataproc-disk-mount == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:17 startup-script[1178]: + start_service google-dataproc-disk-mount
<13>Feb 20 20:44:17 startup-script[1178]: + local -r service=google-dataproc-disk-mount
<13>Feb 20 20:44:17 startup-script[1178]: + local -r unit=google-dataproc-disk-mount.service
<13>Feb 20 20:44:17 startup-script[1178]: + retry_constant_short systemctl start google-dataproc-disk-mount.service
<13>Feb 20 20:44:17 startup-script[1178]: + retry_constant_custom 30 1 systemctl start google-dataproc-disk-mount.service
<13>Feb 20 20:44:17 startup-script[1178]: + local -r max_retry_time=30
<13>Feb 20 20:44:17 startup-script[1178]: + local -r retry_delay=1
<13>Feb 20 20:44:17 startup-script[1178]: + cmd=("${@:3}")
<13>Feb 20 20:44:17 startup-script[1178]: + local -r cmd
<13>Feb 20 20:44:17 startup-script[1178]: + local -r max_retries=30
<13>Feb 20 20:44:17 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:17 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:17 startup-script[1178]: + set +x
<13>Feb 20 20:44:17 startup-script[1178]: About to run 'systemctl start google-dataproc-disk-mount.service' with retries...
<13>Feb 20 20:44:17 startup-script[1178]: 'systemctl start google-dataproc-disk-mount.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:17 startup-script[1178]: + return 0
<13>Feb 20 20:44:17 startup-script[1178]: + bash -e configuration_script.sh
<13>Feb 20 20:44:17 startup-script[1178]: + readonly METADATA_HOST=metadata.google.internal
<13>Feb 20 20:44:17 startup-script[1178]: + METADATA_HOST=metadata.google.internal
<13>Feb 20 20:44:17 startup-script[1178]: + readonly INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Feb 20 20:44:17 startup-script[1178]: + INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Feb 20 20:44:17 startup-script[1178]: + readonly IDENTITY_KEY=service-accounts/default/identity
<13>Feb 20 20:44:17 startup-script[1178]: + IDENTITY_KEY=service-accounts/default/identity
<13>Feb 20 20:44:17 startup-script[1178]: + readonly GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Feb 20 20:44:17 startup-script[1178]: + GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Feb 20 20:44:17 startup-script[1178]: + readonly SERIAL_DEVICE=/dev/ttyS3
<13>Feb 20 20:44:17 startup-script[1178]: + SERIAL_DEVICE=/dev/ttyS3
<13>Feb 20 20:44:17 startup-script[1178]: + readonly GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Feb 20 20:44:17 startup-script[1178]: + GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Feb 20 20:44:17 startup-script[1178]: + readonly TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Feb 20 20:44:17 startup-script[1178]: + TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Feb 20 20:44:17 startup-script[1178]: ++ get_dataproc_property dataproc.encryption.keygen.enabled
<13>Feb 20 20:44:17 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:17 startup-script[1178]: + cluster_keys_enabled=
<13>Feb 20 20:44:17 startup-script[1178]: ++ get_dataproc_property_or_default dataproc.encryption.keygen.rotation_hours 6
<13>Feb 20 20:44:17 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:17 startup-script[1178]: + cluster_keys_rotation_hours=6
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + chmod u+rwx /usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:17 startup-script[1178]: + rm /etc/udev/rules.d/80-ttyS3.rules
<13>Feb 20 20:44:17 startup-script[1178]: + udevadm trigger
<13>Feb 20 20:44:17 startup-script[1178]: + chown root:dialout /dev/ttyS3
<13>Feb 20 20:44:17 startup-script[1178]: + set -e
<13>Feb 20 20:44:17 startup-script[1178]: + loginfo 'Running configure_hadoop.sh'
<13>Feb 20 20:44:17 startup-script[1178]: + echo 'Running configure_hadoop.sh'
<13>Feb 20 20:44:17 startup-script[1178]: Running configure_hadoop.sh
<13>Feb 20 20:44:17 startup-script[1178]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Feb 20 20:44:17 startup-script[1178]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Feb 20 20:44:17 startup-script[1178]: + mkdir -p /hadoop/tmp
<13>Feb 20 20:44:17 startup-script[1178]: + export DEFAULT_NUM_MAPS=100
<13>Feb 20 20:44:17 startup-script[1178]: + DEFAULT_NUM_MAPS=100
<13>Feb 20 20:44:17 startup-script[1178]: + export DEFAULT_NUM_REDUCES=40
<13>Feb 20 20:44:17 startup-script[1178]: + DEFAULT_NUM_REDUCES=40
<13>Feb 20 20:44:17 startup-script[1178]: ++ grep -c processor /proc/cpuinfo
<13>Feb 20 20:44:17 startup-script[1178]: + NUM_CORES=4
<13>Feb 20 20:44:17 startup-script[1178]: + export NUM_CORES
<13>Feb 20 20:44:17 startup-script[1178]: ++ python -c 'print(int(4 // 1.0))'
<13>Feb 20 20:44:17 startup-script[1178]: + MAP_SLOTS=4
<13>Feb 20 20:44:17 startup-script[1178]: + export MAP_SLOTS
<13>Feb 20 20:44:17 startup-script[1178]: ++ python -c 'print(int(4 // 2.0))'
<13>Feb 20 20:44:17 startup-script[1178]: + REDUCE_SLOTS=2
<13>Feb 20 20:44:17 startup-script[1178]: + export REDUCE_SLOTS
<13>Feb 20 20:44:17 startup-script[1178]: ++ free -m
<13>Feb 20 20:44:17 startup-script[1178]: ++ awk '/^Mem:/{print $2}'
<13>Feb 20 20:44:17 startup-script[1178]: + TOTAL_MEM_MB=16008
<13>Feb 20 20:44:17 startup-script[1178]: ++ python -c 'print(int(16008 * 0.4))'
<13>Feb 20 20:44:17 startup-script[1178]: + HADOOP_MR_MASTER_MEM_MB=6403
<13>Feb 20 20:44:17 startup-script[1178]: + [[ -x configure_mrv2_mem.py ]]
<13>Feb 20 20:44:17 startup-script[1178]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Feb 20 20:44:17 startup-script[1178]: + TEMP_ENV_FILE=/tmp/mrv2_oy6_tmp_env.sh
<13>Feb 20 20:44:17 startup-script[1178]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_oy6_tmp_env.sh --total_memory 16008 --available_memory_ratio 0.8 --total_cores 4 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Feb 20 20:44:17 startup-script[1178]: + source /tmp/mrv2_oy6_tmp_env.sh
<13>Feb 20 20:44:17 startup-script[1178]: ++ export YARN_MIN_MEM_MB=1024
<13>Feb 20 20:44:17 startup-script[1178]: ++ YARN_MIN_MEM_MB=1024
<13>Feb 20 20:44:17 startup-script[1178]: ++ export YARN_MAX_MEM_MB=12288
<13>Feb 20 20:44:17 startup-script[1178]: ++ YARN_MAX_MEM_MB=12288
<13>Feb 20 20:44:17 startup-script[1178]: ++ export NODEMANAGER_MEM_MB=12288
<13>Feb 20 20:44:17 startup-script[1178]: ++ NODEMANAGER_MEM_MB=12288
<13>Feb 20 20:44:17 startup-script[1178]: ++ export APP_MASTER_MEM_MB=6144
<13>Feb 20 20:44:17 startup-script[1178]: ++ APP_MASTER_MEM_MB=6144
<13>Feb 20 20:44:17 startup-script[1178]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Feb 20 20:44:17 startup-script[1178]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Feb 20 20:44:17 startup-script[1178]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4915m
<13>Feb 20 20:44:17 startup-script[1178]: ++ APP_MASTER_JAVA_OPTS=-Xmx4915m
<13>Feb 20 20:44:17 startup-script[1178]: ++ export MAP_MEM_MB=3072
<13>Feb 20 20:44:17 startup-script[1178]: ++ MAP_MEM_MB=3072
<13>Feb 20 20:44:17 startup-script[1178]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Feb 20 20:44:17 startup-script[1178]: ++ CORES_PER_MAP_ROUNDED=1
<13>Feb 20 20:44:17 startup-script[1178]: ++ export MAP_JAVA_OPTS=-Xmx2457m
<13>Feb 20 20:44:17 startup-script[1178]: ++ MAP_JAVA_OPTS=-Xmx2457m
<13>Feb 20 20:44:17 startup-script[1178]: ++ export REDUCE_MEM_MB=6144
<13>Feb 20 20:44:17 startup-script[1178]: ++ REDUCE_MEM_MB=6144
<13>Feb 20 20:44:17 startup-script[1178]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Feb 20 20:44:17 startup-script[1178]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Feb 20 20:44:17 startup-script[1178]: ++ export REDUCE_JAVA_OPTS=-Xmx4915m
<13>Feb 20 20:44:17 startup-script[1178]: ++ REDUCE_JAVA_OPTS=-Xmx4915m
<13>Feb 20 20:44:17 startup-script[1178]: ++ python -c 'print(min(32 * 1024, int(16008 / 4)))'
<13>Feb 20 20:44:17 startup-script[1178]: + HADOOP_CLIENT_MEM_MB=4002
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:17 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:17 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:17 startup-script[1178]: + set +x
<13>Feb 20 20:44:17 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:17 startup-script[1178]: + return 1
<13>Feb 20 20:44:17 startup-script[1178]: + GC_LOG_OPTS='-XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails'
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Feb 20 20:44:17 startup-script[1178]: ++ get_data_dirs
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -a mount_points
<13>Feb 20 20:44:17 startup-script[1178]: ++ mapfile -t mount_points
<13>Feb 20 20:44:17 startup-script[1178]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Feb 20 20:44:17 startup-script[1178]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Feb 20 20:44:17 startup-script[1178]: +++ true
<13>Feb 20 20:44:17 startup-script[1178]: ++ (( 0 ))
<13>Feb 20 20:44:17 startup-script[1178]: ++ echo /
<13>Feb 20 20:44:17 startup-script[1178]: ++ return
<13>Feb 20 20:44:17 startup-script[1178]: + MAPRED_DIRS=/hadoop/mapred
<13>Feb 20 20:44:17 startup-script[1178]: + MAPRED_DIRS_ARRAY=(${MAPRED_DIRS})
<13>Feb 20 20:44:17 startup-script[1178]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Feb 20 20:44:17 startup-script[1178]: + MAPRED_LOCAL_DIRS_ARRAY=(${MAPRED_LOCAL_DIRS})
<13>Feb 20 20:44:17 startup-script[1178]: + YARN_DIRS=/hadoop/yarn
<13>Feb 20 20:44:17 startup-script[1178]: + YARN_DIRS_ARRAY=(${YARN_DIRS})
<13>Feb 20 20:44:17 startup-script[1178]: ++ get_yarn_nm_local_dirs
<13>Feb 20 20:44:17 startup-script[1178]: ++ data_dirs=($(get_data_dirs))
<13>Feb 20 20:44:17 startup-script[1178]: +++ get_data_dirs
<13>Feb 20 20:44:17 startup-script[1178]: +++ local -a mount_points
<13>Feb 20 20:44:17 startup-script[1178]: +++ mapfile -t mount_points
<13>Feb 20 20:44:17 startup-script[1178]: ++++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Feb 20 20:44:17 startup-script[1178]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Feb 20 20:44:17 startup-script[1178]: ++++ true
<13>Feb 20 20:44:17 startup-script[1178]: +++ (( 0 ))
<13>Feb 20 20:44:17 startup-script[1178]: +++ echo /
<13>Feb 20 20:44:17 startup-script[1178]: +++ return
<13>Feb 20 20:44:17 startup-script[1178]: ++ local -r data_dirs
<13>Feb 20 20:44:17 startup-script[1178]: ++ echo /hadoop/yarn/nm-local-dir
<13>Feb 20 20:44:17 startup-script[1178]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Feb 20 20:44:17 startup-script[1178]: + NODEMANAGER_LOCAL_DIRS_ARRAY=(${NODEMANAGER_LOCAL_DIRS})
<13>Feb 20 20:44:17 startup-script[1178]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Feb 20 20:44:17 startup-script[1178]: + chgrp hadoop -L -R /hadoop /hadoop/tmp
<13>Feb 20 20:44:17 startup-script[1178]: + chown -L -R mapred:hadoop /hadoop/mapred
<13>Feb 20 20:44:17 startup-script[1178]: + chown -L -R yarn:hadoop /hadoop/yarn
<13>Feb 20 20:44:17 startup-script[1178]: + chmod g+rwx -R /hadoop /hadoop/mapred /hadoop/yarn
<13>Feb 20 20:44:17 startup-script[1178]: + chmod 777 -R /hadoop/tmp
<13>Feb 20 20:44:17 startup-script[1178]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Feb 20 20:44:17 startup-script[1178]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Feb 20 20:44:17 startup-script[1178]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Feb 20 20:44:17 startup-script[1178]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Feb 20 20:44:17 startup-script[1178]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Feb 20 20:44:17 startup-script[1178]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Feb 20 20:44:17 startup-script[1178]: + cat
<13>Feb 20 20:44:17 startup-script[1178]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:17 startup-script[1178]: + readonly CORE_TEMPLATE=core-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + CORE_TEMPLATE=core-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + readonly YARN_TEMPLATE=yarn-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + YARN_TEMPLATE=yarn-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + merge_hadoop_configurations core-site.xml core-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r config=core-site.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r template=core-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:17 startup-script[1178]: + merge_hadoop_configurations mapred-site.xml mapred-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r config=mapred-site.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r template=mapred-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:17 startup-script[1178]: + merge_hadoop_configurations yarn-site.xml yarn-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r template=yarn-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:17 startup-script[1178]: + merge_hadoop_configurations capacity-scheduler.xml capacity-scheduler-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r config=capacity-scheduler.xml
<13>Feb 20 20:44:17 startup-script[1178]: + local -r template=capacity-scheduler-template.xml
<13>Feb 20 20:44:17 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + merge_hadoop_configurations distcp-default.xml distcp-template.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=distcp-default.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r template=distcp-template.xml
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property mapred-site.xml mapreduce.application.classpath '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*'
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=mapred-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=mapreduce.application.classpath
<13>Feb 20 20:44:18 startup-script[1178]: + local -r 'property_value=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*'
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.application.classpath --value '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property mapred-site.xml mapreduce.client.submit.file.replication 2
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=mapred-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=mapreduce.client.submit.file.replication
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=2
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.client.submit.file.replication --value 2 --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.application.classpath '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*'
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=yarn.application.classpath
<13>Feb 20 20:44:18 startup-script[1178]: + local -r 'property_value=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*'
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.application.classpath --value '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Feb 20 20:44:18 startup-script[1178]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Feb 20 20:44:18 startup-script[1178]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:18 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:18 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:18 startup-script[1178]: + set +x
<13>Feb 20 20:44:18 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:18 startup-script[1178]: + return 0
<13>Feb 20 20:44:18 startup-script[1178]: + cat
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.nodemanager.env-whitelist PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=yarn.nodemanager.env-whitelist
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.env-whitelist --value PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.nm.liveness-monitor.expiry-interval-ms 120000
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=yarn.nm.liveness-monitor.expiry-interval-ms
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=120000
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nm.liveness-monitor.expiry-interval-ms --value 120000 --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.log-aggregation-enable false
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=yarn.log-aggregation-enable
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=false
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + ZK_QUORUM=project1-cluster-m:2181,:2181,:2181
<13>Feb 20 20:44:18 startup-script[1178]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property hdfs-site.xml dfs.namenode.file.close.num-committed-allowed 1
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=hdfs-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=dfs.namenode.file.close.num-committed-allowed
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=1
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Feb 20 20:44:18 startup-script[1178]: + set_hadoop_property core-site.xml hadoop.http.filter.initializers org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Feb 20 20:44:18 startup-script[1178]: + local -r config=core-site.xml
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_name=hadoop.http.filter.initializers
<13>Feb 20 20:44:18 startup-script[1178]: + local -r property_value=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Feb 20 20:44:18 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.webapp.cross-origin.enabled true
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.resourcemanager.webapp.cross-origin.enabled
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=true
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.timeline-service.http-cross-origin.enabled true
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.timeline-service.http-cross-origin.enabled
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=true
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.timeline-service.enabled true
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.timeline-service.enabled
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=true
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.timeline-service.hostname project1-cluster-m
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.timeline-service.hostname
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=project1-cluster-m
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value project1-cluster-m --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.timeline-service.bind-host 0.0.0.0
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.timeline-service.bind-host
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=0.0.0.0
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.system-metrics-publisher.enabled true
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.resourcemanager.system-metrics-publisher.enabled
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=true
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.timeline-service.generic-application-history.enabled true
<13>Feb 20 20:44:19 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_name=yarn.timeline-service.generic-application-history.enabled
<13>Feb 20 20:44:19 startup-script[1178]: + local -r property_value=true
<13>Feb 20 20:44:19 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Feb 20 20:44:19 startup-script[1178]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:19 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:19 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:19 startup-script[1178]: + set +x
<13>Feb 20 20:44:19 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:19 startup-script[1178]: + return 1
<13>Feb 20 20:44:19 startup-script[1178]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:19 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:19 startup-script[1178]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Feb 20 20:44:19 startup-script[1178]: ++ get_metadata_datanode_enabled
<13>Feb 20 20:44:19 startup-script[1178]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Feb 20 20:44:19 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:19 startup-script[1178]: + DATAPROC_DATANODE_ENABLED=true
<13>Feb 20 20:44:19 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:19 startup-script[1178]: ++ create_or_validate_include_file_path
<13>Feb 20 20:44:19 startup-script[1178]: ++ local include_path
<13>Feb 20 20:44:19 startup-script[1178]: +++ get_include_file_path
<13>Feb 20 20:44:19 startup-script[1178]: +++ local include_path
<13>Feb 20 20:44:19 startup-script[1178]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Feb 20 20:44:19 startup-script[1178]: +++ include_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: +++ [[ -z gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include ]]
<13>Feb 20 20:44:19 startup-script[1178]: +++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: ++ include_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: ++ [[ gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include == gs://* ]]
<13>Feb 20 20:44:19 startup-script[1178]: ++ loginfo 'Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include'
<13>Feb 20 20:44:19 startup-script[1178]: ++ echo 'Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include'
<13>Feb 20 20:44:19 startup-script[1178]: Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: ++ gcs_file_exists gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: ++ local -r file=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:19 startup-script[1178]: +++ gsutil -q stat gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:22 startup-script[1178]: ++ result=
<13>Feb 20 20:44:22 startup-script[1178]: ++ [[ '' != 1 ]]
<13>Feb 20 20:44:22 startup-script[1178]: ++ return 0
<13>Feb 20 20:44:22 startup-script[1178]: ++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:22 startup-script[1178]: + INCLUDE_PATH=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:22 startup-script[1178]: ++ create_or_validate_exclude_file_path
<13>Feb 20 20:44:22 startup-script[1178]: ++ local exclude_path
<13>Feb 20 20:44:22 startup-script[1178]: +++ get_exclude_file_path
<13>Feb 20 20:44:22 startup-script[1178]: +++ local exclude_path
<13>Feb 20 20:44:22 startup-script[1178]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Feb 20 20:44:22 startup-script[1178]: +++ exclude_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: +++ [[ -z gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml ]]
<13>Feb 20 20:44:22 startup-script[1178]: +++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: ++ exclude_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: ++ [[ gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml == gs://* ]]
<13>Feb 20 20:44:22 startup-script[1178]: ++ loginfo 'Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml'
<13>Feb 20 20:44:22 startup-script[1178]: ++ echo 'Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml'
<13>Feb 20 20:44:22 startup-script[1178]: Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: ++ gcs_file_exists gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: ++ local -r file=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:22 startup-script[1178]: +++ gsutil -q stat gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:23 startup-script[1178]: ++ result=
<13>Feb 20 20:44:23 startup-script[1178]: ++ [[ '' != 1 ]]
<13>Feb 20 20:44:23 startup-script[1178]: ++ return 0
<13>Feb 20 20:44:23 startup-script[1178]: ++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:23 startup-script[1178]: + EXCLUDE_PATH=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:23 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.include-path gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:23 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:23 startup-script[1178]: + local -r property_name=yarn.resourcemanager.nodes.include-path
<13>Feb 20 20:44:23 startup-script[1178]: + local -r property_value=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:23 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.include-path --value gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include --clobber
<13>Feb 20 20:44:23 startup-script[1178]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.exclude-path gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:23 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:23 startup-script[1178]: + local -r property_name=yarn.resourcemanager.nodes.exclude-path
<13>Feb 20 20:44:23 startup-script[1178]: + local -r property_value=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:23 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.exclude-path --value gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml --clobber
<13>Feb 20 20:44:23 startup-script[1178]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:23 startup-script[1178]: + MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Feb 20 20:44:23 startup-script[1178]: + [[ LOCAL == \Y\A\R\N ]]
<13>Feb 20 20:44:23 startup-script[1178]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:23 startup-script[1178]: + readonly YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Feb 20 20:44:23 startup-script[1178]: + YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Feb 20 20:44:23 startup-script[1178]: + merge_hadoop_configurations yarn-site.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Feb 20 20:44:23 startup-script[1178]: + local -r config=yarn-site.xml
<13>Feb 20 20:44:23 startup-script[1178]: + local -r template=/usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Feb 20 20:44:23 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:23 startup-script[1178]: + is_component_selected kerberos
<13>Feb 20 20:44:23 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:23 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:23 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:23 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:23 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_dataproc_property yarn.docker.enable
<13>Feb 20 20:44:23 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:23 startup-script[1178]: + ENABLE_DOCKER_YARN=
<13>Feb 20 20:44:23 startup-script[1178]: + [[ -z '' ]]
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_dataproc_property docker.yarn.enable
<13>Feb 20 20:44:23 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:23 startup-script[1178]: + ENABLE_DOCKER_YARN=
<13>Feb 20 20:44:23 startup-script[1178]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:23 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:23 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:23 startup-script[1178]: + set +x
<13>Feb 20 20:44:23 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:23 startup-script[1178]: + return 0
<13>Feb 20 20:44:23 startup-script[1178]: + is_component_selected docker-ce
<13>Feb 20 20:44:23 startup-script[1178]: + local -r component=docker-ce
<13>Feb 20 20:44:23 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:23 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:23 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:23 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *docker-ce* ]]
<13>Feb 20 20:44:23 startup-script[1178]: + set -e
<13>Feb 20 20:44:23 startup-script[1178]: + loginfo 'Running configure_connectors.sh'
<13>Feb 20 20:44:23 startup-script[1178]: + echo 'Running configure_connectors.sh'
<13>Feb 20 20:44:23 startup-script[1178]: Running configure_connectors.sh
<13>Feb 20 20:44:23 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Feb 20 20:44:23 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Feb 20 20:44:23 startup-script[1178]: + set -euxo pipefail
<13>Feb 20 20:44:23 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh
<13>Feb 20 20:44:23 startup-script[1178]: ++ readonly DOCKER_PATH=/var/lib/docker
<13>Feb 20 20:44:23 startup-script[1178]: ++ DOCKER_PATH=/var/lib/docker
<13>Feb 20 20:44:23 startup-script[1178]: ++ readonly GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Feb 20 20:44:23 startup-script[1178]: ++ GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Feb 20 20:44:23 startup-script[1178]: + is_component_selected docker-ce
<13>Feb 20 20:44:23 startup-script[1178]: + local -r component=docker-ce
<13>Feb 20 20:44:23 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:23 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:23 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:23 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:23 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *docker-ce* ]]
<13>Feb 20 20:44:23 startup-script[1178]: + readonly RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Feb 20 20:44:23 startup-script[1178]: + RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Feb 20 20:44:23 startup-script[1178]: + readonly POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Feb 20 20:44:23 startup-script[1178]: + POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Feb 20 20:44:23 startup-script[1178]: + readonly PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Feb 20 20:44:23 startup-script[1178]: + PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Feb 20 20:44:23 startup-script[1178]: + readonly POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Feb 20 20:44:23 startup-script[1178]: + POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Feb 20 20:44:23 startup-script[1178]: + readonly METADATA_ADDRESS=169.254.169.254
<13>Feb 20 20:44:23 startup-script[1178]: + METADATA_ADDRESS=169.254.169.254
<13>Feb 20 20:44:23 startup-script[1178]: + readonly METADATA_PORT=80
<13>Feb 20 20:44:23 startup-script[1178]: + METADATA_PORT=80
<13>Feb 20 20:44:23 startup-script[1178]: + readonly METADATA_PASSTHROUGH_PORT=987
<13>Feb 20 20:44:23 startup-script[1178]: + METADATA_PASSTHROUGH_PORT=987
<13>Feb 20 20:44:23 startup-script[1178]: + readonly METADATA_PROXY_PORT=988
<13>Feb 20 20:44:23 startup-script[1178]: + METADATA_PROXY_PORT=988
<13>Feb 20 20:44:23 startup-script[1178]: + readonly 'TOKEN_SOURCE_METADATA=TOKEN FROM METADATA ATTRIBUTES'
<13>Feb 20 20:44:23 startup-script[1178]: + TOKEN_SOURCE_METADATA='TOKEN FROM METADATA ATTRIBUTES'
<13>Feb 20 20:44:23 startup-script[1178]: + readonly 'TOKEN_SOURCE_GCS=TOKEN FROM GCS'
<13>Feb 20 20:44:23 startup-script[1178]: + TOKEN_SOURCE_GCS='TOKEN FROM GCS'
<13>Feb 20 20:44:23 startup-script[1178]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Feb 20 20:44:23 startup-script[1178]: + CONFIGBUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:23 startup-script[1178]: + readonly CONFIGBUCKET
<13>Feb 20 20:44:23 startup-script[1178]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Feb 20 20:44:23 startup-script[1178]: + CLUSTER_UUID=a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:23 startup-script[1178]: + readonly CLUSTER_UUID
<13>Feb 20 20:44:23 startup-script[1178]: ++ /usr/share/google/get_metadata_value id
<13>Feb 20 20:44:24 startup-script[1178]: + INSTANCE_UUID=5934482648196493799
<13>Feb 20 20:44:24 startup-script[1178]: + readonly INSTANCE_UUID
<13>Feb 20 20:44:24 startup-script[1178]: + readonly TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Feb 20 20:44:24 startup-script[1178]: + TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.metadata.proxy.enabled
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + METADATA_PROXY_ENABLED=
<13>Feb 20 20:44:24 startup-script[1178]: + readonly METADATA_PROXY_ENABLED
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.exclusive.user
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + DATAPROC_EXCLUSIVE_USER=
<13>Feb 20 20:44:24 startup-script[1178]: + readonly DATAPROC_EXCLUSIVE_USER
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.personal-auth.override-user-display-name
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME=
<13>Feb 20 20:44:24 startup-script[1178]: + readonly PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME
<13>Feb 20 20:44:24 startup-script[1178]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:24 startup-script[1178]: + WORKER_COUNT=0
<13>Feb 20 20:44:24 startup-script[1178]: + is_component_selected yarn
<13>Feb 20 20:44:24 startup-script[1178]: + local -r component=yarn
<13>Feb 20 20:44:24 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:24 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:24 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *yarn* ]]
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property_or_default yarn.log-aggregation.enabled false
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + LOG_AGGREGATION_ENABLED=true
<13>Feb 20 20:44:24 startup-script[1178]: + readonly LOG_AGGREGATION_ENABLED
<13>Feb 20 20:44:24 startup-script[1178]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + loginfo 'Enabling YARN log aggregation.'
<13>Feb 20 20:44:24 startup-script[1178]: + echo 'Enabling YARN log aggregation.'
<13>Feb 20 20:44:24 startup-script[1178]: Enabling YARN log aggregation.
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_yarn_site yarn.log-aggregation-enable true
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation-enable
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=true
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation-enable true
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation-enable
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=true
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=true
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value true --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + is_component_selected kerberos
<13>Feb 20 20:44:24 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:24 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:24 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:24 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:24 startup-script[1178]: + readonly YARN_LOG_SERVER_SCHEMA=http
<13>Feb 20 20:44:24 startup-script[1178]: + YARN_LOG_SERVER_SCHEMA=http
<13>Feb 20 20:44:24 startup-script[1178]: + readonly YARN_LOG_SERVER_PORT=19888
<13>Feb 20 20:44:24 startup-script[1178]: + YARN_LOG_SERVER_PORT=19888
<13>Feb 20 20:44:24 startup-script[1178]: + readonly YARN_LOG_SERVER_URL=http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + YARN_LOG_SERVER_URL=http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_yarn_site yarn.log.server.url http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log.server.url
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log.server.url http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log.server.url
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=http://project1-cluster-m:19888/jobhistory/logs
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log.server.url --value http://project1-cluster-m:19888/jobhistory/logs --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + [[ -n dataproc-temp-europe-west3-671384469824-mghpndlt ]]
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_yarn_site yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.remote-app-log-dir --value gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/yarn-logs --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:24 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:24 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:24 startup-script[1178]: + set +x
<13>Feb 20 20:44:24 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:24 startup-script[1178]: + return 0
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_yarn_site yarn.log-aggregation.file-formats IFile,TFile
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation.file-formats
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=IFile,TFile
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-formats IFile,TFile
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation.file-formats
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=IFile,TFile
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=IFile,TFile
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-formats --value IFile,TFile --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_yarn_site yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-controller.IFile.class --value org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + PERSIST_HISTORY_TO_GCS=true
<13>Feb 20 20:44:24 startup-script[1178]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + loginfo 'Enabling persisting MapReduce job history files to GCS.'
<13>Feb 20 20:44:24 startup-script[1178]: + echo 'Enabling persisting MapReduce job history files to GCS.'
<13>Feb 20 20:44:24 startup-script[1178]: Enabling persisting MapReduce job history files to GCS.
<13>Feb 20 20:44:24 startup-script[1178]: + [[ -n dataproc-temp-europe-west3-671384469824-mghpndlt ]]
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_mapred_site mapreduce.jobhistory.done-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.done-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.done-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.done-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.done-dir --value gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_mapred_site mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.intermediate-done-dir --value gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/mapreduce-job-history/done_intermediate --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_mapred_site mapreduce.jobhistory.move.interval-ms 1000
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=1000
<13>Feb 20 20:44:24 startup-script[1178]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.move.interval-ms 1000
<13>Feb 20 20:44:24 startup-script[1178]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Feb 20 20:44:24 startup-script[1178]: + local -r value=1000
<13>Feb 20 20:44:24 startup-script[1178]: + local -r mode=overwrite
<13>Feb 20 20:44:24 startup-script[1178]: + local -r delimiter=,
<13>Feb 20 20:44:24 startup-script[1178]: + local skip=false
<13>Feb 20 20:44:24 startup-script[1178]: + local old_value
<13>Feb 20 20:44:24 startup-script[1178]: + local new_value
<13>Feb 20 20:44:24 startup-script[1178]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + new_value=1000
<13>Feb 20 20:44:24 startup-script[1178]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.move.interval-ms --value 1000 --create_if_absent --clobber
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.users
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: + DATAPROC_USERS=
<13>Feb 20 20:44:24 startup-script[1178]: + [[ -n '' ]]
<13>Feb 20 20:44:24 startup-script[1178]: + is_component_selected kerberos
<13>Feb 20 20:44:24 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:24 startup-script[1178]: + local activated_components
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_components_to_activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:24 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:24 startup-script[1178]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:24 startup-script[1178]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:24 startup-script[1178]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:24 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:24 startup-script[1178]: + loginfo 'Merging user-specified cluster properties'
<13>Feb 20 20:44:24 startup-script[1178]: + echo 'Merging user-specified cluster properties'
<13>Feb 20 20:44:24 startup-script[1178]: Merging user-specified cluster properties
<13>Feb 20 20:44:24 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Feb 20 20:44:24 startup-script[1178]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Feb 20 20:44:24 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Feb 20 20:44:24 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/core.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/core.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/distcp.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/distcp.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/mapred.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/mapred.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/yarn.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/yarn.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_java_properties /tmp/cluster/properties/hadoop-log4j.properties /etc/hadoop/conf/log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r src=/tmp/cluster/properties/hadoop-log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r dest=/etc/hadoop/conf/log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/hadoop-log4j.properties ]]
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/cluster/properties/hadoop-log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/hadoop-log4j.properties.
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + merge_xml_properties /tmp/cluster/properties/hbase.xml /etc/hbase/conf/hbase-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local src=/tmp/cluster/properties/hbase.xml
<13>Feb 20 20:44:25 startup-script[1178]: + local dest=/etc/hbase/conf/hbase-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/hbase.xml ]]
<13>Feb 20 20:44:25 startup-script[1178]: + bdconfig merge_configurations --configuration_file /etc/hbase/conf/hbase-site.xml --source_configuration_file /tmp/cluster/properties/hbase.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/hbase.xml.
<13>Feb 20 20:44:25 startup-script[1178]: + merge_java_properties /tmp/cluster/properties/hbase-log4j.properties /etc/hbase/conf/log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r src=/tmp/cluster/properties/hbase-log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r dest=/etc/hbase/conf/log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /tmp/cluster/properties/hbase-log4j.properties ]]
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/cluster/properties/hbase-log4j.properties
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Feb 20 20:44:25 startup-script[1178]: Merged /tmp/cluster/properties/hbase-log4j.properties.
<13>Feb 20 20:44:25 startup-script[1178]: + DATAPROC_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'DATAPROC_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server'
<13>Feb 20 20:44:25 startup-script[1178]: DATAPROC_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + ARTIFACTS_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'ARTIFACTS_TO_KEEP: dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-datanode hadoop-yarn-nodemanager autofs bash-completion bc chrony git jq vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid earlyoom flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j npd pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas-dev netcat python-numpy python-pip python-reque
<13>Feb 20 20:44:25 startup-script[1178]: sts python-setuptools linux-kbuild-5.10'
<13>Feb 20 20:44:25 startup-script[1178]: ARTIFACTS_TO_KEEP: dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-datanode hadoop-yarn-nodemanager autofs bash-completion bc chrony git jq vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid earlyoom flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j npd pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas-dev netcat python-numpy python-pip python-requests pyth
<13>Feb 20 20:44:25 startup-script[1178]: on-setuptools linux-kbuild-5.10
<13>Feb 20 20:44:25 startup-script[1178]: + COMPONENTS_TO_ACTIVATE=($(intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP))
<13>Feb 20 20:44:25 startup-script[1178]: ++ intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n values=SELECTED_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n filter=ARTIFACTS_TO_KEEP
<13>Feb 20 20:44:25 startup-script[1178]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-datanode hadoop-yarn-nodemanager autofs bash-completion bc chrony git jq vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid earlyoom flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j npd pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libsnappy-dev libssl-dev libzstd-dev openssl uuid-runtime linux-headers-cloud-amd64 linux-image-cloud-amd64 libopenblas-dev netcat python-numpy python-pip python-requests pytho
<13>Feb 20 20:44:25 startup-script[1178]: n-setuptools linux-kbuild-5.10
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'COMPONENTS_TO_ACTIVATE: earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn'
<13>Feb 20 20:44:25 startup-script[1178]: COMPONENTS_TO_ACTIVATE: earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Generating post_hdfs_env.sh'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Generating post_hdfs_env.sh'
<13>Feb 20 20:44:25 startup-script[1178]: Generating post_hdfs_env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + cat
<13>Feb 20 20:44:25 startup-script[1178]: + chmod +x /usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Feb 20 20:44:25 startup-script[1178]: + NON_ACTIVATED_COMPONENTS=($(difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Feb 20 20:44:25 startup-script[1178]: ++ difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Feb 20 20:44:25 startup-script[1178]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'NON_ACTIVATED_COMPONENTS: docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server'
<13>Feb 20 20:44:25 startup-script[1178]: NON_ACTIVATED_COMPONENTS: docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + ARTIFACTS_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL))
<13>Feb 20 20:44:25 startup-script[1178]: ++ difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n filter=ARTIFACTS_TO_UNINSTALL
<13>Feb 20 20:44:25 startup-script[1178]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server'
<13>Feb 20 20:44:25 startup-script[1178]: ARTIFACTS_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + COMPONENTS_TO_UNINSTALL=($(intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Feb 20 20:44:25 startup-script[1178]: ++ intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'COMPONENTS_TO_UNINSTALL: docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server'
<13>Feb 20 20:44:25 startup-script[1178]: COMPONENTS_TO_UNINSTALL: docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + PACKAGES_TO_UNINSTALL=($(difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Feb 20 20:44:25 startup-script[1178]: ++ difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Feb 20 20:44:25 startup-script[1178]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ sort -u
<13>Feb 20 20:44:25 startup-script[1178]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'PACKAGES_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc'
<13>Feb 20 20:44:25 startup-script[1178]: PACKAGES_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-zkfc
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported_for_s8s
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + configure_monitoring_agent
<13>Feb 20 20:44:25 startup-script[1178]: + is_arm64
<13>Feb 20 20:44:25 startup-script[1178]: ++ uname -m
<13>Feb 20 20:44:25 startup-script[1178]: + [[ x86_64 == \a\a\r\c\h\6\4 ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported_for_s8s
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/observability_agents/legacy.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ set -eux
<13>Feb 20 20:44:25 startup-script[1178]: ++ CLOUD_AGENTS_DL_URL=https://dl.google.com/cloudagents
<13>Feb 20 20:44:25 startup-script[1178]: + SERVICES=("${SERVICES[@]}" "$(monitoring_agent_service_name)")
<13>Feb 20 20:44:25 startup-script[1178]: ++ monitoring_agent_service_name
<13>Feb 20 20:44:25 startup-script[1178]: ++ echo stackdriver-agent
<13>Feb 20 20:44:25 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/configure_containerised_agents.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ set -eux
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ MONITORING_IMAGE=gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Feb 20 20:44:25 startup-script[1178]: ++ LOGGING_IMAGE=gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Feb 20 20:44:25 startup-script[1178]: ++ UCP_OBSERVABILITY_BASE_PATH=/etc/ucp
<13>Feb 20 20:44:25 startup-script[1178]: ++ UCP_METRICS_AGENT_VERSION=0.86.0
<13>Feb 20 20:44:25 startup-script[1178]: ++ UCP_METRICS_AGENT_PATH=/etc/ucp/monitoring
<13>Feb 20 20:44:25 startup-script[1178]: ++ UCP_LOGGING_AGENT_PATH=/etc/ucp/logging
<13>Feb 20 20:44:25 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/configure_oss_monitoring.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ set -eu
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++++ is_rocky
<13>Feb 20 20:44:25 startup-script[1178]: +++++ os_id
<13>Feb 20 20:44:25 startup-script[1178]: +++++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:25 startup-script[1178]: +++++ cut -d= -f2
<13>Feb 20 20:44:25 startup-script[1178]: +++++ xargs
<13>Feb 20 20:44:25 startup-script[1178]: ++++ [[ debian == \r\o\c\k\y ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++++ APT_SENTINEL=apt.lastupdate
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/configure_containerised_agents.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++ set -eux
<13>Feb 20 20:44:25 startup-script[1178]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Feb 20 20:44:25 startup-script[1178]: +++ MONITORING_IMAGE=gcr.io/cloud-dataproc/observability/monitoring-agent:latest
<13>Feb 20 20:44:25 startup-script[1178]: +++ LOGGING_IMAGE=gcr.io/cloud-dataproc/observability/logging-agent:latest
<13>Feb 20 20:44:25 startup-script[1178]: +++ UCP_OBSERVABILITY_BASE_PATH=/etc/ucp
<13>Feb 20 20:44:25 startup-script[1178]: +++ UCP_METRICS_AGENT_VERSION=0.86.0
<13>Feb 20 20:44:25 startup-script[1178]: +++ UCP_METRICS_AGENT_PATH=/etc/ucp/monitoring
<13>Feb 20 20:44:25 startup-script[1178]: +++ UCP_LOGGING_AGENT_PATH=/etc/ucp/logging
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Feb 20 20:44:25 startup-script[1178]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_metadata_dataproc_region
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly REGION_NAME=europe-west3
<13>Feb 20 20:44:25 startup-script[1178]: ++ REGION_NAME=europe-west3
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_metadata_role
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly ROLE=Master
<13>Feb 20 20:44:25 startup-script[1178]: ++ ROLE=Master
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly SPARK_CONF_DIR=/etc/spark/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly COLLECTD_S8S_PLUGIN_DIR=/usr/local/share/google/dataproc/bdutil/conf/collectd/s8s
<13>Feb 20 20:44:25 startup-script[1178]: ++ COLLECTD_S8S_PLUGIN_DIR=/usr/local/share/google/dataproc/bdutil/conf/collectd/s8s
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly FLINK_CONF=/usr/lib/flink/conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ FLINK_CONF=/usr/lib/flink/conf
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_property_in_xml /etc/hadoop/conf/capacity-scheduler.xml yarn.scheduler.capacity.root.queues
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ YARN_QUEUES=default
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly YARN_QUEUES
<13>Feb 20 20:44:25 startup-script[1178]: +++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:25 startup-script[1178]: ++ MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Feb 20 20:44:25 startup-script[1178]: ++ readonly MASTER_RUN_DRIVER_LOCATION
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_property dataproc.monitoring.stackdriver.jmx.enable
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ STACKDRIVER_MONITORING_JMX_ENABLED=
<13>Feb 20 20:44:25 startup-script[1178]: + use_ucp_observability_agents
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported_for_s8s
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + use_ucp_observability_agents
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + is_default_metrics_enabled
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.monitoring.default.metrics.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + local -r default_metrics_enabled=true
<13>Feb 20 20:44:25 startup-script[1178]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/configure_default_metrics.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ set -e
<13>Feb 20 20:44:25 startup-script[1178]: ++ set -u
<13>Feb 20 20:44:25 startup-script[1178]: ++ loginfo 'Running configure_default_metrics.sh'
<13>Feb 20 20:44:25 startup-script[1178]: ++ echo 'Running configure_default_metrics.sh'
<13>Feb 20 20:44:25 startup-script[1178]: Running configure_default_metrics.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Feb 20 20:44:25 startup-script[1178]: ++ update_default_collectd_config_files
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -r collectd_plugin_dir=/opt/stackdriver/collectd/etc/collectd.d/
<13>Feb 20 20:44:25 startup-script[1178]: ++ is_metric_source_enabled monitoringAgentDefaults
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -r input_metric_source=monitoringAgentDefaults
<13>Feb 20 20:44:25 startup-script[1178]: ++ local stack_driver_monitoring_enabled
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ stack_driver_monitoring_enabled=false
<13>Feb 20 20:44:25 startup-script[1178]: ++ [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ local metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_enabled_metric_sources_array metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -n ms_array=metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: ++ local metric_sources_enabled_delimited_by_comma
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_property dataproc.monitoring.metric.sources
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ metric_sources_enabled_delimited_by_comma=
<13>Feb 20 20:44:25 startup-script[1178]: ++ use_ucp_observability_agents
<13>Feb 20 20:44:25 startup-script[1178]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: +++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:25 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:25 startup-script[1178]: ++ is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: ++ [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: ++ local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: ++ case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:25 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:25 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:25 startup-script[1178]: ++ ms_array=(${metric_sources_enabled_delimited_by_comma//,/ })
<13>Feb 20 20:44:25 startup-script[1178]: ++ [[    =~  monitoringAgentDefaults  ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:25 startup-script[1178]: ++ sed -i /stackdriver_agent/d /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ cp /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf /etc/stackdriver/collectd.conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ cp /usr/local/share/google/dataproc/bdutil/conf/collectd_processes_default_metrics.conf /opt/stackdriver/collectd/etc/collectd.d//collectd_processes_default_metrics.conf
<13>Feb 20 20:44:25 startup-script[1178]: ++ cp /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_default_metrics.conf /opt/stackdriver/collectd/etc/collectd.d//collectd_spark_default_metrics.conf
<13>Feb 20 20:44:25 startup-script[1178]: + is_any_metric_source_enabled
<13>Feb 20 20:44:25 startup-script[1178]: + local stack_driver_monitoring_enabled
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + stack_driver_monitoring_enabled=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: + get_enabled_metric_sources_array metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: + local -n ms_array=metric_sources_enabled_array
<13>Feb 20 20:44:25 startup-script[1178]: + local metric_sources_enabled_delimited_by_comma
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.monitoring.metric.sources
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + metric_sources_enabled_delimited_by_comma=
<13>Feb 20 20:44:25 startup-script[1178]: + use_ucp_observability_agents
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + ms_array=(${metric_sources_enabled_delimited_by_comma//,/ })
<13>Feb 20 20:44:25 startup-script[1178]: + [[ 0 -gt 0 ]]
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Using legacy observability agents.'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Using legacy observability agents.'
<13>Feb 20 20:44:25 startup-script[1178]: Using legacy observability agents.
<13>Feb 20 20:44:25 startup-script[1178]: + delete_unused_legacy_or_ucp_agents
<13>Feb 20 20:44:25 startup-script[1178]: + use_ucp_observability_agents
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:25 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:25 startup-script[1178]: + set +x
<13>Feb 20 20:44:25 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + return 1
<13>Feb 20 20:44:25 startup-script[1178]: + configure_conscrypt
<13>Feb 20 20:44:25 startup-script[1178]: + local conscrypt_enabled
<13>Feb 20 20:44:25 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Feb 20 20:44:25 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: + conscrypt_enabled=true
<13>Feb 20 20:44:25 startup-script[1178]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:25 startup-script[1178]: + is_arm64
<13>Feb 20 20:44:25 startup-script[1178]: ++ uname -m
<13>Feb 20 20:44:25 startup-script[1178]: + [[ x86_64 == \a\a\r\c\h\6\4 ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local -r suffix=x86_64
<13>Feb 20 20:44:25 startup-script[1178]: + local -r conscrypt_shared_lib=/usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Feb 20 20:44:25 startup-script[1178]: + is_java8
<13>Feb 20 20:44:25 startup-script[1178]: + [[ /usr/lib/jvm/temurin-8-jdk-amd64 == *\-\8\-* ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local conscrypt_jar
<13>Feb 20 20:44:25 startup-script[1178]: ++ compgen -G '/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-*.jar'
<13>Feb 20 20:44:25 startup-script[1178]: + conscrypt_jar=/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar
<13>Feb 20 20:44:25 startup-script[1178]: + ln -s /usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/ext/conscrypt-openjdk.jar
<13>Feb 20 20:44:25 startup-script[1178]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Feb 20 20:44:25 startup-script[1178]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security.tmp
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -n '' ]]
<13>Feb 20 20:44:25 startup-script[1178]: + mv -f /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security.tmp /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -n '' ]]
<13>Feb 20 20:44:25 startup-script[1178]: + create_common_restart_drop_in_configs
<13>Feb 20 20:44:25 startup-script[1178]: + create_restart_drop_in_config /etc/systemd/system/common/restart.conf on-failure 0
<13>Feb 20 20:44:25 startup-script[1178]: + local -r conf_path=/etc/systemd/system/common/restart.conf
<13>Feb 20 20:44:25 startup-script[1178]: + local -r restart_type=on-failure
<13>Feb 20 20:44:25 startup-script[1178]: + local -r interval_sec=0
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ dirname /etc/systemd/system/common/restart.conf
<13>Feb 20 20:44:25 startup-script[1178]: + mkdir -p /etc/systemd/system/common
<13>Feb 20 20:44:25 startup-script[1178]: + cat
<13>Feb 20 20:44:25 startup-script[1178]: + create_restart_drop_in_config /etc/systemd/system/common/worker-restart.conf always 0
<13>Feb 20 20:44:25 startup-script[1178]: + local -r conf_path=/etc/systemd/system/common/worker-restart.conf
<13>Feb 20 20:44:25 startup-script[1178]: + local -r restart_type=always
<13>Feb 20 20:44:25 startup-script[1178]: + local -r interval_sec=0
<13>Feb 20 20:44:25 startup-script[1178]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Feb 20 20:44:25 startup-script[1178]: ++ dirname /etc/systemd/system/common/worker-restart.conf
<13>Feb 20 20:44:25 startup-script[1178]: + mkdir -p /etc/systemd/system/common
<13>Feb 20 20:44:25 startup-script[1178]: + cat
<13>Feb 20 20:44:25 startup-script[1178]: + loginfo 'Pre-activating components'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Pre-activating components'
<13>Feb 20 20:44:25 startup-script[1178]: Pre-activating components
<13>Feb 20 20:44:25 startup-script[1178]: + pre_activate_components earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn
<13>Feb 20 20:44:25 startup-script[1178]: + components=("$@")
<13>Feb 20 20:44:25 startup-script[1178]: + local components
<13>Feb 20 20:44:25 startup-script[1178]: + run_components_scripts pre-activate earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn
<13>Feb 20 20:44:25 startup-script[1178]: + local -r script_type=pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + all_components=("${@:2}")
<13>Feb 20 20:44:25 startup-script[1178]: + local -r all_components
<13>Feb 20 20:44:25 startup-script[1178]: + components=()
<13>Feb 20 20:44:25 startup-script[1178]: + local components
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/earlyoom.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/earlyoom.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/jupyter.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/jupyter.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/knox.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/knox.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/npd.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/npd.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/proxy-agent.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/proxy-agent.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + for component in "${all_components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh ]]
<13>Feb 20 20:44:25 startup-script[1178]: + components+=("${component}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Components with pre-activate script: hdfs hive-metastore hive-server2 mysql pig spark tez yarn'
<13>Feb 20 20:44:25 startup-script[1178]: Components with pre-activate script: hdfs hive-metastore hive-server2 mysql pig spark tez yarn
<13>Feb 20 20:44:25 startup-script[1178]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Feb 20 20:44:25 startup-script[1178]: + mkdir -p /tmp/dataproc/sentinel
<13>Feb 20 20:44:25 startup-script[1178]: + names=()
<13>Feb 20 20:44:25 startup-script[1178]: + local names
<13>Feb 20 20:44:25 startup-script[1178]: + scripts=()
<13>Feb 20 20:44:25 startup-script[1178]: + local scripts
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + for cmp in "${components[@]}"
<13>Feb 20 20:44:25 startup-script[1178]: + names+=("${cmp}.${script_type}")
<13>Feb 20 20:44:25 startup-script[1178]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Feb 20 20:44:25 startup-script[1178]: + execute_task_graph 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/co
<13>Feb 20 20:44:25 startup-script[1178]: mponents/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.0
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Generating makefile for the task graph'
<13>Feb 20 20:44:25 startup-script[1178]: Generating makefile for the task graph
<13>Feb 20 20:44:25 startup-script[1178]: + mkdir -p /tmp/dataproc/make
<13>Feb 20 20:44:25 startup-script[1178]: + local makefile
<13>Feb 20 20:44:25 startup-script[1178]: ++ mktemp /tmp/dataproc/make/makefile.XXXXXX
<13>Feb 20 20:44:25 startup-script[1178]: + makefile=/tmp/dataproc/make/makefile.bbg6ZZ
<13>Feb 20 20:44:25 startup-script[1178]: + generate_task_graph_makefile 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc
<13>Feb 20 20:44:25 startup-script[1178]: /bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.0
<13>Feb 20 20:44:25 startup-script[1178]: + names=()
<13>Feb 20 20:44:25 startup-script[1178]: + local names
<13>Feb 20 20:44:25 startup-script[1178]: + read -r -a names
<13>Feb 20 20:44:25 startup-script[1178]: + scripts=()
<13>Feb 20 20:44:25 startup-script[1178]: + local scripts
<13>Feb 20 20:44:25 startup-script[1178]: + read -r -a scripts
<13>Feb 20 20:44:25 startup-script[1178]: + deps_manifests=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifests
<13>Feb 20 20:44:25 startup-script[1178]: + read -r -a deps_manifests
<13>Feb 20 20:44:25 startup-script[1178]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Feb 20 20:44:25 startup-script[1178]: + task_args=("${@:5}")
<13>Feb 20 20:44:25 startup-script[1178]: + local -r task_args
<13>Feb 20 20:44:25 startup-script[1178]: + targets=()
<13>Feb 20 20:44:25 startup-script[1178]: + local targets
<13>Feb 20 20:44:25 startup-script[1178]: + (( i = 0 ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=hdfs.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/hdfs.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/hdfs.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: hdfs.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/hdfs.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=hive-metastore.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/hive-metastore.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: hive-metastore.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-metastore.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=hive-server2.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + dep_names=($(cat "${deps_manifest}"))
<13>Feb 20 20:44:25 startup-script[1178]: ++ cat /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Feb 20 20:44:25 startup-script[1178]: + local dep_names
<13>Feb 20 20:44:25 startup-script[1178]: + (( j = 0 ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( j < 4 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local dep_name=mysql.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  mysql\.pre-activate  ]]
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Feb 20 20:44:25 startup-script[1178]: + (( j++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( j < 4 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local dep_name=hdfs.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hdfs\.pre-activate  ]]
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Feb 20 20:44:25 startup-script[1178]: + (( j++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( j < 4 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local dep_name=hive-metastore.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hive-metastore\.pre-activate  ]]
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Feb 20 20:44:25 startup-script[1178]: + (( j++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( j < 4 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local dep_name=tez.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  tez\.pre-activate  ]]
<13>Feb 20 20:44:25 startup-script[1178]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Feb 20 20:44:25 startup-script[1178]: + (( j++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( j < 4 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: hive-server2.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-server2.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=mysql.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/mysql.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/mysql.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: mysql.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/mysql.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=pig.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/pig.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/pig.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: pig.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/pig.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=spark.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/spark.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/spark.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: spark.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/spark.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=tez.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/tez.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/tez.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: tez.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/tez.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + local name=yarn.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Feb 20 20:44:25 startup-script[1178]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps
<13>Feb 20 20:44:25 startup-script[1178]: + deps=()
<13>Feb 20 20:44:25 startup-script[1178]: + local deps
<13>Feb 20 20:44:25 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps ]]
<13>Feb 20 20:44:25 startup-script[1178]: + local target=/tmp/dataproc/sentinel/yarn.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + targets+=("${target}")
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '/tmp/dataproc/sentinel/yarn.pre-activate: '
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\t@echo '\''Running task: yarn.pre-activate'\'''
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\tbash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }'
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e '\ttouch /tmp/dataproc/sentinel/yarn.pre-activate\n'
<13>Feb 20 20:44:25 startup-script[1178]: + (( i++ ))
<13>Feb 20 20:44:25 startup-script[1178]: + (( i < 8 ))
<13>Feb 20 20:44:25 startup-script[1178]: + echo -e 'all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Generated makefile:'
<13>Feb 20 20:44:25 startup-script[1178]: Generated makefile:
<13>Feb 20 20:44:25 startup-script[1178]: + cat /tmp/dataproc/make/makefile.bbg6ZZ
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/hdfs.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: hdfs.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/hive-metastore.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: hive-metastore.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: hive-server2.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/mysql.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: mysql.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/pig.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: pig.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/spark.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: spark.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/tez.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: tez.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: /tmp/dataproc/sentinel/yarn.pre-activate: 
<13>Feb 20 20:44:25 startup-script[1178]: 	@echo 'Running task: yarn.pre-activate'
<13>Feb 20 20:44:25 startup-script[1178]: 	bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: 	touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: 
<13>Feb 20 20:44:25 startup-script[1178]: all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: + echo 'Running task graph:'
<13>Feb 20 20:44:25 startup-script[1178]: Running task graph:
<13>Feb 20 20:44:25 startup-script[1178]: + make -f /tmp/dataproc/make/makefile.bbg6ZZ all
<13>Feb 20 20:44:25 startup-script[1178]: Running task: hdfs.pre-activate
<13>Feb 20 20:44:25 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Feb 20 20:44:25 startup-script[1178]: + set_log_tag pre-activate-component-hdfs
<13>Feb 20 20:44:25 startup-script[1178]: + local -r tag=pre-activate-component-hdfs
<13>Feb 20 20:44:25 startup-script[1178]: + exec
<13>Feb 20 20:44:25 startup-script[1178]: ++ logger -s -t 'pre-activate-component-hdfs[2439]'
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + readonly HDFS_ADMIN=hdfs
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + HDFS_ADMIN=hdfs
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + readonly HDFS_SITE_XML=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + HDFS_SITE_XML=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ get_metadata_cluster_name
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: + export CLUSTER_NAME
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ get_metadata_master
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:25 startup-script[1178]: <13>Feb 20 20:44:25 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ get_metadata_master_additional
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + NUM_MASTERS=1
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ get_metadata_worker_count
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + NUM_WORKERS=0
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + DATANODE_PACKAGES=('hadoop-hdfs-datanode')
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + THIRD_MASTER_OPTIONAL_PACKAGES=('hadoop-hdfs-namenode' 'hadoop-hdfs-zkfc')
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + (( i = 0 ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + (( i < 1 ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + declare MASTER_HOSTNAME_0=project1-cluster-m
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + export MASTER_HOSTNAME_0
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + (( i++ ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + (( i < 1 ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ is_component_selected kerberos
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ local -r component=kerberos
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ local activated_components
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ get_components_to_activate
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ set +x
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ echo false
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + KERBEROS_ENABLED=false
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + export ENABLE_HDFS_PERMISSIONS=false
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + ENABLE_HDFS_PERMISSIONS=false
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ get_data_dirs
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ local -a mount_points
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ mapfile -t mount_points
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: +++ true
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ (( 0 ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ echo /
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ return
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_DIRS=/hadoop/dfs
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_DIRS_ARRAY=(${HDFS_DIRS})
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_DATA_DIRS_ARRAY=(${HDFS_DATA_DIRS})
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + chown -L -R hdfs:hadoop /hadoop/dfs /hadoop/dfs
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + chmod -R 700 /hadoop/dfs
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ free -m
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ awk '/^Mem:/{print $2}'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + TOTAL_MEM=16008
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ python -c 'print(int(16008 * 0.4 / 2))'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + NAMENODE_MEM_MB=3201
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + SECONDARYNAMENODE_MEM_MB=3201
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local reenable_x=false
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ -o xtrace ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + set +x
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + case ${compare_versions_result} in
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + return 1
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + GC_LOG_OPTS='-XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + cat
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + readonly HDFS_TEMPLATE=hdfs-template.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_TEMPLATE=hdfs-template.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + (( NUM_WORKERS == 0 ))
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.replication --value 1 --clobber
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + readonly HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_in_xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local domain
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ dnsdomainname
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + domain=europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.http-address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.namenode.http-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.http-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: I0220 20:44:26.496520 140402012182336 xml_config_commands.py:181] Property value is: "project1-cluster-m:50070"
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + prop_value=project1-cluster-m:50070
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ project1-cluster-m:50070 != \N\o\n\e ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + loginfo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + echo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ echo project1-cluster-m:50070
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ sed -e s/0.0.0.0/project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal/g
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + updated_value=project1-cluster-m:50070
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ project1-cluster-m:50070 != \p\r\o\j\e\c\t\1\-\c\l\u\s\t\e\r\-\m\:\5\0\0\7\0 ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.https-address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.namenode.https-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.https-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: I0220 20:44:26.609127 139903882016576 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.http-address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.namenode.secondary.http-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.http-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: I0220 20:44:26.718016 140549788997440 xml_config_commands.py:181] Property value is: "project1-cluster-m:50090"
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + prop_value=project1-cluster-m:50090
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ project1-cluster-m:50090 != \N\o\n\e ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + loginfo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + echo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ echo project1-cluster-m:50090
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ sed -e s/0.0.0.0/project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal/g
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + updated_value=project1-cluster-m:50090
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ project1-cluster-m:50090 != \p\r\o\j\e\c\t\1\-\c\l\u\s\t\e\r\-\m\:\5\0\0\9\0 ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.https-address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.namenode.secondary.https-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.https-address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: I0220 20:44:26.830039 140580109883200 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ hostname -f
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.datanode.address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: I0220 20:44:26.941766 140356191946560 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.http.address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_name=dfs.datanode.http.address
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:26 startup-script[1178]: <13>Feb 20 20:44:26 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.http.address
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: I0220 20:44:27.049559 139762123441984 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.https.address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local prop_name=dfs.datanode.https.address
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.https.address
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: I0220 20:44:27.156911 139844312721216 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.ipc.address project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local prop_name=dfs.datanode.ipc.address
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local host_name=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local updated_value
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local prop_value
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.ipc.address
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: I0220 20:44:27.263892 140077549836096 xml_config_commands.py:181] Property value is: "None"
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + prop_value=None
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + [[ None != \N\o\n\e ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + update_hdfs_param_values
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.work.multiplier.per.iteration --value 20 --clobber
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.max-streams --value 20 --clobber
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.replication.max-streams-hard-limit --value 40 --clobber
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ create_or_validate_include_file_path
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ local include_path
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ get_include_file_path
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ local include_path
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ include_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ [[ -z gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ include_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ [[ gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include == gs://* ]]
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ loginfo 'Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include'
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ echo 'Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include'
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: Checking if include membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ gcs_file_exists gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: ++ local -r file=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:27 startup-script[1178]: <13>Feb 20 20:44:27 pre-activate-component-hdfs[2439]: +++ gsutil -q stat gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ result=
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ [[ '' != 1 ]]
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ return 0
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: + INCLUDE_PATH=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ create_or_validate_exclude_file_path
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ local exclude_path
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ get_exclude_file_path
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ local exclude_path
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ exclude_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ [[ -z gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml ]]
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ exclude_path=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ [[ gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml == gs://* ]]
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ loginfo 'Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml'
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ echo 'Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml'
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: Checking if exclude membership files exist in GCS: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ gcs_file_exists gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: ++ local -r file=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:28 startup-script[1178]: <13>Feb 20 20:44:28 pre-activate-component-hdfs[2439]: +++ gsutil -q stat gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ result=
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ [[ '' != 1 ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ return 0
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ echo gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + EXCLUDE_PATH=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts --value gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include --clobber
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts.exclude --value gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_exclude.xml --clobber
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ get_metadata_datanode_enabled
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + [[ true != \t\r\u\e ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ get_metadata_role
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: ++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hdfs[2439]: + [[ 1 == \3 ]]
<13>Feb 20 20:44:30 startup-script[1178]: touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Feb 20 20:44:30 startup-script[1178]: Running task: hive-metastore.pre-activate
<13>Feb 20 20:44:30 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_metadata_master
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:30 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: ++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_metadata_master_additional
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:30 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:30 startup-script[1178]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:30 startup-script[1178]: ++ NUM_MASTERS=1
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_metadata_role
<13>Feb 20 20:44:30 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:30 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: ++ ROLE=Master
<13>Feb 20 20:44:30 startup-script[1178]: ++ [[ 1 -gt 1 ]]
<13>Feb 20 20:44:30 startup-script[1178]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:30 startup-script[1178]: + set -x
<13>Feb 20 20:44:30 startup-script[1178]: + set_log_tag pre-activate-component-hive-metastore
<13>Feb 20 20:44:30 startup-script[1178]: + local -r tag=pre-activate-component-hive-metastore
<13>Feb 20 20:44:30 startup-script[1178]: + exec
<13>Feb 20 20:44:30 startup-script[1178]: ++ logger -s -t 'pre-activate-component-hive-metastore[2898]'
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://project1-cluster-m:9083 --clobber
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + is_component_selected hdfs
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + local -r component=hdfs
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + local activated_components
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ get_components_to_activate
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hdfs* ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.warehouse.dir
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: ++ set +x
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + HIVE_WAREHOUSE_DIR=
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + [[ '' == \g\s\:\/\/* ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + METADATASTORE_JDBC_URI=jdbc:mysql://project1-cluster-m/metastore
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-hive-metastore[2898]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://project1-cluster-m/metastore --clobber
<13>Feb 20 20:44:30 startup-script[1178]: touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Feb 20 20:44:30 startup-script[1178]: Running task: mysql.pre-activate
<13>Feb 20 20:44:30 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Feb 20 20:44:30 startup-script[1178]: + set_log_tag pre-activate-component-mysql
<13>Feb 20 20:44:30 startup-script[1178]: + local -r tag=pre-activate-component-mysql
<13>Feb 20 20:44:30 startup-script[1178]: + exec
<13>Feb 20 20:44:30 startup-script[1178]: ++ logger -s -t 'pre-activate-component-mysql[2939]'
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + [[ project1-cluster-m != \p\r\o\j\e\c\t\1\-\c\l\u\s\t\e\r\-\m ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + start_mysql
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local service_name
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + is_rocky
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: ++ os_id
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: ++ cut -d= -f2
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: ++ xargs
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + [[ debian == \r\o\c\k\y ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + service_name=mysql
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + enable_service mysql
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r service=mysql
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r unit=mysql.service
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + retry_constant_short systemctl enable mysql.service
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + retry_constant_custom 30 1 systemctl enable mysql.service
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r max_retry_time=30
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r retry_delay=1
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + cmd=("${@:3}")
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r cmd
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local -r max_retries=30
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + local reenable_x=false
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + [[ -o xtrace ]]
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: + set +x
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: About to run 'systemctl enable mysql.service' with retries...
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: Synchronizing state of mysql.service with SysV service script with /lib/systemd/systemd-sysv-install.
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: Executing: /lib/systemd/systemd-sysv-install enable mysql
<13>Feb 20 20:44:30 startup-script[1178]: <13>Feb 20 20:44:30 pre-activate-component-mysql[2939]: Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service.
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: 'systemctl enable mysql.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + return 0
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r drop_in_dir=/etc/systemd/system/mysql.service.d
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + mkdir -p /etc/systemd/system/mysql.service.d
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local props
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ retry_constant_short systemctl show mysql.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ retry_constant_custom 30 1 systemctl show mysql.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ local -r retry_delay=1
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ local -r cmd
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ local -r max_retries=30
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ local reenable_x=false
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: About to run 'systemctl show mysql.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: 'systemctl show mysql.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ return 0
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + props='Restart=on-failure
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: RemainAfterExit=no'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ mysql != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ mysql != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ Restart=on-failure
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ mysql == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ mysql == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + retry_constant systemctl start mysql
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + retry_constant_custom 300 1 systemctl start mysql
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r max_retry_time=300
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r retry_delay=1
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + cmd=("${@:3}")
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r cmd
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local -r max_retries=300
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + local reenable_x=false
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ -o xtrace ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: About to run 'systemctl start mysql' with retries...
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: 'systemctl start mysql' succeeded after 1 execution(s).
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + return 0
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + change_file_permission
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + echo 'Changing file permissions on my.cnf'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: Changing file permissions on my.cnf
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + chmod 600 /etc/mysql/my.cnf
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + ensure_myconf_link_correct
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + is_debian12
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + is_debian
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ os_id
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ cut -d= -f2
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ xargs
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ debian == \d\e\b\i\a\n ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ os_version
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ grep '^VERSION_ID=' /etc/os-release
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ xargs
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: ++ cut -d= -f2
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + [[ 10 == \1\2* ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + reset_mysql_password
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: + set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: Connecting to mysql to reset password
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: mysql: [Warning] Using a password on the command line interface can be insecure.
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: Mysql password reset.
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-mysql[2939]: Stored mysql information in a safe place
<13>Feb 20 20:44:31 startup-script[1178]: touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Feb 20 20:44:31 startup-script[1178]: Running task: tez.pre-activate
<13>Feb 20 20:44:31 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Feb 20 20:44:31 startup-script[1178]: + set_log_tag pre-activate-component-tez
<13>Feb 20 20:44:31 startup-script[1178]: + local -r tag=pre-activate-component-tez
<13>Feb 20 20:44:31 startup-script[1178]: + exec
<13>Feb 20 20:44:31 startup-script[1178]: ++ logger -s -t 'pre-activate-component-tez[3089]'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + readonly TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + pre_activate_tez
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + configure_ui_war
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local tmp_dir
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ mktemp -d
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + tmp_dir=/tmp/tmp.olq3nIb2kB
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.olq3nIb2kB
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local -r tez_configs=/tmp/tmp.olq3nIb2kB/config/configs.env
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local -r tez_ui_js=/tmp/tmp.olq3nIb2kB/assets/tez-ui.js
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local ats_v2_port=8192
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + is_component_selected kerberos
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local -r component=kerberos
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local activated_components
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_components_to_activate
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local ats_v2_enabled=false
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + [[ -n '' ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + [[ -f /tmp/tmp.olq3nIb2kB/config/configs.env ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#\(.*\)//atsV2Enabled: true\(.*\)#\1atsV2Enabled: false\2#' /tmp/tmp.olq3nIb2kB/config/configs.env
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://project1-cluster-m:8188"\2#' /tmp/tmp.olq3nIb2kB/config/configs.env
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#\(.*\)//timelineV2: "http://localhost:8192"\(.*\)#\1timelineV2: "http://project1-cluster-m:8192"\2#' /tmp/tmp.olq3nIb2kB/config/configs.env
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://project1-cluster-m:8088"\2#' /tmp/tmp.olq3nIb2kB/config/configs.env
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + [[ -f /tmp/tmp.olq3nIb2kB/assets/tez-ui.js ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#"timeline":"localhost:8188"#"timeline":"project1-cluster-m:8188"#' /tmp/tmp.olq3nIb2kB/assets/tez-ui.js
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#"timelineV2":"localhost:8192"#"timelineV2":"project1-cluster-m:8192"#' /tmp/tmp.olq3nIb2kB/assets/tez-ui.js
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + sed -i 's#"rm":"localhost:8088"#"rm":"project1-cluster-m:8088"#' /tmp/tmp.olq3nIb2kB/assets/tez-ui.js
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + is_component_selected knox
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local -r component=knox
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local activated_components
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_components_to_activate
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *knox* ]]
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + local cluster_ui_hostname
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ get_dataproc_property dataproc.proxy.public.hostname
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ set +x
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: ++ sed 's/\\:/:/'
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + cluster_ui_hostname=https://r6r2ldsze5dpzj4vu4syltssoq-dot-europe-west3.dataproc.googleusercontent.com
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + cat
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + cd /tmp/tmp.olq3nIb2kB
<13>Feb 20 20:44:31 startup-script[1178]: <13>Feb 20 20:44:31 pre-activate-component-tez[3089]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./META-INF ./WEB-INF ./assets ./config ./fonts ./index.html
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + cd ..
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + rm -rf /tmp/tmp.olq3nIb2kB
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + touch -d @1706648617 /usr/lib/tez/tez-ui-0.9.2.war
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + configure_yarn_for_tez
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + configure_tez
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + local history_logging_service_class=ATSHistoryLoggingService
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: ++ set +x
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + [[ -n '' ]]
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://project1-cluster-m:8188/tez-ui/ --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + merge_user_properties
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + local src=/tmp/cluster/properties/tez.xml
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + local dest=/etc/tez/conf/tez-site.xml
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Feb 20 20:44:32 startup-script[1178]: <13>Feb 20 20:44:32 pre-activate-component-tez[3089]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-tez[3089]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-tez[3089]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-tez[3089]: Merged /tmp/cluster/properties/tez.xml.
<13>Feb 20 20:44:33 startup-script[1178]: touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: Running task: hive-server2.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-activate-component-hive-server2
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-activate-component-hive-server2
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-activate-component-hive-server2[3178]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_metadata_bucket
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + CONFIGBUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_metadata_cluster_uuid
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + CLUSTER_UUID=a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_metadata_master
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_metadata_master_additional
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + NUM_MASTERS=1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.user.install.directory --value gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/hive/user-install-dir --clobber
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + return 1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local src=/tmp/cluster/properties/hive.xml
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local dest=/etc/hive/conf/hive-site.xml
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: Merged /tmp/cluster/properties/hive.xml.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + merge_java_properties /tmp/cluster/properties/hive-log4j2.properties /etc/hive/conf/hive-log4j2.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local -r src=/tmp/cluster/properties/hive-log4j2.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local -r dest=/etc/hive/conf/hive-log4j2.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + [[ ! -f /tmp/cluster/properties/hive-log4j2.properties ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + cat /tmp/cluster/properties/hive-log4j2.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + loginfo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: + echo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-hive-server2[3178]: Merged /tmp/cluster/properties/hive-log4j2.properties.
<13>Feb 20 20:44:33 startup-script[1178]: touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: Running task: pig.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-activate-component-pig
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-activate-component-pig
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-activate-component-pig[3228]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + merge_user_properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r src=/tmp/cluster/properties/pig.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r dest=/etc/pig/conf/pig.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + cat /tmp/cluster/properties/pig.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: Merged /tmp/cluster/properties/pig.properties.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + merge_java_properties /tmp/cluster/properties/pig-log4j.properties /etc/pig/conf/log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r src=/tmp/cluster/properties/pig-log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r dest=/etc/pig/conf/log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + [[ ! -f /tmp/cluster/properties/pig-log4j.properties ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + cat /tmp/cluster/properties/pig-log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + loginfo 'Merged /tmp/cluster/properties/pig-log4j.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: + echo 'Merged /tmp/cluster/properties/pig-log4j.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-pig[3228]: Merged /tmp/cluster/properties/pig-log4j.properties.
<13>Feb 20 20:44:33 startup-script[1178]: touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: Running task: spark.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_metadata_master
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:33 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: ++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_metadata_master_additional
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:33 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:33 startup-script[1178]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:33 startup-script[1178]: ++ NUM_MASTERS=1
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_metadata_role
<13>Feb 20 20:44:33 startup-script[1178]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:33 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: ++ ROLE=Master
<13>Feb 20 20:44:33 startup-script[1178]: ++ [[ 1 -gt 1 ]]
<13>Feb 20 20:44:33 startup-script[1178]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:33 startup-script[1178]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Feb 20 20:44:33 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/nvidia-drivers.sh
<13>Feb 20 20:44:33 startup-script[1178]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Feb 20 20:44:33 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/spark.sh
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_HOME=/usr/lib/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_HOME=/usr/lib/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_JARS_DIR=/usr/lib/spark/jars
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_JARS_DIR=/usr/lib/spark/jars
<13>Feb 20 20:44:33 startup-script[1178]: ++ [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:33 startup-script[1178]: ++ [[ 2.0 =~ ^2\.[01]$ ]]
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONNECTOR_DIR=/usr/lib/spark/external
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONNECTOR_DIR=/usr/lib/spark/external
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONF_DIR=/etc/spark/conf
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_DEFAULT_IMAGE_FILE=/tmp/spark-default-image.tar
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_DEFAULT_IMAGE_FILE=/tmp/spark-default-image.tar
<13>Feb 20 20:44:33 startup-script[1178]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_DATA_DIR=/hadoop/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_DATA_DIR=/hadoop/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_LOG_DIR=/var/log/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_LOG_DIR=/var/log/spark
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_WORK_DIR=/hadoop/spark/work
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_WORK_DIR=/hadoop/spark/work
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONNECT_PORT=15001
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONNECT_PORT=15001
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONNECT_PROXY_PORT=8443
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONNECT_PROXY_PORT=8443
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONNECT_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect.service
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONNECT_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect.service
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_CONNECT_PROXY_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect-proxy.service
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_CONNECT_PROXY_SYSTEMD_UNIT_PATH=/usr/lib/systemd/system/spark-connect-proxy.service
<13>Feb 20 20:44:33 startup-script[1178]: ++ export SPARK_NATIVE_HOME_DIR=/usr/lib/spark/native
<13>Feb 20 20:44:33 startup-script[1178]: ++ SPARK_NATIVE_HOME_DIR=/usr/lib/spark/native
<13>Feb 20 20:44:33 startup-script[1178]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Feb 20 20:44:33 startup-script[1178]: ++ CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:33 startup-script[1178]: ++ CLUSTER_UUID=a6df27d4-edf3-467e-95d1-fc6408be3b22
<13>Feb 20 20:44:33 startup-script[1178]: ++ CONFIGBUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:33 startup-script[1178]: ++ TEMP_BUCKET=dataproc-temp-europe-west3-671384469824-mghpndlt
<13>Feb 20 20:44:33 startup-script[1178]: ++ HCFS_ROOT_URI=hdfs://project1-cluster-m
<13>Feb 20 20:44:33 startup-script[1178]: ++ MASTER_HOSTNAME_0=project1-cluster-m
<13>Feb 20 20:44:33 startup-script[1178]: ++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:33 startup-script[1178]: ++ DATAPROC_MASTER_FQDN=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:33 startup-script[1178]: ++ MASTER_HOSTNAMES=(project1-cluster-m)
<13>Feb 20 20:44:33 startup-script[1178]: ++ NUM_MASTERS=1
<13>Feb 20 20:44:33 startup-script[1178]: ++ NUM_WORKERS=0
<13>Feb 20 20:44:33 startup-script[1178]: ++ PREFIX=project1-cluster
<13>Feb 20 20:44:33 startup-script[1178]: ++ PROJECT=gcp-bigquery-project1
<13>Feb 20 20:44:33 startup-script[1178]: ++ ROLE=Master
<13>Feb 20 20:44:33 startup-script[1178]: + set -x
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-activate-component-spark
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-activate-component-spark
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-activate-component-spark[3300]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + readonly HCFS_EVENTLOG_DIR=hdfs://project1-cluster-m/user/spark/eventlog
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + HCFS_EVENTLOG_DIR=hdfs://project1-cluster-m/user/spark/eventlog
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + readonly SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + pre_activate_spark
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + update_bq_connector
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local connector_version
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_metadata_spark_bq_connector_version
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_VERSION attributes/SPARK_BQ_CONNECTOR_VERSION
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + connector_version=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local connector_url
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_metadata_spark_bq_connector_url
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_URL attributes/SPARK_BQ_CONNECTOR_URL
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + connector_url=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local jar_name
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -z '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -n '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -n '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + init_local_dirs_common
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + ln -sf /hadoop/spark/work /usr/lib/spark/work
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_env_common
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_arrow
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local event_log_dir=hdfs://project1-cluster-m/user/spark/eventlog
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local persist_history_to_gcs
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + persist_history_to_gcs=true
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + loginfo 'Enabling persisting Spark job history files to GCS.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo 'Enabling persisting Spark job history files to GCS.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: Enabling persisting Spark job history files to GCS.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -n dataproc-temp-europe-west3-671384469824-mghpndlt ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local gcs_history_dir_path=a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + event_log_dir=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_env_yarn
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_defaults_yarn
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_component_selected hive-server2
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r component=hive-server2
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local activated_components
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_components_to_activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hive-server2* ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local enable_docker_yarn
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property_or_default yarn.docker.enable ''
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + enable_docker_yarn=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -z '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property_or_default docker.yarn.enable ''
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + enable_docker_yarn=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_efm
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local spark_efm_property
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property efm.spark.shuffle
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + spark_efm_property=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ '' == \p\r\i\m\a\r\y\-\w\o\r\k\e\r ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_lineage
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local lineage_enabled
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: +++ get_metadata_dataproc_lineage_enabled
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: +++ get_dataproc_metadata DATAPROC_METADATA_LINEAGE_ENABLED attributes/DATAPROC_LINEAGE_ENABLED
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: +++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property_or_default dataproc.lineage.enabled ''
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + lineage_enabled=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + rm -f /usr/local/share/google/dataproc/lib/openlineage-spark.jar
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_component_selected kerberos
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r component=kerberos
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local activated_components
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_components_to_activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + merge_user_properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r src=/tmp/cluster/properties/spark.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r dest=/etc/spark/conf/spark-defaults.conf
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat /tmp/cluster/properties/spark.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: Merged /tmp/cluster/properties/spark.properties.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local dest=/etc/spark/conf/spark-env.sh
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat /tmp/cluster/properties/spark-env.sh
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 1
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + merge_java_properties /tmp/cluster/properties/spark-log4j.properties /etc/spark/conf/log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r src=/tmp/cluster/properties/spark-log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r dest=/etc/spark/conf/log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r 'header=\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ ! -f /tmp/cluster/properties/spark-log4j.properties ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo -e '\n# User-supplied properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat /tmp/cluster/properties/spark-log4j.properties
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + loginfo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: Merged /tmp/cluster/properties/spark-log4j.properties.
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + case ${compare_versions_result} in
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + return 0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + configure_broadcast_join 0.0075 200
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r fraction=0.0075
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r max_mb=200
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local user_broadcast_join_threshold_mb
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.sql.autoBroadcastJoinThreshold
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + user_broadcast_join_threshold_mb=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ -n '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local spark_executor_memory
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.executor.memory
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + spark_executor_memory=6157m
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local spark_executor_memory_bytes
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ get_size_in_bytes 6157m
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ local size=6157M
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ size=6157M
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ size=6157M
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ numfmt --from=iec 6157M
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + spark_executor_memory_bytes=6456082432
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local broadcast_join_threshold_mb
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ python -c 'print(min(max(int(6456082432 * 0.0075 / 1024 / 1024), 10), 200))'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + broadcast_join_threshold_mb=46
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + cat
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ 0 == \0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + run_in_background --tag spark-create-event-log-dir create_event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local -r pid=3432
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + shift 2
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ ! -f /tmp/dataproc/commands/3432.running ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo create_event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + echo 'Started background process [create_event_log_dir] as pid 3432'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: Started background process [create_event_log_dir] as pid 3432
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + run_with_logger --tag spark-create-event-log-dir create_event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local tag=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + local pid=3432
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + tag=spark-create-event-log-dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + shift 2
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + [[ 1 -eq 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: + create_event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ logger -s -t 'spark-create-event-log-dir[3432]'
<13>Feb 20 20:44:33 startup-script[1178]: Running task: yarn.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: bash -e /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + local event_log_dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + event_log_dir=gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + [[ gs://dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history == gs://* ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + loginfo 'Creating Spark event log dir in GCS'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + echo 'Creating Spark event log dir in GCS'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: Creating Spark event log dir in GCS
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + event_log_dir=dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + event_log_dir=dataproc-temp-europe-west3-671384469824-mghpndlt/a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + local event_log_dir_bucket=dataproc-temp-europe-west3-671384469824-mghpndlt
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + local event_log_dir_path=a6df27d4-edf3-467e-95d1-fc6408be3b22/spark-job-history
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-activate-component-yarn
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-activate-component-yarn
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-activate-component-yarn[3476]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + ATSV2_BIGTABLE_RESOURCE=
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + [[ -n '' ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + rm -Rf /usr/local/share/google/dataproc/lib/bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + is_component_selected kerberos
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + local -r component=kerberos
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + local activated_components
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ get_components_to_activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ set +x
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-yarn[3476]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:33 startup-script[1178]: touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'All pre-activate scripts done'
<13>Feb 20 20:44:33 startup-script[1178]: All pre-activate scripts done
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling unselected components'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling unselected components'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling unselected components
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_components docker-ce dpms-proxy druid flink hbase hive-webhcat-server hudi kafka-server kerberos presto ranger rubix solr-server zeppelin zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: + components=("$@")
<13>Feb 20 20:44:33 startup-script[1178]: + local components
<13>Feb 20 20:44:33 startup-script[1178]: + mkdir -p /tmp/dataproc/components/pre-uninstall
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component docker-ce'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component docker-ce'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-docker-ce[3502]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-docker-ce[3502]: + mark_packages_to_uninstall docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-docker-ce[3502]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-docker-ce[3502]: + touch /tmp/dataproc/uninstall/docker-ce
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component dpms-proxy'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component dpms-proxy'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component dpms-proxy
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component dpms-proxy
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=dpms-proxy
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Component dpms-proxy doesn'\''t have a pre-uninstall script'
<13>Feb 20 20:44:33 startup-script[1178]: Component dpms-proxy doesn't have a pre-uninstall script
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component druid'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component druid'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component druid
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component druid
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=druid
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/druid.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-druid
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-druid
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-druid[3511]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/druid.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-druid[3511]: + mark_packages_to_uninstall druid
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-druid[3511]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-druid[3511]: + touch /tmp/dataproc/uninstall/druid
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component flink'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component flink'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component flink
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component flink
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=flink
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/flink.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-flink
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-flink
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-flink[3520]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/flink.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-flink[3520]: + mark_packages_to_uninstall flink
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-flink[3520]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-flink[3520]: + touch /tmp/dataproc/uninstall/flink
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component hbase'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component hbase'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component hbase
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component hbase
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=hbase
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/hbase.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-hbase
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-hbase
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-hbase[3529]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hbase[3529]: + mark_packages_to_uninstall_pre_activate hbase hive-hbase
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hbase[3529]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hbase[3529]: + touch /tmp/dataproc/uninstall-pre-activate/hbase
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hbase[3529]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hbase[3529]: + touch /tmp/dataproc/uninstall-pre-activate/hive-hbase
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/hbase.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component hive-webhcat-server'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component hive-webhcat-server'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + [[ 200 -ge 200 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + [[ 200 -le 299 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: <13>Feb 20 20:44:33 spark-create-event-log-dir[3432]: + return 0
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-activate-component-spark[3300]: ++ echo 0
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-hive-webhcat-server[3546]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hive-webhcat-server[3546]: + mark_packages_to_uninstall hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hive-webhcat-server[3546]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-hive-webhcat-server[3546]: + touch /tmp/dataproc/uninstall/hive-webhcat-server
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component hudi'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component hudi'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component hudi
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component hudi
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=hudi
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Component hudi doesn'\''t have a pre-uninstall script'
<13>Feb 20 20:44:33 startup-script[1178]: Component hudi doesn't have a pre-uninstall script
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component kafka-server'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component kafka-server'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-kafka-server[3562]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kafka-server[3562]: + mark_packages_to_uninstall kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kafka-server[3562]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kafka-server[3562]: + touch /tmp/dataproc/uninstall/kafka-server
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component kerberos'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component kerberos'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component kerberos
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component kerberos
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=kerberos
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-kerberos
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-kerberos
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-kerberos[3579]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + is_rocky
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: ++ os_id
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: ++ cut -d= -f2
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: ++ xargs
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + [[ debian == \r\o\c\k\y ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + mark_packages_to_uninstall krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/krb5-user
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/krb5-config
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/krb5-kdc
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/krb5-admin-server
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/krb5-kpropd
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-kerberos[3579]: + touch /tmp/dataproc/uninstall/xinetd
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component presto'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component presto'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component presto
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component presto
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=presto
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/presto.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-presto
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-presto
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-presto[3605]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-presto[3605]: + mark_packages_to_uninstall presto
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-presto[3605]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-presto[3605]: + touch /tmp/dataproc/uninstall/presto
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/presto.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component ranger'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component ranger'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component ranger
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component ranger
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=ranger
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/ranger.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-ranger
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-ranger
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-ranger[3621]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/ranger.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-ranger[3621]: + mark_packages_to_uninstall ranger
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-ranger[3621]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-ranger[3621]: + touch /tmp/dataproc/uninstall/ranger
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component rubix'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component rubix'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component rubix
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component rubix
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=rubix
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/rubix.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-rubix
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-rubix
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-rubix[3637]'
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-rubix[3637]: + mark_packages_to_uninstall rubix
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-rubix[3637]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-rubix[3637]: + touch /tmp/dataproc/uninstall/rubix
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/rubix.done
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component solr-server'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component solr-server'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-solr-server[3653]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-solr-server[3653]: + mark_packages_to_uninstall solr-server
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-solr-server[3653]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-solr-server[3653]: + touch /tmp/dataproc/uninstall/solr-server
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component zeppelin'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component zeppelin'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-zeppelin[3669]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zeppelin[3669]: + mark_packages_to_uninstall zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zeppelin[3669]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zeppelin[3669]: + touch /tmp/dataproc/uninstall/zeppelin
<13>Feb 20 20:44:33 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Pre-uninstalling component zookeeper-server'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Pre-uninstalling component zookeeper-server'
<13>Feb 20 20:44:33 startup-script[1178]: Pre-uninstalling component zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: + pre_uninstall_component zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r component=zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh ]]
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh'
<13>Feb 20 20:44:33 startup-script[1178]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.running
<13>Feb 20 20:44:33 startup-script[1178]: + local exit_code=0
<13>Feb 20 20:44:33 startup-script[1178]: + bash -e /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Feb 20 20:44:33 startup-script[1178]: + set_log_tag pre-uninstall-component-zookeper
<13>Feb 20 20:44:33 startup-script[1178]: + local -r tag=pre-uninstall-component-zookeper
<13>Feb 20 20:44:33 startup-script[1178]: + exec
<13>Feb 20 20:44:33 startup-script[1178]: ++ logger -s -t 'pre-uninstall-component-zookeper[3685]'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.done
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zookeper[3685]: + mark_packages_to_uninstall zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zookeper[3685]: + for package in "$@"
<13>Feb 20 20:44:33 startup-script[1178]: <13>Feb 20 20:44:33 pre-uninstall-component-zookeper[3685]: + touch /tmp/dataproc/uninstall/zookeeper-server
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Uninstalling packages which must be uninstalled before activating components'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Uninstalling packages which must be uninstalled before activating components'
<13>Feb 20 20:44:33 startup-script[1178]: Uninstalling packages which must be uninstalled before activating components
<13>Feb 20 20:44:33 startup-script[1178]: + uninstall_packages_pre_activate
<13>Feb 20 20:44:33 startup-script[1178]: + local packages_to_uninstall
<13>Feb 20 20:44:33 startup-script[1178]: + packages_to_uninstall=($(list_packages_to_uninstall_pre_activate))
<13>Feb 20 20:44:33 startup-script[1178]: ++ list_packages_to_uninstall_pre_activate
<13>Feb 20 20:44:33 startup-script[1178]: ++ find /tmp/dataproc/uninstall-pre-activate/ -type f -exec basename '{}' ';'
<13>Feb 20 20:44:33 startup-script[1178]: + [[ 2 -gt 0 ]]
<13>Feb 20 20:44:33 startup-script[1178]: + os_uninstall_packages hive-hbase hbase
<13>Feb 20 20:44:33 startup-script[1178]: + packages_to_uninstall=("$@")
<13>Feb 20 20:44:33 startup-script[1178]: + local -r packages_to_uninstall
<13>Feb 20 20:44:33 startup-script[1178]: + loginfo 'Uninstalling packages: hive-hbase hbase'
<13>Feb 20 20:44:33 startup-script[1178]: + echo 'Uninstalling packages: hive-hbase hbase'
<13>Feb 20 20:44:33 startup-script[1178]: Uninstalling packages: hive-hbase hbase
<13>Feb 20 20:44:33 startup-script[1178]: + local -r 'uninstall_cmd=DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Feb 20 20:44:33 startup-script[1178]: + retry_constant_short bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Feb 20 20:44:33 startup-script[1178]: + retry_constant_custom 30 1 bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase'
<13>Feb 20 20:44:33 startup-script[1178]: + local -r max_retry_time=30
<13>Feb 20 20:44:33 startup-script[1178]: + local -r retry_delay=1
<13>Feb 20 20:44:33 startup-script[1178]: + cmd=("${@:3}")
<13>Feb 20 20:44:33 startup-script[1178]: + local -r cmd
<13>Feb 20 20:44:33 startup-script[1178]: + local -r max_retries=30
<13>Feb 20 20:44:33 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:33 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:33 startup-script[1178]: + set +x
<13>Feb 20 20:44:33 startup-script[1178]: About to run 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase' with retries...
<13>Feb 20 20:44:34 startup-script[1178]: Reading package lists...
<13>Feb 20 20:44:35 startup-script[1178]: Building dependency tree...
<13>Feb 20 20:44:35 startup-script[1178]: Reading state information...
<13>Feb 20 20:44:35 startup-script[1178]: The following packages will be REMOVED:
<13>Feb 20 20:44:35 startup-script[1178]:   hbase* hive-hbase*
<13>Feb 20 20:44:35 startup-script[1178]: 0 upgraded, 0 newly installed, 2 to remove and 4 not upgraded.
<13>Feb 20 20:44:35 startup-script[1178]: After this operation, 288 MB disk space will be freed.
<13>Feb 20 20:44:36 startup-script[1178]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 197702 files and directories currently installed.)
<13>Feb 20 20:44:36 startup-script[1178]: Removing hive-hbase (3.1.3-2) ...
<13>Feb 20 20:44:36 startup-script[1178]: Removing hbase (2.2.7-1) ...
<13>Feb 20 20:44:36 startup-script[1178]: Processing triggers for man-db (2.8.5-2) ...
<13>Feb 20 20:44:36 startup-script[1178]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 197146 files and directories currently installed.)
<13>Feb 20 20:44:36 startup-script[1178]: Purging configuration files for hbase (2.2.7-1) ...
<13>Feb 20 20:44:37 startup-script[1178]: 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hive-hbase hbase' succeeded after 1 execution(s).
<13>Feb 20 20:44:37 startup-script[1178]: + return 0
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Starting to uninstall artifacts'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Starting to uninstall artifacts'
<13>Feb 20 20:44:37 startup-script[1178]: Starting to uninstall artifacts
<13>Feb 20 20:44:37 startup-script[1178]: + start_uninstall_artifacts
<13>Feb 20 20:44:37 startup-script[1178]: + local blocking_default=
<13>Feb 20 20:44:37 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:37 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:37 startup-script[1178]: + return 0
<13>Feb 20 20:44:37 startup-script[1178]: + local blocking
<13>Feb 20 20:44:37 startup-script[1178]: ++ get_dataproc_property_or_default dataproc.uninstall.packages.blocking ''
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: + blocking=
<13>Feb 20 20:44:37 startup-script[1178]: + [[ -z '' ]]
<13>Feb 20 20:44:37 startup-script[1178]: + local init_action_count
<13>Feb 20 20:44:37 startup-script[1178]: ++ /usr/share/google/get_metadata_value attributes/dataproc-initialization-script-count
<13>Feb 20 20:44:37 startup-script[1178]: + init_action_count=0
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 0 == \0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + blocking=false
<13>Feb 20 20:44:37 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag delayed_uninstall_artifacts delayed_uninstall_artifacts
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3856
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=delayed_uninstall_artifacts
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 1 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + delayed_uninstall_artifacts
<13>Feb 20 20:44:37 startup-script[1178]: ++ get_dataproc_property_or_default dataproc.multi.user.metadata.proxy.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'delayed_uninstall_artifacts[3856]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 delayed_uninstall_artifacts[3856]: + sleep 60
<13>Feb 20 20:44:37 startup-script[1178]: + MULTI_USER_METADATA_PROXY_ENABLED=false
<13>Feb 20 20:44:37 startup-script[1178]: + readonly MULTI_USER_METADATA_PROXY_ENABLED
<13>Feb 20 20:44:37 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Starting services'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Starting services'
<13>Feb 20 20:44:37 startup-script[1178]: Starting services
<13>Feb 20 20:44:37 startup-script[1178]: + start_services dpms-proxy hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-datanode hadoop-yarn-nodemanager stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + services=("$@")
<13>Feb 20 20:44:37 startup-script[1178]: + local services
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array dpms-proxy DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=dpms-proxy
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \d\p\m\s\-\p\r\o\x\y\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-namenode DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-namenode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-namenode COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-namenode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hive-metastore DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\m\e\t\a\s\t\o\r\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hive-server2 DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\s\e\r\v\e\r\2\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hive-webhcat-server DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hive-webhcat-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\i\v\e\-\w\e\b\h\c\a\t\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array knox DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=knox
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \k\n\o\x\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array proxy-agent DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \p\r\o\x\y\-\a\g\e\n\t\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array solr-server DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=solr-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \s\o\l\r\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array spark-history-server DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=spark-history-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array spark-history-server COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=spark-history-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \s\p\a\r\k\-\h\i\s\t\o\r\y\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-resourcemanager DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-resourcemanager COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3868
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'setup_service hadoop-yarn-resourcemanager'
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3868
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=setup-hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 3868'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 3868
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + setup_service hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array jupyter DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=jupyter
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \j\u\p\y\t\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-mapreduce-historyserver DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-mapreduce-historyserver COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\m\a\p\r\e\d\u\c\e\-\h\i\s\t\o\r\y\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[3868]'
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3870
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3870.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'setup_service hadoop-mapreduce-historyserver'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 3870'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 3870
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array mysql-server DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=mysql-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array mysql-server COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=mysql-server
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \m\y\s\q\l\-\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-timelineserver DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-timelineserver COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\t\i\m\e\l\i\n\e\s\e\r\v\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3873
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3873.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'setup_service hadoop-yarn-timelineserver'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 3873'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 3873
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array zeppelin DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=zeppelin
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \z\e\p\p\e\l\i\n\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-secondarynamenode COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-secondarynamenode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\s\e\c\o\n\d\a\r\y\n\a\m\e\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-datanode DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-datanode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-hdfs-datanode COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-hdfs-datanode
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + continue
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-nodemanager DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array hadoop-yarn-nodemanager COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag setup-hadoop-yarn-nodemanager setup_service hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r service=hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + enable_service hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r service=hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r unit=hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + retry_constant_short systemctl enable hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3874
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3874.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'setup_service hadoop-yarn-nodemanager'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-resourcemanager[3868]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [setup_service hadoop-yarn-nodemanager] as pid 3874'
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [setup_service hadoop-yarn-nodemanager] as pid 3874
<13>Feb 20 20:44:37 startup-script[1178]: + for service in "${services[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + in_array stackdriver-agent DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3873
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=setup-hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=DATAPROC_COMPONENTS
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + setup_service hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql npd pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \s\t\a\c\k\d\r\i\v\e\r\-\a\g\e\n\t\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + in_array stackdriver-agent COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + local -r value=stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local -n values=COMPONENT_SERVICES
<13>Feb 20 20:44:37 startup-script[1178]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \s\t\a\c\k\d\r\i\v\e\r\-\a\g\e\n\t\ * ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag setup-stackdriver-agent setup_service stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3877
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[3873]'
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3877.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'setup_service stackdriver-agent'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [setup_service stackdriver-agent] as pid 3877'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [setup_service stackdriver-agent] as pid 3877
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating components'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating components'
<13>Feb 20 20:44:37 startup-script[1178]: Activating components
<13>Feb 20 20:44:37 startup-script[1178]: + activate_components earlyoom hdfs hive-metastore hive-server2 jupyter knox mapreduce miniconda3 mysql npd pig proxy-agent spark tez yarn
<13>Feb 20 20:44:37 startup-script[1178]: + components=("$@")
<13>Feb 20 20:44:37 startup-script[1178]: + local components
<13>Feb 20 20:44:37 startup-script[1178]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ earlyoom != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: earlyoom'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: earlyoom'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-earlyoom activate_component earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3883
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3883.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-earlyoom activate_component earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component earlyoom'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component earlyoom] as pid 3883'
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component earlyoom] as pid 3883
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3883
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ hdfs != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: hdfs'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: hdfs'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: hdfs
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-hdfs activate_component hdfs
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3887
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-earlyoom[3883]'
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component hdfs'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component hdfs] as pid 3887'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component hdfs] as pid 3887
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-hdfs activate_component hdfs
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ hive-metastore != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: hive-metastore'
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: hive-metastore'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3887
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-hive-metastore activate_component hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-hdfs
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag setup-hadoop-yarn-nodemanager setup_service hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3874
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component hdfs
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3890
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3890.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component hive-metastore'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component hive-metastore] as pid 3890'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component hive-metastore] as pid 3890
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ hive-server2 != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: hive-server2'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: hive-server2'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-hive-server2 activate_component hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3892
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3892.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component hive-server2'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component hive-server2] as pid 3892'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component hive-server2] as pid 3892
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ jupyter != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: jupyter'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: jupyter'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: jupyter
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-jupyter activate_component jupyter
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-hive-server2 activate_component hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r component=earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3892
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3894
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3894.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component jupyter'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component jupyter] as pid 3894'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component jupyter] as pid 3894
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ knox != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: knox'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: knox'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: knox
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-knox activate_component knox
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3896
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3896.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component knox'
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component knox] as pid 3896'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component knox] as pid 3896
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-knox activate_component knox
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ mapreduce != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: mapreduce'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: mapreduce'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-mapreduce activate_component mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3896
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-knox
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component knox
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3898
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-hive-server2[3892]'
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3898.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component mapreduce'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component mapreduce] as pid 3898'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component mapreduce] as pid 3898
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ miniconda3 != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-miniconda3 activate_component miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-knox[3896]'
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3901
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3901.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-hive-metastore activate_component hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3890
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-jupyter activate_component jupyter
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3894
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-jupyter
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r component=hive-server2
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + touch /tmp/dataproc/components/activate/hive-server2.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component miniconda3] as pid 3901'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component miniconda3] as pid 3901
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ mysql != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: mysql'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: mysql'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: mysql
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-mysql activate_component mysql
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3912
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3912.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component mysql'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component mysql] as pid 3912'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component mysql] as pid 3912
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ npd != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: npd'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: npd'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: npd
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-npd activate_component npd
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3913
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3913.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component npd'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component npd] as pid 3913'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component npd] as pid 3913
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ pig != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: pig'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: pig'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: pig
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-pig activate_component pig
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3914
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3914.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component pig'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component pig] as pid 3914'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component pig] as pid 3914
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ proxy-agent != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: proxy-agent'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: proxy-agent'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-proxy-agent activate_component proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3915
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3915.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component proxy-agent'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component proxy-agent] as pid 3915'
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-miniconda3 activate_component miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3901
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-mapreduce activate_component mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3898
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag setup-stackdriver-agent setup_service stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=setup-hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + setup_service hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3877
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=setup-stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + setup_service stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'setup-stackdriver-agent[3877]'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component proxy-agent] as pid 3915
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ spark != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: spark'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: spark'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: spark
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-spark activate_component spark
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3924
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3924.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component spark'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component spark] as pid 3924'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component spark] as pid 3924
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ tez != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: tez'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: tez'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: tez
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-tez activate_component tez
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3925
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3925.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component tez'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component tez] as pid 3925'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component tez] as pid 3925
<13>Feb 20 20:44:37 startup-script[1178]: + for component in "${components[@]}"
<13>Feb 20 20:44:37 startup-script[1178]: + [[ yarn != \k\e\r\b\e\r\o\s ]]
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Activating: yarn'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Activating: yarn'
<13>Feb 20 20:44:37 startup-script[1178]: Activating: yarn
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag activate-component-yarn activate_component yarn
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=3926
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-yarn activate_component yarn
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-hdfs[3887]'
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3926.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'activate_component yarn'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [activate_component yarn] as pid 3926'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [activate_component yarn] as pid 3926
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3926
<13>Feb 20 20:44:37 startup-script[1178]: + is_service_installed google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-yarn
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + local -r service=google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local output
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component yarn
<13>Feb 20 20:44:37 startup-script[1178]: + local status
<13>Feb 20 20:44:37 startup-script[1178]: + (( i = 0 ))
<13>Feb 20 20:44:37 startup-script[1178]: + (( i < 10 ))
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local -r component=knox
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + touch /tmp/dataproc/components/activate/knox.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3870
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=setup-hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + setup_service hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + local -r component=hdfs
<13>Feb 20 20:44:37 startup-script[1178]: ++ systemctl cat google-osconfig-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + touch /tmp/dataproc/components/activate/hdfs.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r service=hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + enable_service hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r service=hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r unit=hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + retry_constant_short systemctl enable hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-timelineserver[3873]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-yarn[3926]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + touch /tmp/dataproc/components/activate/earlyoom.running
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-hive-metastore[3890]'
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'setup-hadoop-yarn-nodemanager[3874]'
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-mapreduce[3898]'
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-miniconda3[3901]'
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-jupyter[3894]'
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-npd activate_component npd
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-pig activate_component pig
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-mysql activate_component mysql
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-proxy-agent activate_component proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: + output='# /lib/systemd/system/google-osconfig-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: [Unit]
<13>Feb 20 20:44:37 startup-script[1178]: Description=Google OSConfig Agent
<13>Feb 20 20:44:37 startup-script[1178]: After=local-fs.target network-online.target
<13>Feb 20 20:44:37 startup-script[1178]: Wants=local-fs.target network-online.target
<13>Feb 20 20:44:37 startup-script[1178]: 
<13>Feb 20 20:44:37 startup-script[1178]: [Service]
<13>Feb 20 20:44:37 startup-script[1178]: ExecStart=/usr/bin/google_osconfig_agent
<13>Feb 20 20:44:37 startup-script[1178]: Restart=always
<13>Feb 20 20:44:37 startup-script[1178]: RestartSec=1
<13>Feb 20 20:44:37 startup-script[1178]: StartLimitInterval=120
<13>Feb 20 20:44:37 startup-script[1178]: StartLimitBurst=3
<13>Feb 20 20:44:37 startup-script[1178]: KillMode=mixed
<13>Feb 20 20:44:37 startup-script[1178]: KillSignal=SIGTERM
<13>Feb 20 20:44:37 startup-script[1178]: 
<13>Feb 20 20:44:37 startup-script[1178]: [Install]
<13>Feb 20 20:44:37 startup-script[1178]: WantedBy=multi-user.target
<13>Feb 20 20:44:37 startup-script[1178]: 
<13>Feb 20 20:44:37 startup-script[1178]: # /etc/systemd/system/google-osconfig-agent.service.d/dataproc.conf
<13>Feb 20 20:44:37 startup-script[1178]: [Unit]
<13>Feb 20 20:44:37 startup-script[1178]: ConditionPathExists=!/etc/google-dataproc/hermetic_vm'
<13>Feb 20 20:44:37 startup-script[1178]: + status=0
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 0 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 0
<13>Feb 20 20:44:37 startup-script[1178]: + is_hermetic_vm
<13>Feb 20 20:44:37 startup-script[1178]: + local -r sentinel_file=/etc/google-dataproc/hermetic_vm
<13>Feb 20 20:44:37 startup-script[1178]: + [[ -f /etc/google-dataproc/hermetic_vm ]]
<13>Feb 20 20:44:37 startup-script[1178]: + local hermetic_vm
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + local -r component=hive-metastore
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3914
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-pig
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component pig
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + touch /tmp/dataproc/components/activate/hive-metastore.running
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3913
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-npd
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component npd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r service=stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + enable_service stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r service=stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r unit=stackdriver-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + retry_constant_short systemctl enable stackdriver-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + retry_constant_custom 30 1 systemctl enable stackdriver-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r component=mapreduce
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + touch /tmp/dataproc/components/activate/mapreduce.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: About to run 'systemctl enable stackdriver-agent.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3912
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-mysql
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component mysql
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[3870]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r component=miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + touch /tmp/dataproc/components/activate/miniconda3.running
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3915
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: ++ /usr/share/google/get_metadata_value attributes/hermetic-vm
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-pig[3914]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-npd[3913]'
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-mysql[3912]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r service=hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + enable_service hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r service=hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r unit=hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + retry_constant_short systemctl enable hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + retry_constant_custom 30 1 systemctl enable hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-mapreduce-historyserver[3870]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r component=npd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + touch /tmp/dataproc/components/activate/npd.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r start=1708461877.352733407
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local -r component=jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-proxy-agent[3915]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r component=pig
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + touch /tmp/dataproc/components/activate/pig.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r start=1708461877.357599062
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r component=mysql
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + touch /tmp/dataproc/components/activate/mysql.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r service=hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + enable_service hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r service=hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r unit=hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + retry_constant_short systemctl enable hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r start=1708461877.364000534
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: About to run 'systemctl enable hadoop-yarn-nodemanager.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: hadoop-yarn-nodemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-hadoop-yarn-nodemanager[3874]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-nodemanager
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-spark activate_component spark
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3924
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-spark
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component spark
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag activate-component-tez activate_component tez
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=3925
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=activate-component-tez
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + activate_component tez
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local -r start=1708461877.368653455
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r start=1708461877.369106495
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + local -r component=proxy-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + touch /tmp/dataproc/components/activate/proxy-agent.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + local -r start=1708461877.368594287
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r end=1708461877.372246786
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r start=1708461877.372535089
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + echo 'Component pig took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: Component pig took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + local -r time_file=/tmp/dataproc/components/activate/pig.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + touch /tmp/dataproc/components/activate/pig.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + local -r start=1708461877.374875729
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-pig[3914]: + touch /tmp/dataproc/components/activate/pig.done
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: stackdriver-agent.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 setup-stackdriver-agent[3877]: Executing: /lib/systemd/systemd-sysv-install enable stackdriver-agent
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r start=1708461877.370267338
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + touch /tmp/dataproc/components/activate/jupyter.running
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-spark[3924]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'activate-component-tez[3925]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r component=spark
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + touch /tmp/dataproc/components/activate/spark.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local -r start=1708461877.400064320
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r component=tez
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + touch /tmp/dataproc/components/activate/tez.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r component=yarn
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + mkdir -p /tmp/dataproc/components/activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + touch /tmp/dataproc/components/activate/yarn.running
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local exit_code=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + local -r start=1708461877.408029762
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r start=1708461877.410061971
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r start=1708461877.424608679
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r start=1708461877.427070289
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r start=1708461877.428581681
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + bash -e /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r end=1708461877.434342042
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + echo 'Component mysql took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: Component mysql took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + local -r time_file=/tmp/dataproc/components/activate/mysql.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + touch /tmp/dataproc/components/activate/mysql.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r end=1708461877.459754997
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + echo 'Component tez took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: Component tez took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + local -r time_file=/tmp/dataproc/components/activate/tez.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + touch /tmp/dataproc/components/activate/tez.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mysql[3912]: + touch /tmp/dataproc/components/activate/mysql.done
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + main
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r earlyoom_config_file=/etc/default/earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + is_earlyoom_enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + get_dataproc_property_or_default internal.node.main.memory-protection.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + install_effective_python_profile
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ ! -f /etc/profile.d/effective-python.sh ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + generate_effective_python_profile
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + is_component_selected miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r component=miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + is_component_selected hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r component=hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-tez[3925]: + touch /tmp/dataproc/components/activate/tez.done
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly CLUSTER_PROPS=/tmp/cluster/properties
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + CLUSTER_PROPS=/tmp/cluster/properties
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly KNOX_INSTALL_DIRECTORY=/usr/lib/knox
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + KNOX_INSTALL_DIRECTORY=/usr/lib/knox
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: ++ hostname -f
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + FQDN=project1-cluster-m.europe-west3-c.c.gcp-bigquery-project1.internal
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly FQDN
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly GATEWAY_CONF=/usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + GATEWAY_CONF=/usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly KNOX_SERVICE_INIT_SCRIPT=/usr/lib/systemd/system/knox.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + KNOX_SERVICE_INIT_SCRIPT=/usr/lib/systemd/system/knox.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + configure_earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r earlyoom_log_file=/var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r 'prefer_regex=^.*java[[:space:]].*application[_0-9]+[^[:space:]]+container[_0-9]+.*'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local earlyoom_threshold
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: ++ get_dataproc_property_or_default internal.node.main.memory-protection.threshold.kib 65536
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hdfs* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + echo 'Delaying starting MapReduce history server until HDFS is ready'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: Delaying starting MapReduce history server until HDFS is ready
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + readonly IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r end=1708461877.531549457
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + TOPOLOGY_TEMPLATE=knox/topology-template.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + (( 1 > 1 ))
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + generate_config
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + mv knox/gateway-site-template.xml /usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + echo 'Component mapreduce took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: Component mapreduce took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + local -r time_file=/tmp/dataproc/components/activate/mapreduce.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + touch /tmp/dataproc/components/activate/mapreduce.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *miniconda3* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ set -euo pipefail
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/conda.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: +++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: +++ export CONDA_HOME=/opt/conda/default
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: +++ CONDA_HOME=/opt/conda/default
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/nfs.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: +++ export NFS_MOUNT_PATH=/mnt/dataproc
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: +++ NFS_MOUNT_PATH=/mnt/dataproc
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ export MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ export MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ export PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ /opt/conda/miniconda3 != \/\o\p\t\/\c\o\n\d\a\/\d\e\f\a\u\l\t ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + ln -f -s /opt/conda/miniconda3 /opt/conda/default
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + emit_conda_profile /opt/conda/default
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r conda_dir=/opt/conda/default
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r python_bin=/opt/conda/default/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local temp
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_metadata_master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ mktemp
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + temp=/tmp/tmp.NY4qPCxDFA
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_metadata_master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + ln -f -s /opt/conda/default/etc/profile.d/conda.sh /etc/profile.d/conda.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + merge_xml_properties /tmp/cluster/properties/knox.xml /usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local src=/tmp/cluster/properties/knox.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + local dest=/usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + [[ ! -f /tmp/cluster/properties/knox.xml ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-knox[3896]: + bdconfig merge_configurations --configuration_file /usr/lib/knox/conf/gateway-site.xml --source_configuration_file /tmp/cluster/properties/knox.xml --resolve_environment_variables --create_if_absent --clobber
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + is_default_system_metrics_enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + chmod +x /opt/conda/default/etc/profile.d/conda.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_dataproc_property dataproc.monitoring.default.metrics.system.enable
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo false
<13>Feb 20 20:44:37 startup-script[1178]: + hermetic_vm=false
<13>Feb 20 20:44:37 startup-script[1178]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Starting service google-osconfig-agent'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Starting service google-osconfig-agent'
<13>Feb 20 20:44:37 startup-script[1178]: Starting service google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: + run_in_background --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: + local -r pid=4246
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/4246.running ]]
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Started background process [enable_and_start_service google-osconfig-agent] as pid 4246'
<13>Feb 20 20:44:37 startup-script[1178]: Started background process [enable_and_start_service google-osconfig-agent] as pid 4246
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Feb 20 20:44:37 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-mapreduce[3898]: + touch /tmp/dataproc/components/activate/mapreduce.done
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + earlyoom_threshold=65536
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:37 startup-script[1178]: + run_with_logger --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: + local tag=
<13>Feb 20 20:44:37 startup-script[1178]: + local pid=4246
<13>Feb 20 20:44:37 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:37 startup-script[1178]: + tag=start-osconfig-service
<13>Feb 20 20:44:37 startup-script[1178]: + shift 2
<13>Feb 20 20:44:37 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: + enable_and_start_service google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + touch /var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + chmod a+rw /var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: ++ logger -s -t 'start-osconfig-service[4246]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_property dataproc.proxy.agent.endpoint
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ sed 's/\\:/:/'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + readonly WAIT_TIMEOUT_SECONDS=200
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + WAIT_TIMEOUT_SECONDS=200
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + readonly INITIAL_WORKER_COUNT=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + INITIAL_WORKER_COUNT=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + enable_and_start_service earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r service=earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + enable_service earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r service=earlyoom
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r unit=earlyoom.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + retry_constant_short systemctl enable earlyoom.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + retry_constant_custom 30 1 systemctl enable earlyoom.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-earlyoom[3883]: About to run 'systemctl enable earlyoom.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r default_metrics_system_enabled=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r service=google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + enable_service google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r service=google-osconfig-agent
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r unit=google-osconfig-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + retry_constant_short systemctl enable google-osconfig-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + retry_constant_custom 30 1 systemctl enable google-osconfig-agent.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r max_retry_time=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local -r max_retries=30
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 start-osconfig-service[4246]: About to run 'systemctl enable google-osconfig-agent.service' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + mv -n -v /tmp/tmp.NY4qPCxDFA /etc/profile.d/effective-python.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_dataproc_property dataproc.monitoring.job.yarn.metrics.enable
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + COMPONENT_GATEWAY_HA_ENABLED=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + readonly COMPONENT_GATEWAY_HA_ENABLED
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + activate_spark
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + should_start_history_server
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ 0 == \0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + is_component_selected hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r component=hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + ATSV2_BIGTABLE_RESOURCE=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + [[ -n '' ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: renamed '/tmp/tmp.NY4qPCxDFA' -> '/etc/profile.d/effective-python.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + chmod a+r /etc/profile.d/effective-python.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r end=1708461877.628585182
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + echo 'Component yarn took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: Component yarn took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + local -r time_file=/tmp/dataproc/components/activate/yarn.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + touch /tmp/dataproc/components/activate/yarn.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + rm -Rf /tmp/tmp.NY4qPCxDFA
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_metadata_master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-yarn[3926]: + touch /tmp/dataproc/components/activate/yarn.done
<13>Feb 20 20:44:37 startup-script[1178]: + STACKDRIVER_LOGGING_ENABLED=
<13>Feb 20 20:44:37 startup-script[1178]: + [[ '' == \f\a\l\s\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly HOSTNAME=https://europe-west3.dataproc.cloud.google.com/
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + HOSTNAME=https://europe-west3.dataproc.cloud.google.com/
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly PROXY=https://europe-west3.dataproc.cloud.google.com/tun/m/4592f092208ecc84946b8f8f8016274df1b36a14/
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + PROXY=https://europe-west3.dataproc.cloud.google.com/tun/m/4592f092208ecc84946b8f8f8016274df1b36a14/
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly HOST=localhost:8443
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + HOST=localhost:8443
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly SHIM_PATH=websocket-shim
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SHIM_PATH=websocket-shim
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + customize_conda_env /opt/conda/miniconda3 /opt/conda/miniconda3/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r conda_install_path=/opt/conda/miniconda3
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r conda_bin_dir=/opt/conda/miniconda3/bin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ get_dataproc_property conda.env.config.uri
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ logging_agent_service_name
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo google-fluentd
<13>Feb 20 20:44:37 startup-script[1178]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Feb 20 20:44:37 startup-script[1178]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Feb 20 20:44:37 startup-script[1178]: Stackdriver enabled; enabling google-fluentd.
<13>Feb 20 20:44:37 startup-script[1178]: + use_ucp_observability_agents
<13>Feb 20 20:44:37 startup-script[1178]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_property dataproc.proxy.backend.id
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ get_dataproc_property dataproc.observability.containerised.legacy.agents.enable
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_dataproc_property dataproc.metrics.node.yarn.nodemanager.health.enable
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hdfs* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + should_start_spark_connect
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_metadata_role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r conda_env_config_uri=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ get_dataproc_property conda.packages
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + configure_npd true true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r yarn_job_metrics_enabled=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r yarn_nm_metrics_enabled=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r init_script=/etc/systemd/system/npd.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r config_dir=/usr/local/share/google/dataproc/npd-config
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs=()
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local npd_configs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs+=("--stackdriver-config=${config_dir}/exporter/stackdriver-exporter.json")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs+=("--config.system-stats-monitor=${config_dir}/system-stats-monitor.json,${config_dir}/net-cgroup-system-stats-monitor.json")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local yarn_job_monitor_config_file=yarn-rm-monitor
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + is_component_selected kerberos
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r component=kerberos
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_metadata_master_additional
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_metadata_master_additional
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly BACKEND=r6r2ldsze5dpzj4vu4syltssoq
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + BACKEND=r6r2ldsze5dpzj4vu4syltssoq
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r conda_packages=
<13>Feb 20 20:44:37 startup-script[1178]: + local -r legacy_agents_enabled_prop=
<13>Feb 20 20:44:37 startup-script[1178]: + is_containerised_legacy_agents_supported
<13>Feb 20 20:44:37 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: + is_version_at_least 2.0 2.2
<13>Feb 20 20:44:37 startup-script[1178]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_property dataproc.proxy.agent.enablewebsockets
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ get_dataproc_property pip.packages
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: + case ${compare_versions_result} in
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + return 1
<13>Feb 20 20:44:37 startup-script[1178]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Feb 20 20:44:37 startup-script[1178]: ++ set -eu
<13>Feb 20 20:44:37 startup-script[1178]: ++ loginfo 'Running configure_fluentd.sh'
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 'Running configure_fluentd.sh'
<13>Feb 20 20:44:37 startup-script[1178]: Running configure_fluentd.sh
<13>Feb 20 20:44:37 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Feb 20 20:44:37 startup-script[1178]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Feb 20 20:44:37 startup-script[1178]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Feb 20 20:44:37 startup-script[1178]: ++++ is_rocky
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly ENABLE_WEBSOCKETS=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + ENABLE_WEBSOCKETS=true
<13>Feb 20 20:44:37 startup-script[1178]: +++++ os_id
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs+=("--config.yarn-monitor=${config_dir}/${yarn_job_monitor_config_file}.json")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local yarn_nm_monitor_config_file=yarn-nm-monitor
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + is_component_selected kerberos
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r component=kerberos
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_property dataproc.proxy.agent.sessioncookie
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: +++++ cut -d= -f2
<13>Feb 20 20:44:37 startup-script[1178]: +++++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:37 startup-script[1178]: +++++ xargs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + role=Master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local spark_connect_enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ get_java_property /tmp/cluster/properties/dataproc.properties internal.s8s.spark-connect.enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r pip_packages=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ -n '' ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + retry_constant_custom 4 1 install_conda_packages /opt/conda/miniconda3/bin ''
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r max_retry_time=4
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ NUM_MASTERS=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r max_retries=4
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: About to run 'install_conda_packages /opt/conda/miniconda3/bin ' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: 'install_conda_packages /opt/conda/miniconda3/bin ' succeeded after 1 execution(s).
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + return 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + retry_constant_custom 4 1 install_pip_packages ''
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r max_retry_time=4
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r retry_delay=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_metadata_role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + cmd=("${@:3}")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r cmd
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r max_retries=4
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: About to run 'install_pip_packages ' with retries...
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: 'install_pip_packages ' succeeded after 1 execution(s).
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + return 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly DATAPROC_MASTER
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly SESSION_COOKIE_NAME=_xsrf
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SESSION_COOKIE_NAME=_xsrf
<13>Feb 20 20:44:37 startup-script[1178]: ++++ [[ debian == \r\o\c\k\y ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Feb 20 20:44:37 startup-script[1178]: +++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Feb 20 20:44:37 startup-script[1178]: +++++ APT_SENTINEL=apt.lastupdate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r end=1708461877.769804194
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + echo 'Component miniconda3 took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: Component miniconda3 took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + local -r time_file=/tmp/dataproc/components/activate/miniconda3.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + touch /tmp/dataproc/components/activate/miniconda3.time
<13>Feb 20 20:44:37 startup-script[1178]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/earlyoom.sh
<13>Feb 20 20:44:37 startup-script[1178]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Feb 20 20:44:37 startup-script[1178]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Feb 20 20:44:37 startup-script[1178]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Feb 20 20:44:37 startup-script[1178]: ++ is_containerised_legacy_agents_supported
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ is_version_at_least 2.0 2.2
<13>Feb 20 20:44:37 startup-script[1178]: ++ local reenable_x=false
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_metadata_bucket
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_property dataproc.proxy.agent.enablebanner
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + spark_connect_enabled=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-miniconda3[3901]: + touch /tmp/dataproc/components/activate/miniconda3.done
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r end=1708461877.792828264
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + echo 'Component spark took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs+=("--config.yarn-monitor=${config_dir}/${yarn_nm_monitor_config_file}.json")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: Component spark took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + is_earlyoom_enabled
<13>Feb 20 20:44:37 startup-script[1178]: ++ case ${compare_versions_result} in
<13>Feb 20 20:44:37 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:37 startup-script[1178]: ++ return 1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + local -r time_file=/tmp/dataproc/components/activate/spark.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + get_dataproc_property_or_default internal.node.main.memory-protection.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + touch /tmp/dataproc/components/activate/spark.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ mkdir -p /etc/google-fluentd/config.d
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ NUM_MASTERS=1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_metadata_role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-spark[3924]: + touch /tmp/dataproc/components/activate/spark.done
<13>Feb 20 20:44:37 startup-script[1178]: ++ mkdir -p /etc/google-fluentd/plugin
<13>Feb 20 20:44:37 startup-script[1178]: ++ sed -i -e 's/enable_monitoring true/enable_monitoring false/' /etc/google-fluentd/google-fluentd.conf
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly ENABLE_BANNER=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + ENABLE_BANNER=true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly SIGNIN_URL=https://europe-west3.dataproc.cloud.google.com/_signin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SIGNIN_URL=https://europe-west3.dataproc.cloud.google.com/_signin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SHIM_WEBSOCKETS_FLAG=-shim-websockets
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SHIM_WEBSOCKETS_FLAG=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SESSION_COOKIE_NAME_FLAG=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + [[ -n _xsrf ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + SESSION_COOKIE_NAME_FLAG=-session-cookie-name=_xsrf
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_metadata_project_id
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + DATAPROC_BUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly DATAPROC_BUCKET
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_property jupyter.hub.enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: true
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r earlyoom_log_file=/var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + touch /var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + JUPYTERHUB_ENABLED=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly JUPYTERHUB_ENABLED
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + chmod a+rw /var/log/earlyoom.log
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_property jupyter.hub.menu.enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ cp -r /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-agent.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-dpms-proxy.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-startup.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-yarn-userlogs.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc_fluentd.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/metadata-proxy.conf /etc/google-fluentd/config.d
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + npd_configs+=("--config.system-log-monitor=${config_dir}/earlyoom-monitor.json")
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + create_systemd_service /etc/systemd/system/npd.service '--stackdriver-config=/usr/local/share/google/dataproc/npd-config/exporter/stackdriver-exporter.json --config.system-stats-monitor=/usr/local/share/google/dataproc/npd-config/system-stats-monitor.json,/usr/local/share/google/dataproc/npd-config/net-cgroup-system-stats-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-rm-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-nm-monitor.json --config.system-log-monitor=/usr/local/share/google/dataproc/npd-config/earlyoom-monitor.json'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r service_path=/etc/systemd/system/npd.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + local -r 'npd_config_args=--stackdriver-config=/usr/local/share/google/dataproc/npd-config/exporter/stackdriver-exporter.json --config.system-stats-monitor=/usr/local/share/google/dataproc/npd-config/system-stats-monitor.json,/usr/local/share/google/dataproc/npd-config/net-cgroup-system-stats-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-rm-monitor.json --config.yarn-monitor=/usr/local/share/google/dataproc/npd-config/yarn-nm-monitor.json --config.system-log-monitor=/usr/local/share/google/dataproc/npd-config/earlyoom-monitor.json'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + chmod a+rw /etc/systemd/system/npd.service
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-npd[3913]: + systemctl daemon-reload
<13>Feb 20 20:44:37 startup-script[1178]: ++ cp -r /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/filter_add_insert_ids.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/in_object_space_dump.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/statusz.rb /etc/google-fluentd/plugin
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ ROLE=Master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ [[ 1 -gt 1 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + set -x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + activate_hive_server2
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + configure_hive_hbase
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + is_component_selected hbase
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r component=hbase
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Feb 20 20:44:37 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hbase* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + is_component_selected hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r component=hdfs
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local activated_components
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ get_components_to_activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + ADD_HUB_MENU=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly ADD_HUB_MENU
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_property jupyter.dataprocuser.enabled
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *hdfs* ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + echo 'Delaying starting Hive server until HDFS is ready'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: Delaying starting Hive server until HDFS is ready
<13>Feb 20 20:44:37 startup-script[1178]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: ++ date +%s.%N
<13>Feb 20 20:44:37 startup-script[1178]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Feb 20 20:44:37 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r end=1708461877.925858563
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r runtime_s=0
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + echo 'Component hive-server2 took 0s to activate'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: Component hive-server2 took 0s to activate
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + local -r time_file=/tmp/dataproc/components/activate/hive-server2.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + touch /tmp/dataproc/components/activate/hive-server2.time
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + JUPYTER_DATAPROC_USER_ENABLED=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly JUPYTER_DATAPROC_USER_ENABLED
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + install -d /etc/jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + cat
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-server2[3892]: + touch /tmp/dataproc/components/activate/hive-server2.done
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + chmod a+r /etc/jupyter
<13>Feb 20 20:44:37 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:37 startup-script[1178]: ++ CONTAINER_LOGGING_ENABLED=
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_property internal.s8s.jupyter.kernel
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: +++ get_dataproc_property_or_default dataproc.cluster.caching.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: +++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ ROLE=Master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ [[ 1 -gt 1 ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ set -euo pipefail
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + S8S_JUPYTER_KERNEL_TO_ENABLE=
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + readonly S8S_JUPYTER_KERNEL_TO_ENABLE
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + activate_jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local service_username
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: ++ set -x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + readonly PROJECT_ID=gcp-bigquery-project1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: + PROJECT_ID=gcp-bigquery-project1
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: + MASTER_HOSTNAMES=(${DATAPROC_MASTER} ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: ++ get_metadata_cluster_name
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: ++ is_caching_enabled=false
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ is_earlyoom_enabled
<13>Feb 20 20:44:37 startup-script[1178]: ++ [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:37 startup-script[1178]: ++ get_dataproc_property_or_default internal.node.main.memory-protection.enabled false
<13>Feb 20 20:44:37 startup-script[1178]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_service_username
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_metadata_cluster_name
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ get_metadata_master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-hive-metastore[3890]: +++ set +x
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ echo root
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + service_username=root
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + generate_notebook_location
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local -r default_notebook_dir=gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: + local notebook_dir
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ get_dataproc_property_or_default jupyter.notebook.gcs.dir gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:37 startup-script[1178]: <13>Feb 20 20:44:37 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + notebook_dir=gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + notebook_dir=gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: ++ cut -d / -f 1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: ++ echo gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + NOTEBOOK_BUCKET=gcp-bigquery-project1-bucket
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + NOTEBOOK_PATH=notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + check_gcs_bucket_access gs://gcp-bigquery-project1-bucket/notebooks/jupyter jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + local -r gcs_path=gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + local -r component=jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + [[ gs://gcp-bigquery-project1-bucket/notebooks/jupyter == \g\s\:\/\/* ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + local bucket
<13>Feb 20 20:44:38 startup-script[1178]: true
<13>Feb 20 20:44:38 startup-script[1178]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/optional_logging/config.d/dataproc-earlyoom.conf /etc/google-fluentd/config.d
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: ++ sed -E 's/(gs:\/\/[^/]+).*/\1/'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: ++ echo gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + bucket=gs://gcp-bigquery-project1-bucket
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: + local message
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-jupyter[3894]: ++ env BOTO_CONFIG= gsutil ls gs://gcp-bigquery-project1-bucket/
<13>Feb 20 20:44:38 startup-script[1178]: ++ logging_agent_service_name
<13>Feb 20 20:44:38 startup-script[1178]: ++ echo google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: ++ logging_agent_service_name
<13>Feb 20 20:44:38 startup-script[1178]: ++ echo google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: + local -r pid=4707
<13>Feb 20 20:44:38 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:38 startup-script[1178]: + shift 2
<13>Feb 20 20:44:38 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/4707.running ]]
<13>Feb 20 20:44:38 startup-script[1178]: + echo 'setup_service google-fluentd'
<13>Feb 20 20:44:38 startup-script[1178]: + echo 'Started background process [setup_service google-fluentd] as pid 4707'
<13>Feb 20 20:44:38 startup-script[1178]: Started background process [setup_service google-fluentd] as pid 4707
<13>Feb 20 20:44:38 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:38 startup-script[1178]: + [[ europe-west3 != \g\l\o\b\a\l ]]
<13>Feb 20 20:44:38 startup-script[1178]: + add_regional_bigtop_repo europe-west3
<13>Feb 20 20:44:38 startup-script[1178]: + local -r region=europe-west3
<13>Feb 20 20:44:38 startup-script[1178]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:38 startup-script[1178]: + is_test_bigtop_repo /etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:38 startup-script[1178]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:38 startup-script[1178]: + return '!' grep -q dataproc-bigtop-repo /etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:38 startup-script[1178]: /usr/local/share/google/dataproc/bdutil/os/shared.sh: line 8: return: !: numeric argument required
<13>Feb 20 20:44:38 startup-script[1178]: + local regional_bigtop_repo_uri
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + activate_hdfs
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + mkdir -p /var/run/hadoop-hdfs
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:38 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:38 startup-script[1178]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: + local tag=
<13>Feb 20 20:44:38 startup-script[1178]: + local pid=4707
<13>Feb 20 20:44:38 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:38 startup-script[1178]: + tag=setup-google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: + shift 2
<13>Feb 20 20:44:38 startup-script[1178]: + [[ 2 -eq 0 ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + chown root:hdfs /var/run/hadoop-hdfs
<13>Feb 20 20:44:38 startup-script[1178]: + setup_service google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: ++ cat /etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:38 startup-script[1178]: ++ cut -d ' ' -f 2
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ get_metadata_master_additional
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: ++ sed s#dataproc-bigtop-repo#goog-dataproc-bigtop-repo-europe-west3#
<13>Feb 20 20:44:38 startup-script[1178]: ++ head -1
<13>Feb 20 20:44:38 startup-script[1178]: ++ grep 'deb .*goog-dataproc-bigtop-repo-europe-west3.* dataproc contrib'
<13>Feb 20 20:44:38 startup-script[1178]: ++ logger -s -t 'setup-google-fluentd[4707]'
<13>Feb 20 20:44:38 startup-script[1178]: + regional_bigtop_repo_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-europe-west3/2_0_deb10_20240130_122020-RC01
<13>Feb 20 20:44:38 startup-script[1178]: + [[ https://storage.googleapis.com/goog-dataproc-bigtop-repo-europe-west3/2_0_deb10_20240130_122020-RC01 == */ ]]
<13>Feb 20 20:44:38 startup-script[1178]: + local -r bigtop_key_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-europe-west3/2_0_deb10_20240130_122020-RC01/archive.key
<13>Feb 20 20:44:38 startup-script[1178]: + curl -fsS --retry-connrefused --retry 3 --retry-delay 5 https://storage.googleapis.com/goog-dataproc-bigtop-repo-europe-west3/2_0_deb10_20240130_122020-RC01/archive.key
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r service=google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + enable_service google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r service=google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r unit=google-fluentd.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + retry_constant_short systemctl enable google-fluentd.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + retry_constant_custom 30 1 systemctl enable google-fluentd.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 setup-google-fluentd[4707]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + chmod 775 /var/run/hadoop-hdfs
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + start_master_services
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + login_as_hdfs_if_kerberos_enabled
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + is_component_selected kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local -r component=kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local activated_components
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: ++ get_components_to_activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: + apt-key add -
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + loginfo 'Merged /tmp/cluster/properties/knox.xml.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + echo 'Merged /tmp/cluster/properties/knox.xml.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: Merged /tmp/cluster/properties/knox.xml.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + generate_topology
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + mv knox/topology-template.xml /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + [[ 1 == \1 ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + start_hdfs_namenode
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + case "${MASTER_INDEX?}" in
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ NUM_MASTERS=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + loginfo 'Formatting NameNode'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local master_hostname
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + echo 'Formatting NameNode'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: Formatting NameNode
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + retry_constant_short su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_metadata_master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ get_metadata_role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: +++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + retry_constant_custom 30 1 su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hdfs[3887]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + readonly CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + CLUSTER_NAME=project1-cluster
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_metadata_dataproc_region
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + master_hostname=project1-cluster-m
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local scheme=http
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local host=localhost
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local apphistory_port=8188
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local yarn_ats_v2_port=8192
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local sparkhistory_port=18080
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local hdfs_port=9870
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local nodeui_port=8042
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local jobhistory_port=19888
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local yarn_port=8088
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local presto_port=8060
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local trino_port=8060
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local hiveserver2ui_port=10002
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local flinkhistory_port=8500
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + is_component_selected kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r component=kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local activated_components
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_components_to_activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + readonly REGION_NAME=europe-west3
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + REGION_NAME=europe-west3
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + readonly SIGNOUT_URL=https://europe-west3.dataproc.cloud.google.com/_signout
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + SIGNOUT_URL=https://europe-west3.dataproc.cloud.google.com/_signout
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + readonly BANNER_HTML=/opt/dataproc/proxy-agent/banner.html
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + BANNER_HTML=/opt/dataproc/proxy-agent/banner.html
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + mkdir -p /opt/dataproc/proxy-agent/
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + cat
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + INJECT_BANNER_FLAG=
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + INJECT_BANNER_FLAG='-banner-height=40px -inject-banner="$(cat /opt/dataproc/proxy-agent/banner.html)"'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + AFTER='local-fs.target network-online.target'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + is_ubuntu
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ ROLE=Master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ [[ 1 -gt 1 ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + set -x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ os_id
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ xargs
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ get_metadata_role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ cut -d= -f2
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ debian == \u\b\u\n\t\u ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r topology_conf=/usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_MASTER_HOSTNAME_0}/project1-cluster-m/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + (( 1 > 1 ))
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_SCHEME_}/http/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_HOST_}/localhost/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local jupyter_port
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_property_or_default jupyter.port 8123
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + readonly IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + REQUIRES_LIST=Requires=
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + AFTER_LIST='After=google-guest-agent.service google-startup-scripts.service'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + FORCE_HTTP2_FLAG=
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + should_start_spark_connect
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_metadata_role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + ROLE=Master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + jupyter_port=8123
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i s/JUPYTER-PORT/8123/g /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ get_metadata_master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local zeppelin_port
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_property_or_default zeppelin.port 8080
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + zeppelin_port=8080
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i s/ZEPPELIN-PORT/8080/g /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_APPHISTORY_PORT_}/8188/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_APPTIMELINE_V2_PORT_}/8192/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_SPARKHISTORY_PORT_}/18080/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + DATAPROC_MASTER=project1-cluster-m
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + activate_hive_metastore
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + wait_for_mysql
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + wait_for_port mysql project1-cluster-m 3306
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r name=mysql
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r host=project1-cluster-m
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r port=3306
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r timeout=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r capped_timeout=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + loginfo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=3306 name=mysql.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + echo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=3306 name=mysql.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: Waiting 300 seconds for service to come up on host=project1-cluster-m port=3306 name=mysql.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + retry_constant_custom 300 1 nc -v -z -w 1 project1-cluster-m 3306
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r max_retry_time=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_FLINKHISTORY_PORT_}/8500/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r max_retries=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: About to run 'nc -v -z -w 1 project1-cluster-m 3306' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: Warning: apt-key output should not be parsed (stdout is not a terminal)
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: Connection to project1-cluster-m 3306 port [tcp/mysql] succeeded!
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 3306' succeeded after 1 execution(s).
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + return 0
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + loginfo 'Service up on host=project1-cluster-m port=3306 name=mysql.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + echo 'Service up on host=project1-cluster-m port=3306 name=mysql.'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: Service up on host=project1-cluster-m port=3306 name=mysql.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + start_hive_metastore
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + enable_service hive-metastore
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r service=hive-metastore
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r unit=hive-metastore.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + retry_constant_short systemctl enable hive-metastore.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + retry_constant_custom 30 1 systemctl enable hive-metastore.service
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_HDFS_PORT_}/9870/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: Created symlink /etc/systemd/system/multi-user.target.wants/earlyoom.service → /etc/systemd/system/earlyoom.service.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-hive-metastore[3890]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: Created symlink /etc/systemd/system/multi-user.target.wants/google-osconfig-agent.service → /lib/systemd/system/google-osconfig-agent.service.
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + role=Master
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local spark_connect_enabled
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ get_java_property /tmp/cluster/properties/dataproc.properties internal.s8s.spark-connect.enabled
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_NODEUI_PORT_}/8042/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_JOBHISTORY_PORT_}/19888/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_YARN_PORT_}/8088/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + spark_connect_enabled=
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ Master == \M\a\s\t\e\r ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + cat
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + retry_constant systemctl daemon-reload
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + retry_constant_custom 300 1 systemctl daemon-reload
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local -r max_retry_time=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_PRESTO_PORT_}/8060/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local -r max_retries=300
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-proxy-agent[3915]: About to run 'systemctl daemon-reload' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_TRINO_PORT_}/8060/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + sed -i -e 's/{_HIVESERVER2UI_PORT_}/10002/g' /usr/lib/knox/conf/topologies/default.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + is_component_selected kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r component=kerberos
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local activated_components
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_components_to_activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *kerberos* ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + set_websocket_size
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local max_ws_size=1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + is_component_selected zeppelin
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r component=zeppelin
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local activated_components
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_components_to_activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *zeppelin* ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + set_property_in_xml /usr/lib/knox/conf/gateway-site.xml gateway.websocket.max.binary.buffer.size 1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r xml_file=/usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r name=gateway.websocket.max.binary.buffer.size
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r value=1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r mode=overwrite
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r delimiter=,
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local skip=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local old_value
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local new_value
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + new_value=1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + bdconfig set_property --configuration_file /usr/lib/knox/conf/gateway-site.xml --name gateway.websocket.max.binary.buffer.size --value 1024000 --create_if_absent --clobber
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: 'systemctl enable earlyoom.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: + return 0
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: + local -r drop_in_dir=/etc/systemd/system/earlyoom.service.d
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: + mkdir -p /etc/systemd/system/earlyoom.service.d
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: + local props
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ retry_constant_short systemctl show earlyoom.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ retry_constant_custom 30 1 systemctl show earlyoom.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-earlyoom[3883]: About to run 'systemctl show earlyoom.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + set_property_in_xml /usr/lib/knox/conf/gateway-site.xml gateway.websocket.max.text.buffer.size 1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r xml_file=/usr/lib/knox/conf/gateway-site.xml
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r name=gateway.websocket.max.text.buffer.size
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r value=1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r mode=overwrite
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r delimiter=,
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local skip=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local old_value
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local new_value
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + new_value=1024000
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + bdconfig set_property --configuration_file /usr/lib/knox/conf/gateway-site.xml --name gateway.websocket.max.text.buffer.size --value 1024000 --create_if_absent --clobber
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: 'systemctl enable google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: + return 0
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: + local -r drop_in_dir=/etc/systemd/system/google-osconfig-agent.service.d
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: + mkdir -p /etc/systemd/system/google-osconfig-agent.service.d
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: + local props
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ retry_constant_short systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ retry_constant_custom 30 1 systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: ++ set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 start-osconfig-service[4246]: About to run 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + generate_runfiles_and_start
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ -L /usr/lib/knox/pids ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + rm /usr/lib/knox/pids
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + mkdir /usr/lib/knox/pids
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + retry_constant_short /usr/lib/knox/bin/knoxcli.sh create-master --generate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + retry_constant_custom 30 1 /usr/lib/knox/bin/knoxcli.sh create-master --generate
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r max_retry_time=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r retry_delay=1
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + cmd=("${@:3}")
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r cmd
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local -r max_retries=30
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + local reenable_x=false
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + [[ -o xtrace ]]
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: + set +x
<13>Feb 20 20:44:38 startup-script[1178]: <13>Feb 20 20:44:38 activate-component-knox[3896]: About to run '/usr/lib/knox/bin/knoxcli.sh create-master --generate' with retries...
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + enable_and_start_service npd
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r service=npd
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + enable_service npd
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r service=npd
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r unit=npd.service
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + retry_constant_short systemctl enable npd.service
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + retry_constant_custom 30 1 systemctl enable npd.service
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r max_retry_time=30
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r retry_delay=1
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + cmd=("${@:3}")
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r cmd
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local -r max_retries=30
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + local reenable_x=false
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + [[ -o xtrace ]]
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: + set +x
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 activate-component-npd[3913]: About to run 'systemctl enable npd.service' with retries...
<13>Feb 20 20:44:39 startup-script[1178]: OK
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Adding regional Bigtop repo for europe-west3 in APT sources.'
<13>Feb 20 20:44:39 startup-script[1178]: Adding regional Bigtop repo for europe-west3 in APT sources.
<13>Feb 20 20:44:39 startup-script[1178]: + cat
<13>Feb 20 20:44:39 startup-script[1178]: + cat /etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:39 startup-script[1178]: + mv -f /tmp/dataproc.list /etc/apt/sources.list.d/dataproc.list
<13>Feb 20 20:44:39 startup-script[1178]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:39 startup-script[1178]: + run_in_background --tag backup-original-configs backup_original_configs
<13>Feb 20 20:44:39 startup-script[1178]: + local -r pid=5535
<13>Feb 20 20:44:39 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:39 startup-script[1178]: + shift 2
<13>Feb 20 20:44:39 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/5535.running ]]
<13>Feb 20 20:44:39 startup-script[1178]: + echo backup_original_configs
<13>Feb 20 20:44:39 startup-script[1178]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Started background process [backup_original_configs] as pid 5535'
<13>Feb 20 20:44:39 startup-script[1178]: Started background process [backup_original_configs] as pid 5535
<13>Feb 20 20:44:39 startup-script[1178]: + run_with_logger --tag backup-original-configs backup_original_configs
<13>Feb 20 20:44:39 startup-script[1178]: + wait_on_async_processes
<13>Feb 20 20:44:39 startup-script[1178]: + loginfo 'Waiting on async processes'
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Waiting on async processes'
<13>Feb 20 20:44:39 startup-script[1178]: Waiting on async processes
<13>Feb 20 20:44:39 startup-script[1178]: + local tag=
<13>Feb 20 20:44:39 startup-script[1178]: + local running_file
<13>Feb 20 20:44:39 startup-script[1178]: + local pid=5535
<13>Feb 20 20:44:39 startup-script[1178]: + [[ --tag == \-\-\t\a\g ]]
<13>Feb 20 20:44:39 startup-script[1178]: + tag=backup-original-configs
<13>Feb 20 20:44:39 startup-script[1178]: + shift 2
<13>Feb 20 20:44:39 startup-script[1178]: + [[ 1 -eq 0 ]]
<13>Feb 20 20:44:39 startup-script[1178]: + backup_original_configs
<13>Feb 20 20:44:39 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:39 startup-script[1178]: + local pid
<13>Feb 20 20:44:39 startup-script[1178]: ++ logger -s -t 'backup-original-configs[5535]'
<13>Feb 20 20:44:39 startup-script[1178]: ++ basename /tmp/dataproc/commands/3432
<13>Feb 20 20:44:39 startup-script[1178]: + pid=3432
<13>Feb 20 20:44:39 startup-script[1178]: + local cmd
<13>Feb 20 20:44:39 startup-script[1178]: + cmd=create_event_log_dir
<13>Feb 20 20:44:39 startup-script[1178]: + loginfo 'Waiting on pid=3432 cmd=[create_event_log_dir]'
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Waiting on pid=3432 cmd=[create_event_log_dir]'
<13>Feb 20 20:44:39 startup-script[1178]: Waiting on pid=3432 cmd=[create_event_log_dir]
<13>Feb 20 20:44:39 startup-script[1178]: + echo create_event_log_dir
<13>Feb 20 20:44:39 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3432.exitcode
<13>Feb 20 20:44:39 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3432.exitcode ]]
<13>Feb 20 20:44:39 startup-script[1178]: + local status
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + local -r HIVE_CONF_DIR=/etc/hive/conf
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + local -r TEZ_CONF_DIR=/etc/tez/conf
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + mkdir -p /usr/local/share/google/dataproc/conf/original
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hadoop/conf/yarn-site.xml /usr/local/share/google/dataproc/conf/original/original-yarn-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hadoop/conf/hdfs-site.xml /usr/local/share/google/dataproc/conf/original/original-hdfs-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hadoop/conf/core-site.xml /usr/local/share/google/dataproc/conf/original/original-core-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hadoop/conf/mapred-site.xml /usr/local/share/google/dataproc/conf/original/original-mapred-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: + status=0
<13>Feb 20 20:44:39 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Command cmd=[create_event_log_dir] pid=3432 exited with 0'
<13>Feb 20 20:44:39 startup-script[1178]: + tee /tmp/dataproc/commands/3432.done
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hadoop/conf/capacity-scheduler.xml /usr/local/share/google/dataproc/conf/original/original-capacity-scheduler.xml
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/hive/conf/hive-site.xml /usr/local/share/google/dataproc/conf/original/original-hive-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: Command cmd=[create_event_log_dir] pid=3432 exited with 0
<13>Feb 20 20:44:39 startup-script[1178]: + rm /tmp/dataproc/commands/3432.exitcode /tmp/dataproc/commands/3432.running
<13>Feb 20 20:44:39 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:39 startup-script[1178]: + local pid
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/tez/conf/tez-site.xml /usr/local/share/google/dataproc/conf/original/original-tez-site.xml
<13>Feb 20 20:44:39 startup-script[1178]: ++ basename /tmp/dataproc/commands/3868
<13>Feb 20 20:44:39 startup-script[1178]: <13>Feb 20 20:44:39 backup-original-configs[5535]: + cp /etc/spark/conf/spark-defaults.conf /usr/local/share/google/dataproc/conf/original/original-spark-defaults.conf
<13>Feb 20 20:44:39 startup-script[1178]: + pid=3868
<13>Feb 20 20:44:39 startup-script[1178]: + local cmd
<13>Feb 20 20:44:39 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:39 startup-script[1178]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Feb 20 20:44:39 startup-script[1178]: + loginfo 'Waiting on pid=3868 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'Waiting on pid=3868 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Feb 20 20:44:39 startup-script[1178]: Waiting on pid=3868 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Feb 20 20:44:39 startup-script[1178]: + echo 'setup_service hadoop-yarn-resourcemanager'
<13>Feb 20 20:44:39 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3868.exitcode
<13>Feb 20 20:44:39 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:39 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: 'systemctl daemon-reload' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + retry_constant systemctl enable google-dataproc-component-gateway
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + retry_constant_custom 300 1 systemctl enable google-dataproc-component-gateway
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + local -r max_retry_time=300
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + local -r max_retries=300
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: + set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-proxy-agent[3915]: About to run 'systemctl enable google-dataproc-component-gateway' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: 'systemctl show earlyoom.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: ++ return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + props='Restart=always
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: RemainAfterExit=no'
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ earlyoom != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ earlyoom != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ Restart=always
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ earlyoom == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ earlyoom == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + start_service earlyoom
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r service=earlyoom
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r unit=earlyoom.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + retry_constant_short systemctl start earlyoom.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + retry_constant_custom 30 1 systemctl start earlyoom.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: + set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-earlyoom[3883]: About to run 'systemctl start earlyoom.service' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: ++ return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + props='Restart=always
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: RemainAfterExit=no'
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ Restart=always
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + start_service google-osconfig-agent
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r service=google-osconfig-agent
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r unit=google-osconfig-agent.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + retry_constant_short systemctl start google-osconfig-agent.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + retry_constant_custom 30 1 systemctl start google-osconfig-agent.service
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: + set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 start-osconfig-service[4246]: About to run 'systemctl start google-osconfig-agent.service' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: + message='gs://gcp-bigquery-project1-bucket/.ipynb_checkpoints/
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: gs://gcp-bigquery-project1-bucket/data/
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: gs://gcp-bigquery-project1-bucket/notebooks/'
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-jupyter[3894]: + hadoop fs -mkdir -p gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: 'systemctl enable hadoop-mapreduce-historyserver.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: + return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: + local -r drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: + mkdir -p /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: + local props
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ retry_constant_short systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ retry_constant_custom 30 1 systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: ++ set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-mapreduce-historyserver[3870]: About to run 'systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: 'systemctl enable hadoop-yarn-timelineserver.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: + return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: Created symlink /etc/systemd/system/multi-user.target.wants/npd.service → /etc/systemd/system/npd.service.
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: + mkdir -p /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: + local props
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ retry_constant_short systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: ++ set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-hadoop-yarn-timelineserver[3873]: About to run 'systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: 'systemctl enable stackdriver-agent.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: + return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: + local -r drop_in_dir=/etc/systemd/system/stackdriver-agent.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: + mkdir -p /etc/systemd/system/stackdriver-agent.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: + local props
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ retry_constant_short systemctl show stackdriver-agent.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ retry_constant_custom 30 1 systemctl show stackdriver-agent.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: ++ set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 setup-stackdriver-agent[3877]: About to run 'systemctl show stackdriver-agent.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: 'systemctl enable npd.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: + return 0
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: + local -r drop_in_dir=/etc/systemd/system/npd.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: + mkdir -p /etc/systemd/system/npd.service.d
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: + local props
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ retry_constant_short systemctl show npd.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ retry_constant_custom 30 1 systemctl show npd.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ local -r retry_delay=1
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ local -r cmd
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ local -r max_retries=30
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ local reenable_x=false
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: ++ set +x
<13>Feb 20 20:44:40 startup-script[1178]: <13>Feb 20 20:44:40 activate-component-npd[3913]: About to run 'systemctl show npd.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:40 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:40 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: 'systemctl enable hadoop-yarn-nodemanager.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: + return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: + mkdir -p /etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: + local props
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ retry_constant_short systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ local -r max_retries=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: ++ set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-nodemanager[3874]: About to run 'systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: 'systemctl enable hadoop-yarn-resourcemanager.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: + return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: + mkdir -p /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: + local props
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ retry_constant_short systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ local -r max_retries=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: ++ set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-resourcemanager[3868]: About to run 'systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: 'systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: ++ return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + props='Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: RemainAfterExit=no'
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: 'systemctl show stackdriver-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: ++ return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + props='Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: RemainAfterExit=yes'
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ Restart=no
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-component-gateway.service → /lib/systemd/system/google-dataproc-component-gateway.service.
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: ++ set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: ++ set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local master_run_driver_location
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local master_run_driver_location
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + start_service stackdriver-agent
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r service=stackdriver-agent
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r unit=stackdriver-agent.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + retry_constant_short systemctl start stackdriver-agent.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + retry_constant_custom 30 1 systemctl start stackdriver-agent.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r max_retry_time=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local -r max_retries=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: + set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-stackdriver-agent[3877]: About to run 'systemctl start stackdriver-agent.service' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + start_service hadoop-yarn-timelineserver
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r service=hadoop-yarn-timelineserver
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r unit=hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + retry_constant_short systemctl start hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + retry_constant_custom 30 1 systemctl start hadoop-yarn-timelineserver.service
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r max_retry_time=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local -r max_retries=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: + set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-hadoop-yarn-timelineserver[3873]: About to run 'systemctl start hadoop-yarn-timelineserver.service' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: 'systemctl enable google-fluentd.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: + return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: + local -r drop_in_dir=/etc/systemd/system/google-fluentd.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: + mkdir -p /etc/systemd/system/google-fluentd.service.d
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: + local props
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ retry_constant_short systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ retry_constant_custom 30 1 systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ local -r max_retries=30
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: ++ set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 setup-google-fluentd[4707]: About to run 'systemctl show google-fluentd.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: 'systemctl enable google-dataproc-component-gateway' succeeded after 1 execution(s).
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + return 0
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + retry_constant systemctl start google-dataproc-component-gateway
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + retry_constant_custom 300 1 systemctl start google-dataproc-component-gateway
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + local -r max_retry_time=300
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + local -r retry_delay=1
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + cmd=("${@:3}")
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + local -r cmd
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + local -r max_retries=300
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + local reenable_x=false
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + [[ -o xtrace ]]
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: + set +x
<13>Feb 20 20:44:41 startup-script[1178]: <13>Feb 20 20:44:41 activate-component-proxy-agent[3915]: About to run 'systemctl start google-dataproc-component-gateway' with retries...
<13>Feb 20 20:44:41 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:41 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: 'systemctl enable hive-metastore.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + mkdir -p /etc/systemd/system/hive-metastore.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: 'systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + props='Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: RemainAfterExit=no'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: 'systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + props='Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: RemainAfterExit=no'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ get_metadata_worker_count
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local props
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ retry_constant_short systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ retry_constant_custom 30 1 systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: 'systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: About to run 'systemctl show hive-metastore.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + props='Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: RemainAfterExit=no'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: 'systemctl show npd.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + props='Restart=on-failure
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: RemainAfterExit=no'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ npd != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ npd != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ Restart=on-failure
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ npd == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ npd == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + start_service npd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r service=npd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: 'systemctl show google-fluentd.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r unit=npd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + retry_constant_short systemctl start npd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + retry_constant_custom 30 1 systemctl start npd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + props='Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: RemainAfterExit=yes'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: About to run 'systemctl start npd.service' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: 'systemctl show hive-metastore.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: ++ return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + props='Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: RemainAfterExit=no'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ hive-metastore != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ hive-metastore != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ Restart=no
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + retry_constant systemctl restart hive-metastore
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + retry_constant_custom 300 1 systemctl restart hive-metastore
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r max_retry_time=300
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local -r max_retries=300
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: About to run 'systemctl restart hive-metastore' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: 'systemctl start google-dataproc-component-gateway' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local master_run_driver_location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 start-osconfig-service[4246]: 'systemctl start google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 start-osconfig-service[4246]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: 'systemctl start earlyoom.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: ++ date +%s.%N
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: ++ date +%s.%N
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-hive-metastore[3890]: Warning: The unit file, source configuration file or drop-ins of hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: 'systemctl start npd.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + local -r end=1708461882.135356132
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + local -r runtime_s=5
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + echo 'Component proxy-agent took 5s to activate'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: Component proxy-agent took 5s to activate
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + local -r time_file=/tmp/dataproc/components/activate/proxy-agent.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + touch /tmp/dataproc/components/activate/proxy-agent.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local master_run_driver_location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local master_run_driver_location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: ++ date +%s.%N
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + cat
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r end=1708461882.163828300
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r runtime_s=5
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + echo 'Component npd took 5s to activate'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: Component npd took 5s to activate
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + local -r time_file=/tmp/dataproc/components/activate/npd.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + touch /tmp/dataproc/components/activate/npd.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + local -r end=1708461882.171440396
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + local -r runtime_s=5
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + echo 'Component earlyoom took 5s to activate'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: Component earlyoom took 5s to activate
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + local -r time_file=/tmp/dataproc/components/activate/earlyoom.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + touch /tmp/dataproc/components/activate/earlyoom.time
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + cat
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + cat
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/earlyoom.sh
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-earlyoom[3883]: + touch /tmp/dataproc/components/activate/earlyoom.done
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/npd.sh
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-npd[3913]: + touch /tmp/dataproc/components/activate/npd.done
<13>Feb 20 20:44:42 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh'
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/proxy-agent.sh
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 activate-component-proxy-agent[3915]: + touch /tmp/dataproc/components/activate/proxy-agent.done
<13>Feb 20 20:44:42 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:42 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r worker_count=0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ 0 != 0 ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + ln -s -f /etc/systemd/system/common/worker-restart.conf /etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local am_on_primary_worker_enabled
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ get_dataproc_property am.primary_only
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + start_service google-fluentd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r service=google-fluentd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r unit=google-fluentd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + retry_constant_short systemctl start google-fluentd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + retry_constant_custom 30 1 systemctl start google-fluentd.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-google-fluentd[4707]: About to run 'systemctl start google-fluentd.service' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ LOCAL == \Y\A\R\N ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + start_service hadoop-yarn-resourcemanager
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r service=hadoop-yarn-resourcemanager
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r unit=hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + retry_constant_short systemctl start hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + retry_constant_custom 30 1 systemctl start hadoop-yarn-resourcemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: About to run 'systemctl start hadoop-yarn-resourcemanager.service' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + am_on_primary_worker_enabled=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local master_run_driver_location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-resourcemanager[3868]: Warning: The unit file, source configuration file or drop-ins of hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + start_service hadoop-mapreduce-historyserver
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r service=hadoop-mapreduce-historyserver
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r unit=hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + retry_constant_short systemctl start hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + retry_constant_custom 30 1 systemctl start hadoop-mapreduce-historyserver.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: About to run 'systemctl start hadoop-mapreduce-historyserver.service' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-stackdriver-agent[3877]: 'systemctl start stackdriver-agent.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-stackdriver-agent[3877]: + return 0
<13>Feb 20 20:44:42 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + master_run_driver_location=LOCAL
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + start_service hadoop-yarn-nodemanager
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r service=hadoop-yarn-nodemanager
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r unit=hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + retry_constant_short systemctl start hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + retry_constant_custom 30 1 systemctl start hadoop-yarn-nodemanager.service
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r max_retry_time=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r retry_delay=1
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + cmd=("${@:3}")
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-mapreduce-historyserver[3870]: Warning: The unit file, source configuration file or drop-ins of hadoop-mapreduce-historyserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r cmd
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local -r max_retries=30
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + local reenable_x=false
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + [[ -o xtrace ]]
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: + set +x
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: About to run 'systemctl start hadoop-yarn-nodemanager.service' with retries...
<13>Feb 20 20:44:42 startup-script[1178]: <13>Feb 20 20:44:42 setup-hadoop-yarn-nodemanager[3874]: Warning: The unit file, source configuration file or drop-ins of hadoop-yarn-nodemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:44:42 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:42 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:43 startup-script[1178]: <13>Feb 20 20:44:43 activate-component-hdfs[3887]: WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
<13>Feb 20 20:44:43 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:43 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: Master secret has been persisted to disk.
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: '/usr/lib/knox/bin/knoxcli.sh create-master --generate' succeeded after 1 execution(s).
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: + return 0
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: + chown knox:knox /usr/lib/knox/data/security/master
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: + chmod 400 /usr/lib/knox/data/security/master
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: + chown -R -L knox:knox /usr/lib/knox/pids /usr/lib/knox/conf /usr/lib/knox/data/security
<13>Feb 20 20:44:44 startup-script[1178]: <13>Feb 20 20:44:44 activate-component-knox[3896]: + systemctl daemon-reload
<13>Feb 20 20:44:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:44 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + retry_constant_short systemctl enable knox
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + retry_constant_custom 30 1 systemctl enable knox
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r max_retry_time=30
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r retry_delay=1
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + cmd=("${@:3}")
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r cmd
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r max_retries=30
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local reenable_x=false
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + [[ -o xtrace ]]
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + set +x
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: About to run 'systemctl enable knox' with retries...
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: Created symlink /etc/systemd/system/multi-user.target.wants/knox.service → /lib/systemd/system/knox.service.
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: 'systemctl enable knox' succeeded after 1 execution(s).
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + return 0
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + retry_constant_short systemctl start knox
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + retry_constant_custom 30 1 systemctl start knox
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r max_retry_time=30
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r retry_delay=1
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + cmd=("${@:3}")
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r cmd
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local -r max_retries=30
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + local reenable_x=false
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + [[ -o xtrace ]]
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: + set +x
<13>Feb 20 20:44:45 startup-script[1178]: <13>Feb 20 20:44:45 activate-component-knox[3896]: About to run 'systemctl start knox' with retries...
<13>Feb 20 20:44:45 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:45 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 2024-02-20 20:44:46,238 INFO namenode.NameNode: STARTUP_MSG: 
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: /************************************************************
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG: Starting NameNode
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   host = project1-cluster-m/10.156.0.5
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   version = 3.2.3
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/commons-text-1.10.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/jetty-io-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/guava-28.2-jre.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/javax.activation-api-1.2.0.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/opentracing-api-0.33.0.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-9.8.1.jar:/usr/lib/hadoop/lib/jettison-1.5.4.jar:/usr/lib/hadoop/lib/opentracing-util-0.33.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.35.jar:/u
<13>Feb 20 20:44:46 startup-script[1178]: sr/lib/hadoop/lib/jackson-mapper-asl-1.9.1
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 3.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.8.0.jar:/usr/lib/hadoop/lib/json-smart-2.4.7.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/ini4j-0.5.4.jar:/usr/lib/hadoop/lib/avro-1.9.2.jar:/usr/lib/hadoop/lib/kafka-clients-2.8.2.jar:/usr/lib/hadoop/lib/azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop/lib/azure-storage-7.0.0.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/httpcore-4.4.13.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.2.jar:/usr/lib/hadoop/lib/error_prone_annotations-2.3.4.jar:/usr/lib/hadoop/lib/jdom2-2.0.6.jar:/usr/lib/hadoop/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity
<13>Feb 20 20:44:46 startup-script[1178]: -1.0.1.jar:/usr/lib/hadoop/lib/listenablef
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: uture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/checker-qual-2.10.0.jar:/usr/lib/hadoop/lib/zstd-jni-1.4.9-1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.10.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/ojalgo-43.0.jar:/usr/lib/hadoop/lib/commons-compress-1.21.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:
<13>Feb 20 20:44:46 startup-script[1178]: /usr/lib/hadoop/lib/failureaccess-1.0.1.ja
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: r:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.35.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/aliyun-sdk-oss-3.13.0.jar:/usr/lib/hadoop/lib/wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop/lib/jetty-security-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/lz4-java-1.7.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/aws-java-sdk-bundle-1.11.1034.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.1.jar:/usr/lib/hadoop/lib/jetty-server-9.4.40.v20210413.jar:/usr
<13>Feb 20 20:44:46 startup-script[1178]: /lib/hadoop/lib/accessors-smart-2.4.7.jar:
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: /usr/lib/hadoop/lib/slf4j-reload4j-1.7.35.jar:/usr/lib/hadoop/lib/reload4j-1.2.18.3.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/org.jacoco.agent-0.8.5-runtime.jar:/usr/lib/hadoop/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-ram-3.1.0.jar:/usr/lib/hadoop/lib/jsch-0.1.55.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-kms-2.11.0.jar:/usr/lib/hadoop/lib/aliyun-java-sdk-core-4.5.10.jar:/usr/lib/hadoop/lib/opentracing-noop-0.33.0.jar:/usr/lib/hadoop/lib/jetty-http-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/snappy-java-1.1.10.4.jar:/usr/lib/hadoop/lib/htrace-cor
<13>Feb 20 20:44:46 startup-script[1178]: e4-4.1.0-incubating.jar:/usr/lib/hadoop/li
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: b/jetty-util-9.4.40.v20210413.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.3.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.3.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.3.jar:/usr/lib/hadoop/.//hadoop-common-3.2.3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.3.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.3.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.3.jar:/usr/
<13>Feb 20 20:44:46 startup-script[1178]: lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/h
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: adoop/.//hadoop-aws-3.2.3.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.3.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.3.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.3.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common-3.2.3-tests.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.3.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.3.jar:/usr/lib/hadoop/.//hadoop-resourc
<13>Feb 20 20:44:46 startup-script[1178]: eestimator.jar:/usr/lib/hadoop/.//hadoop-r
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: umen.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.10.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.86.Final-osx-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-proxy-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-rxtx-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-28.2-jre.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/javax.activation-api-1.2.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-kqueue-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-hdfs/lib/javax.
<13>Feb 20 20:44:46 startup-script[1178]: servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: /lib/netty-resolver-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.86.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-native-macos-4.1.86.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.5.4.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-sctp-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.4.7.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/lib/avro-1.9.2.jar:/usr/lib/hadoop
<13>Feb 20 20:44:46 startup-script[1178]: -hdfs/lib/curator-client-2.13.0.jar:/usr/l
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: ib/hadoop-hdfs/lib/netty-handler-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-ssl-ocsp-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-epoll-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.13.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.2.jar:/usr/lib/hadoop-hdfs/lib/error_prone_annotations-2.3.4.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-smtp-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-h
<13>Feb 20 20:44:46 startup-script[1178]: dfs/lib/jersey-server-1.19.jar:/usr/lib/ha
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: doop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/checker-qual-2.10.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.10.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-xml-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-dns-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.21.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.86.Final-linux-x86_64.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-redis-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/li
<13>Feb 20 20:44:46 startup-script[1178]: b/netty-codec-memcache-4.1.86.Final.jar:/u
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: sr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/failureaccess-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-udt-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-classes-macos-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-mqtt-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-socks-4.1.86.Final.jar
<13>Feb 20 20:44:46 startup-script[1178]: :/usr/lib/hadoop-hdfs/lib/commons-math3-3.
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-buffer-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-native-macos-4.1.86.Final-osx-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-haproxy-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-2.4.7.jar:/usr/lib/hadoop-hdfs/lib/reload4j-1.2.18.3.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib
<13>Feb 20 20:44:46 startup-script[1178]: /netty-codec-http-4.1.86.Final.jar:/usr/li
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: b/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-stomp-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/j2objc-annotations-1.3.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.55.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-common-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.10.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-http2-4.1.86.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.86.Final-linux-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/
<13>Feb 20 20:44:46 startup-script[1178]: lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduc
<13>Feb 20 20:44:46 startup-script[1178]: e/.//hadoop-mapreduce-client-common-3.2.3.
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-c
<13>Feb 20 20:44:46 startup-script[1178]: lient-hs-3.2.3.jar:/usr/lib/hadoop-mapredu
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: ce/.//hadoop-mapreduce-examples-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/jakart
<13>Feb 20 20:44:46 startup-script[1178]: a.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-y
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: arn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-2.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-
<13>Feb 20 20:44:46 startup-script[1178]: applications-distributedshell.jar:/usr/lib
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: /hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/lib/hadoop-ya
<13>Feb 20 20:44:46 startup-script[1178]: rn/.//hadoop-yarn-common.jar:/usr/lib/hado
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: op-yarn/.//hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/local/share/google/dataproc/lib/gcs-connector-hadoop3-2.2.19.jar:/usr/local/share/google/dataproc/lib/gcs-connector.jar:/usr/local/share/google/dataproc/lib/ranger_gcs_plugin_client.jar:/usr/local/share/google/dataproc/
<13>Feb 20 20:44:46 startup-script[1178]: lib/spark-metrics-listener-dataproc-2.0-1.
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 8.0.jar:/usr/local/share/google/dataproc/lib/spark-metrics-listener.jar
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 5df651ed036fc78ed422a6389ea38987e1db4563; compiled by 'bigtop' on 2024-01-30T21:06Z
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: STARTUP_MSG:   java = 1.8.0_402
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: ************************************************************/
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hdfs[3887]: 2024-02-20 20:44:46,327 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: 'systemctl restart hive-metastore' succeeded after 1 execution(s).
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: + return 0
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: + wait_for_hive_metastore
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: + local timeout
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-metastore 300
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: ++ set +x
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: + timeout=300
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: + local metastore_uri
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: ++ get_hive_metastore_uri
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: ++ local uris_str
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: +++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.uris
<13>Feb 20 20:44:46 startup-script[1178]: <13>Feb 20 20:44:46 activate-component-hive-metastore[3890]: +++ set +x
<13>Feb 20 20:44:46 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:46 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hdfs[3887]: 2024-02-20 20:44:47,064 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Feb 20 20:44:47 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 setup-google-fluentd[4707]: 'systemctl start google-fluentd.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 setup-google-fluentd[4707]: + return 0
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ uris_str=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ uris=(${uris_str//,/' '})
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ local -a uris
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ for uri in "${uris[@]}"
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ [[ thrift://project1-cluster-m:9083 == *project1-cluster-m* ]]
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ echo thrift://project1-cluster-m:9083
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ return 0
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: + metastore_uri=thrift://project1-cluster-m:9083
<13>Feb 20 20:44:47 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:47 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: + local host
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ sed -n 's#.*://\(.*\):.*#\1#p'
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ echo thrift://project1-cluster-m:9083
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: + host=project1-cluster-m
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: + [[ -z project1-cluster-m ]]
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: + local port
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ sed -n 's#.*://.*:\(.*\)#\1#p'
<13>Feb 20 20:44:47 startup-script[1178]: <13>Feb 20 20:44:47 activate-component-hive-metastore[3890]: ++ echo thrift://project1-cluster-m:9083
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + port=9083
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + [[ -z 9083 ]]
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + wait_for_port hive-metastore project1-cluster-m 9083 300
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r name=hive-metastore
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r host=project1-cluster-m
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r port=9083
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r timeout=300
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r capped_timeout=300
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + loginfo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=9083 name=hive-metastore.'
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + echo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=9083 name=hive-metastore.'
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: Waiting 300 seconds for service to come up on host=project1-cluster-m port=9083 name=hive-metastore.
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + retry_constant_custom 300 1 nc -v -z -w 1 project1-cluster-m 9083
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r max_retry_time=300
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r retry_delay=1
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + cmd=("${@:3}")
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r cmd
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local -r max_retries=300
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + local reenable_x=false
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + [[ -o xtrace ]]
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: + set +x
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: About to run 'nc -v -z -w 1 project1-cluster-m 9083' with retries...
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:48 startup-script[1178]: <13>Feb 20 20:44:48 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 9083' attempt 1/300 failed! Sleeping 1s.
<13>Feb 20 20:44:48 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:48 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: Feb 20, 2024 8:44:48 PM com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics updateMinMaxStats
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: INFO: Detected potential high latency for operation op_get_file_status. latencyMs=869; previousMaxLatencyMs=0; operationCount=1; context=gs://gcp-bigquery-project1-bucket/notebooks/jupyter
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: Feb 20, 2024 8:44:49 PM com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics updateMinMaxStats
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: INFO: Detected potential high latency for operation op_glob_status. latencyMs=1231; previousMaxLatencyMs=0; operationCount=1; context=path=gs://gcp-bigquery-project1-bucket/notebooks/jupyter; pattern=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase$$Lambda$8/811301908@7756c3cd
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + configure_jupyter
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local jupyter_port
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ get_dataproc_property_or_default jupyter.port 8123
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + jupyter_port=8123
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local -r symlink_dir=/etc/jupyter/symlinks_for_jupyterlab_widgets
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + mkdir -p /etc/jupyter/symlinks_for_jupyterlab_widgets
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + ln -s / '/etc/jupyter/symlinks_for_jupyterlab_widgets/Local Disk'
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + echo PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + is_component_selected knox
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local -r component=knox
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local activated_components
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ get_components_to_activate
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *knox* ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ '' != \t\r\u\e ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local cluster_ui_hostname
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ sed 's/\\:/:/'
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ get_dataproc_property dataproc.proxy.public.hostname
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + cluster_ui_hostname=https://r6r2ldsze5dpzj4vu4syltssoq-dot-europe-west3.dataproc.googleusercontent.com
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local cluster_ui_hostname_with_no_protocol
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + cluster_ui_hostname_with_no_protocol=r6r2ldsze5dpzj4vu4syltssoq-dot-europe-west3.dataproc.googleusercontent.com
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ get_dataproc_property jupyter.listen.all.interfaces
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + case ${compare_versions_result} in
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + echo 'c.NotebookApp.ip = '\''127.0.0.1'\'''
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + install_libraries
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + case ${compare_versions_result} in
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + create_ipython_profile
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/ipython profile create
<13>Feb 20 20:44:49 startup-script[1178]: <13>Feb 20 20:44:49 activate-component-hdfs[3887]: 2024-02-20T20:44:49.554+0000: 6.010: [GC (Allocation Failure) 2024-02-20T20:44:49.554+0000: 6.010: [ParNew: 68864K->7223K(77440K), 0.0187149 secs] 68864K->7223K(249472K), 0.0188176 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] 
<13>Feb 20 20:44:50 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:50 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:50 startup-script[1178]: <13>Feb 20 20:44:50 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:50 startup-script[1178]: <13>Feb 20 20:44:50 setup-hadoop-mapreduce-historyserver[3870]: 'systemctl start hadoop-mapreduce-historyserver.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:50 startup-script[1178]: <13>Feb 20 20:44:50 setup-hadoop-mapreduce-historyserver[3870]: + return 0
<13>Feb 20 20:44:50 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:51 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-nodemanager[3874]: 'systemctl start hadoop-yarn-nodemanager.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-nodemanager[3874]: + return 0
<13>Feb 20 20:44:51 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:51 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:51 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-timelineserver[3873]: 'systemctl start hadoop-yarn-timelineserver.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-timelineserver[3873]: + return 0
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:51 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-resourcemanager[3868]: 'systemctl start hadoop-yarn-resourcemanager.service' succeeded after 1 execution(s).
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 setup-hadoop-yarn-resourcemanager[3868]: + return 0
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 activate-component-hdfs[3887]: 2024-02-20 20:44:51,485 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 activate-component-hdfs[3887]: 2024-02-20 20:44:51,486 INFO common.Util: Assuming 'file' scheme for path /hadoop/dfs/name in configuration.
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 activate-component-hdfs[3887]: Formatting using clusterid: CID-6e871ef6-804f-44b4-8767-bd0e59b85ad8
<13>Feb 20 20:44:51 startup-script[1178]: <13>Feb 20 20:44:51 activate-component-hdfs[3887]: 2024-02-20 20:44:51,853 INFO namenode.FSEditLog: Edit logging is async:true
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3868.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=3868 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,045 INFO namenode.FSNamesystem: KeyProvider: null
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3868.done
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[setup_service hadoop-yarn-resourcemanager] pid=3868 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3868.exitcode /tmp/dataproc/commands/3868.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3870
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3870
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3870 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3870 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3870 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'setup_service hadoop-mapreduce-historyserver'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3870.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3870.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,061 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,066 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3870.done
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=3870 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[setup_service hadoop-mapreduce-historyserver] pid=3870 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3870.exitcode /tmp/dataproc/commands/3870.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3873
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,100 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,100 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,100 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,100 INFO namenode.FSNamesystem: HA Enabled: false
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3873
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3873 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3873 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3873 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'setup_service hadoop-yarn-timelineserver'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3873.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3873.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[setup_service hadoop-yarn-timelineserver] pid=3873 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3873.done
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[setup_service hadoop-yarn-timelineserver] pid=3873 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3873.exitcode /tmp/dataproc/commands/3873.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3874
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3874
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='setup_service hadoop-yarn-nodemanager'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3874 cmd=[setup_service hadoop-yarn-nodemanager]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3874 cmd=[setup_service hadoop-yarn-nodemanager]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3874 cmd=[setup_service hadoop-yarn-nodemanager]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'setup_service hadoop-yarn-nodemanager'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3874.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3874.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3874.done
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[setup_service hadoop-yarn-nodemanager] pid=3874 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[setup_service hadoop-yarn-nodemanager] pid=3874 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3874.exitcode /tmp/dataproc/commands/3874.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3877
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3877
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='setup_service stackdriver-agent'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3877 cmd=[setup_service stackdriver-agent]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3877 cmd=[setup_service stackdriver-agent]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3877 cmd=[setup_service stackdriver-agent]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'setup_service stackdriver-agent'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3877.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3877.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3877.done
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[setup_service stackdriver-agent] pid=3877 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[setup_service stackdriver-agent] pid=3877 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3877.exitcode /tmp/dataproc/commands/3877.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3883
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3883
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='activate_component earlyoom'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3883 cmd=[activate_component earlyoom]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3883 cmd=[activate_component earlyoom]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3883 cmd=[activate_component earlyoom]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'activate_component earlyoom'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3883.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3883.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + local status
<13>Feb 20 20:44:52 startup-script[1178]: + status=0
<13>Feb 20 20:44:52 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Command cmd=[activate_component earlyoom] pid=3883 exited with 0'
<13>Feb 20 20:44:52 startup-script[1178]: + tee /tmp/dataproc/commands/3883.done
<13>Feb 20 20:44:52 startup-script[1178]: Command cmd=[activate_component earlyoom] pid=3883 exited with 0
<13>Feb 20 20:44:52 startup-script[1178]: + rm /tmp/dataproc/commands/3883.exitcode /tmp/dataproc/commands/3883.running
<13>Feb 20 20:44:52 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:44:52 startup-script[1178]: + local pid
<13>Feb 20 20:44:52 startup-script[1178]: ++ basename /tmp/dataproc/commands/3887
<13>Feb 20 20:44:52 startup-script[1178]: + pid=3887
<13>Feb 20 20:44:52 startup-script[1178]: + local cmd
<13>Feb 20 20:44:52 startup-script[1178]: + cmd='activate_component hdfs'
<13>Feb 20 20:44:52 startup-script[1178]: + loginfo 'Waiting on pid=3887 cmd=[activate_component hdfs]'
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'Waiting on pid=3887 cmd=[activate_component hdfs]'
<13>Feb 20 20:44:52 startup-script[1178]: Waiting on pid=3887 cmd=[activate_component hdfs]
<13>Feb 20 20:44:52 startup-script[1178]: + echo 'activate_component hdfs'
<13>Feb 20 20:44:52 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3887.exitcode
<13>Feb 20 20:44:52 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:52 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20 20:44:52,303 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-jupyter[3894]: [ProfileCreate] Generating default config file: PosixPath('/root/.ipython/profile_default/ipython_config.py')
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-jupyter[3894]: [ProfileCreate] Generating default config file: PosixPath('/root/.ipython/profile_default/ipython_kernel_config.py')
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-hdfs[3887]: 2024-02-20T20:44:52.539+0000: 8.995: [GC (Allocation Failure) 2024-02-20T20:44:52.540+0000: 8.995: [ParNew: 76087K->6529K(77440K), 0.0976200 secs] 76087K->9750K(249472K), 0.0976849 secs] [Times: user=0.06 sys=0.00, real=0.10 secs] 
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-jupyter[3894]: + local ipython_profile_location
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:52 startup-script[1178]: <13>Feb 20 20:44:52 activate-component-jupyter[3894]: ++ /opt/conda/miniconda3/bin/ipython profile locate default
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:53 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:53 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + ipython_profile_location=/root/.ipython/profile_default
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + echo 'c.InteractiveShellApp.extensions.append('\''google.cloud.bigquery'\'')'
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + enable_jupyter_extensions
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + local -r nbextension_subcomand=nbextension
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + local -r serverextension_subcomand=serverextension
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter nbextension enable widgetsnbextension --py --system
<13>Feb 20 20:44:53 startup-script[1178]: <13>Feb 20 20:44:53 activate-component-hdfs[3887]: 2024-02-20T20:44:53.851+0000: 10.306: [GC (Allocation Failure) 2024-02-20T20:44:53.851+0000: 10.306: [ParNew: 75393K->3378K(77440K), 0.0332944 secs] 78614K->8954K(249472K), 0.0333616 secs] [Times: user=0.02 sys=0.00, real=0.04 secs] 
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:54 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:54 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: 'systemctl start knox' succeeded after 1 execution(s).
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + return 0
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: ++ date +%s.%N
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + local -r end=1708461894.296304564
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + local -r runtime_s=17
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + echo 'Component knox took 17s to activate'
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: Component knox took 17s to activate
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + local -r time_file=/tmp/dataproc/components/activate/knox.time
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + touch /tmp/dataproc/components/activate/knox.time
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + cat
<13>Feb 20 20:44:54 startup-script[1178]: ++ echo 0
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh'
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/knox.sh
<13>Feb 20 20:44:54 startup-script[1178]: <13>Feb 20 20:44:54 activate-component-knox[3896]: + touch /tmp/dataproc/components/activate/knox.done
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:55 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:55 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.553+0000: 12.009: [GC (Allocation Failure) 2024-02-20T20:44:55.553+0000: 12.009: [ParNew: 72242K->3944K(77440K), 0.0343111 secs] 77818K->9519K(249472K), 0.0343642 secs] [Times: user=0.02 sys=0.00, real=0.04 secs] 
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.588+0000: 12.043: [GC (CMS Initial Mark) [1 CMS-initial-mark: 5575K(172032K)] 10880K(249472K), 0.0023490 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.590+0000: 12.046: [CMS-concurrent-mark-start]
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.693+0000: 12.149: [CMS-concurrent-mark: 0.103/0.103 secs] [Times: user=0.07 sys=0.00, real=0.10 secs] 
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.703+0000: 12.159: [CMS-concurrent-preclean-start]
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.704+0000: 12.160: [CMS-concurrent-preclean: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-hdfs[3887]: 2024-02-20T20:44:55.704+0000: 12.160: [CMS-concurrent-abortable-preclean-start]
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-jupyter[3894]: 2024-02-20 20:44:55 INFO: EnableNBExtensionApp Enabling notebook extension jupyter-js-widgets/extension...
<13>Feb 20 20:44:55 startup-script[1178]: <13>Feb 20 20:44:55 activate-component-jupyter[3894]: 2024-02-20 20:44:55 INFO: EnableNBExtensionApp       - Validating: [32mOK[0m
<13>Feb 20 20:44:56 startup-script[1178]: <13>Feb 20 20:44:56 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter nbextension install nbdime --py --system --symlink
<13>Feb 20 20:44:56 startup-script[1178]: <13>Feb 20 20:44:56 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:56 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:56 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]: 2024-02-20 20:44:57 INFO: InstallNBExtensionApp Installing /opt/conda/miniconda3/lib/python3.8/site-packages/nbdime/notebook_ext -> nbdime
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]: 2024-02-20 20:44:57 INFO: InstallNBExtensionApp - Validating: [32mOK[0m
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]: 2024-02-20 20:44:57 INFO: InstallNBExtensionApp 
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]:     To initialize this nbextension in the browser every time the notebook (or other app) loads:
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]:     
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]:           jupyter nbextension enable nbdime --py
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]:     
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.239+0000: 13.695: [CMS-concurrent-abortable-preclean: 0.234/1.535 secs] [Times: user=0.85 sys=0.01, real=1.53 secs] 
<13>Feb 20 20:44:57 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:57 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter nbextension enable nbdime --py --system
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.258+0000: 13.714: [GC (CMS Final Remark) [YG occupancy: 42486 K (77440 K)]2024-02-20T20:44:57.258+0000: 13.714: [Rescan (parallel) , 0.0358897 secs]2024-02-20T20:44:57.294+0000: 13.750: [weak refs processing, 0.0000683 secs]2024-02-20T20:44:57.294+0000: 13.750: [class unloading, 0.0088701 secs]2024-02-20T20:44:57.303+0000: 13.759: [scrub symbol table, 0.0203357 secs]2024-02-20T20:44:57.323+0000: 13.779: [scrub string table, 0.0005633 secs][1 CMS-remark: 5575K(172032K)] 48062K(249472K), 0.0659425 secs] [Times: user=0.03 sys=0.00, real=0.07 secs] 
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.324+0000: 13.780: [CMS-concurrent-sweep-start]
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.327+0000: 13.783: [CMS-concurrent-sweep: 0.003/0.003 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.327+0000: 13.783: [CMS-concurrent-reset-start]
<13>Feb 20 20:44:57 startup-script[1178]: <13>Feb 20 20:44:57 activate-component-hdfs[3887]: 2024-02-20T20:44:57.566+0000: 14.022: [CMS-concurrent-reset: 0.203/0.239 secs] [Times: user=0.08 sys=0.04, real=0.24 secs] 
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: 2024-02-20 20:44:58 INFO: EnableNBExtensionApp Enabling notebook extension nbdime/index...
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: 2024-02-20 20:44:58 INFO: EnableNBExtensionApp       - Validating: [32mOK[0m
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-hdfs[3887]: 2024-02-20T20:44:58.231+0000: 14.690: [GC (Allocation Failure) 2024-02-20T20:44:58.234+0000: 14.690: [ParNew: 72808K->3947K(77440K), 0.0195851 secs] 78327K->9467K(249472K), 0.0226567 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] 
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:58 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:58 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter serverextension enable nbdime --py --system
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: Enabling: nbdime
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: - Writing config: /usr/local/etc/jupyter
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]:     - Validating...
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]:       nbdime 3.0.0 [32mOK[0m
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:44:58 startup-script[1178]: <13>Feb 20 20:44:58 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter serverextension enable jupyter_http_over_ws --py --system
<13>Feb 20 20:44:59 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:44:59 startup-script[1178]: + sleep 1
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 9083' attempt 12/300 failed! Sleeping 1s.
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: Enabling: jupyter_http_over_ws
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: - Writing config: /usr/local/etc/jupyter
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]:     - Validating...
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]:       jupyter_http_over_ws 0.0.7 [32mOK[0m
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + is_version_at_least 2.0 2.0
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + case ${compare_versions_result} in
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + configure_kernels
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + echo 'Creating kernelspec'
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: Creating kernelspec
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: ++ dirname /opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + mkdir -p /opt/conda/miniconda3/share/jupyter/kernels/pyspark
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: ++ dirname /opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + chmod a+r /opt/conda/miniconda3/share/jupyter/kernels/pyspark
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + local -r python=/opt/conda/miniconda3/bin/python
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + echo 'Generating /opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json'
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: Generating /opt/conda/miniconda3/share/jupyter/kernels/pyspark/kernel.json
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter kernelspec list
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-jupyter[3894]: + grep kernels/pyspark
<13>Feb 20 20:44:59 startup-script[1178]: <13>Feb 20 20:44:59 activate-component-hdfs[3887]: 2024-02-20T20:44:59.852+0000: 16.308: [GC (Allocation Failure) 2024-02-20T20:44:59.852+0000: 16.308: [ParNew: 72811K->4668K(77440K), 0.0228474 secs] 78331K->10188K(249472K), 0.0229098 secs] [Times: user=0.02 sys=0.00, real=0.03 secs] 
<13>Feb 20 20:45:00 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:00 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:00 startup-script[1178]: <13>Feb 20 20:45:00 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:01 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:01 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]:   pyspark    /opt/conda/miniconda3/share/jupyter/kernels/pyspark
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + local python_kernel_dir=/opt/conda/miniconda3/share/jupyter/kernels/python3
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + mkdir -p /opt/conda/miniconda3/share/jupyter/kernels/python3
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + local python_kernelspec
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: ++ ls /opt/conda/miniconda3/share/jupyter/kernels/python3/kernel.json
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + python_kernelspec=/opt/conda/miniconda3/share/jupyter/kernels/python3/kernel.json
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + echo 'Generating /opt/conda/miniconda3/share/jupyter/kernels/python3/kernel.json'
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: Generating /opt/conda/miniconda3/share/jupyter/kernels/python3/kernel.json
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter kernelspec list
<13>Feb 20 20:45:01 startup-script[1178]: <13>Feb 20 20:45:01 activate-component-jupyter[3894]: + grep kernels/python
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: Feb 20, 2024 8:45:01 PM com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics updateMinMaxStats
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: INFO: Detected potential high latency for operation op_open. latencyMs=1845; previousMaxLatencyMs=0; operationCount=1; context=gs://gcp-bigquery-project1-bucket/google-cloud-dataproc-metainfo/a6df27d4-edf3-467e-95d1-fc6408be3b22/nodes_include
<13>Feb 20 20:45:02 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:02 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,737 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,751 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,751 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,785 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,785 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Feb 20 20:45:02
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,815 INFO util.GSet: Computing capacity for map BlocksMap
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,834 INFO util.GSet: VM type       = 64-bit
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,852 INFO util.GSet: 2.0% max memory 3.1 GB = 63.4 MB
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,852 INFO util.GSet: capacity      = 2^23 = 8388608 entries
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20T20:45:02.867+0000: 19.322: [GC (Allocation Failure) 2024-02-20T20:45:02.867+0000: 19.322: [ParNew: 67693K->5247K(77440K), 0.0201142 secs] 73213K->10767K(249472K), 0.0201834 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] 
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]:   python3    /opt/conda/miniconda3/share/jupyter/kernels/python3
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,928 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,929 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + case ${compare_versions_result} in
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + return 1
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/python -m spylon_kernel install --sys-prefix
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,974 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,974 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,974 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: defaultReplication         = 1
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: maxReplicationStreams      = 20
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Feb 20 20:45:02 startup-script[1178]: <13>Feb 20 20:45:02 activate-component-hdfs[3887]: 2024-02-20 20:45:02,975 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,281 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,281 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,281 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,281 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
<13>Feb 20 20:45:03 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:03 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,409 INFO util.GSet: Computing capacity for map INodeMap
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,409 INFO util.GSet: VM type       = 64-bit
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,409 INFO util.GSet: 1.0% max memory 3.1 GB = 31.7 MB
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,411 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,425 INFO namenode.FSDirectory: ACLs enabled? false
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,425 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,425 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,430 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,479 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,493 INFO snapshot.SnapshotManager: SkipList is disabled
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,527 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,527 INFO util.GSet: VM type       = 64-bit
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,527 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,530 INFO util.GSet: capacity      = 2^20 = 1048576 entries
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20T20:45:03.563+0000: 20.019: [GC (Allocation Failure) 2024-02-20T20:45:03.563+0000: 20.019: [ParNew: 74111K->8575K(77440K), 0.4117863 secs] 79631K->64061K(249472K), 0.4118587 secs] [Times: user=0.19 sys=0.02, real=0.41 secs] 
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20T20:45:03.975+0000: 20.431: [GC (CMS Initial Mark) [1 CMS-initial-mark: 55485K(172032K)] 65424K(249472K), 0.0104426 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20T20:45:03.985+0000: 20.441: [CMS-concurrent-mark-start]
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,993 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,993 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Feb 20 20:45:03 startup-script[1178]: <13>Feb 20 20:45:03 activate-component-hdfs[3887]: 2024-02-20 20:45:03,993 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,021 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,021 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,033 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,033 INFO util.GSet: VM type       = 64-bit
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,038 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 973.4 KB
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,038 INFO util.GSet: capacity      = 2^17 = 131072 entries
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20T20:45:04.177+0000: 20.637: [CMS-concurrent-mark: 0.192/0.192 secs] [Times: user=0.10 sys=0.01, real=0.19 secs] 
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20T20:45:04.193+0000: 20.648: [CMS-concurrent-preclean-start]
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20T20:45:04.196+0000: 20.652: [CMS-concurrent-preclean: 0.004/0.004 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20T20:45:04.196+0000: 20.652: [CMS-concurrent-abortable-preclean-start]
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,252 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1009272182-10.156.0.5-1708461904160
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,314 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Feb 20 20:45:04 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:04 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:04 startup-script[1178]: <13>Feb 20 20:45:04 activate-component-hdfs[3887]: 2024-02-20 20:45:04,591 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Feb 20 20:45:05 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:05 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:05 startup-script[1178]: <13>Feb 20 20:45:05 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:05 startup-script[1178]: <13>Feb 20 20:45:05 activate-component-hdfs[3887]: 2024-02-20 20:45:05,736 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 395 bytes saved in 1 seconds .
<13>Feb 20 20:45:05 startup-script[1178]: <13>Feb 20 20:45:05 activate-component-hdfs[3887]: 2024-02-20 20:45:05,767 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-jupyter[3894]: [InstallKernelSpec] Installed kernelspec spylon-kernel in /opt/conda/miniconda3/share/jupyter/kernels/spylon-kernel
<13>Feb 20 20:45:06 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:06 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: 2024-02-20 20:45:06,382 INFO namenode.FSNamesystem: Stopping services started for active state
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: 2024-02-20 20:45:06,382 INFO namenode.FSNamesystem: Stopping services started for standby state
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: 2024-02-20 20:45:06,470 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: 2024-02-20 20:45:06,484 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: /************************************************************
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: SHUTDOWN_MSG: Shutting down NameNode at project1-cluster-m/10.156.0.5
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: ************************************************************/
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: Heap
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:  par new generation   total 77440K, used 46625K [0x00000006f7e00000, 0x00000006fd200000, 0x000000070cac0000)
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:   eden space 68864K,  55% used [0x00000006f7e00000, 0x00000006fa328620, 0x00000006fc140000)
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:   from space 8576K,  99% used [0x00000006fc140000, 0x00000006fc99fff8, 0x00000006fc9a0000)
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:   to   space 8576K,   0% used [0x00000006fc9a0000, 0x00000006fc9a0000, 0x00000006fd200000)
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:  concurrent mark-sweep generation total 172032K, used 55485K [0x000000070cac0000, 0x00000007172c0000, 0x00000007c0000000)
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:  Metaspace       used 31897K, capacity 32552K, committed 32912K, reserved 1079296K
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]:   class space    used 3569K, capacity 3690K, committed 3756K, reserved 1048576K
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&           hdfs namenode -format -nonInteractive' succeeded after 1 execution(s).
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + enable_and_start_service hadoop-hdfs-namenode
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-namenode
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + enable_service hadoop-hdfs-namenode
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-namenode
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-namenode.service
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + retry_constant_short systemctl enable hadoop-hdfs-namenode.service
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-namenode.service
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-hdfs[3887]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter kernelspec list
<13>Feb 20 20:45:06 startup-script[1178]: <13>Feb 20 20:45:06 activate-component-jupyter[3894]: + grep kernels/spylon
<13>Feb 20 20:45:07 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:07 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: 'systemctl enable hadoop-hdfs-namenode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + mkdir -p /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local props
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ retry_constant_short systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ local -r max_retry_time=30
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ local -r retry_delay=1
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ cmd=("${@:3}")
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ local -r cmd
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ local -r max_retries=30
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ local reenable_x=false
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ [[ -o xtrace ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: About to run 'systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: 'systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: ++ return 0
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + props='Restart=no
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: RemainAfterExit=no'
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ hadoop-hdfs-namenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ Restart=no
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ Restart=no
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + start_service hadoop-hdfs-namenode
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-namenode
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-namenode.service
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + retry_constant_short systemctl start hadoop-hdfs-namenode.service
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-namenode.service
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: About to run 'systemctl start hadoop-hdfs-namenode.service' with retries...
<13>Feb 20 20:45:07 startup-script[1178]: <13>Feb 20 20:45:07 activate-component-hdfs[3887]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:45:08 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:08 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:08 startup-script[1178]: <13>Feb 20 20:45:08 activate-component-jupyter[3894]:   spylon-kernel    /opt/conda/miniconda3/share/jupyter/kernels/spylon-kernel
<13>Feb 20 20:45:08 startup-script[1178]: <13>Feb 20 20:45:08 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter kernelspec list
<13>Feb 20 20:45:08 startup-script[1178]: <13>Feb 20 20:45:08 activate-component-jupyter[3894]: + grep kernels/ir
<13>Feb 20 20:45:08 startup-script[1178]: <13>Feb 20 20:45:08 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:09 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:09 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]:   ir               /opt/conda/miniconda3/share/jupyter/kernels/ir
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + echo 'Generating /opt/conda/miniconda3/share/jupyter/kernels/ir/kernel.json'
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: Generating /opt/conda/miniconda3/share/jupyter/kernels/ir/kernel.json
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + set_dir_permissions root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local -r service_username=root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + [[ root != \r\o\o\t ]]
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + generate_systemd_unit_and_start root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local -r service_username=root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + configure_default_workspace root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local -r jupyter_username=root
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local -r workspaces_subdir=.jupyter/lab/workspaces
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local -r root_workspaces_dir=/root/.jupyter/lab/workspaces
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + mkdir -p /root/.jupyter/lab/workspaces
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + rm -f '/root/.jupyter/lab/workspaces/*'
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local workspace_file
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: ++ mktemp
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + workspace_file=/tmp/tmp.i7jVoNygBU
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local metadata_id=lab
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + is_version_at_least 2.0 2.1
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + case ${compare_versions_result} in
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + return 1
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + metadata_id=/lab
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-jupyter[3894]: + /opt/conda/miniconda3/bin/jupyter lab workspaces import /tmp/tmp.i7jVoNygBU
<13>Feb 20 20:45:09 startup-script[1178]: <13>Feb 20 20:45:09 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:10 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:10 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:10 startup-script[1178]: <13>Feb 20 20:45:10 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:10 startup-script[1178]: <13>Feb 20 20:45:10 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 9083' attempt 23/300 failed! Sleeping 1s.
<13>Feb 20 20:45:11 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:11 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: Saved workspace: /root/.jupyter/lab/workspaces/lab-a511.jupyterlab-workspace
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + rm /tmp/tmp.i7jVoNygBU
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + cp /root/.jupyter/lab/workspaces/lab-a511.jupyterlab-workspace /root/.jupyter/lab/workspaces/default-37a8.jupyterlab-workspace
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + sed -i -e 's/\/lab/default/g' /root/.jupyter/lab/workspaces/default-37a8.jupyterlab-workspace
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + [[ root != \r\o\o\t ]]
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + [[ '' == \t\r\u\e ]]
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local exec_start
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + exec_start='/bin/bash -c '\''/opt/conda/miniconda3/bin/jupyter notebook &>> /var/log/jupyter_notebook.log'\'''
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + echo 'Generating /usr/lib/systemd/system/jupyter.service'
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: Generating /usr/lib/systemd/system/jupyter.service
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + retry_constant systemctl daemon-reload
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + retry_constant_custom 300 1 systemctl daemon-reload
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local -r max_retry_time=300
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local -r retry_delay=1
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + cmd=("${@:3}")
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local -r cmd
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local -r max_retries=300
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:11 startup-script[1178]: <13>Feb 20 20:45:11 activate-component-jupyter[3894]: About to run 'systemctl daemon-reload' with retries...
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: 'systemctl daemon-reload' succeeded after 1 execution(s).
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant systemctl enable jupyter
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant_custom 300 1 systemctl enable jupyter
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retry_time=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r retry_delay=1
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + cmd=("${@:3}")
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r cmd
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retries=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: About to run 'systemctl enable jupyter' with retries...
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: Created symlink /etc/systemd/system/multi-user.target.wants/jupyter.service → /lib/systemd/system/jupyter.service.
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: 'systemctl enable jupyter' succeeded after 1 execution(s).
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant systemctl start jupyter
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant_custom 300 1 systemctl start jupyter
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retry_time=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r retry_delay=1
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + cmd=("${@:3}")
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r cmd
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retries=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: About to run 'systemctl start jupyter' with retries...
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: 'systemctl start jupyter' succeeded after 1 execution(s).
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + wait_for_jupyter_server
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local jupyter_port
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local base_path
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + is_component_selected knox
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r component=knox
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local activated_components
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ get_components_to_activate
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ tr '[:upper:]' '[:lower:]'
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ get_dataproc_property dataproc.components.activate
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:45:12 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:12 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + activated_components='jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3'
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ jupyter knox proxy-agent hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom npd miniconda3 == *knox* ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ '' != \t\r\u\e ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ get_dataproc_property_or_default jupyter.port 8123
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: ++ set +x
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + jupyter_port=8123
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + base_path=gateway/default/jupyter/
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant is_kernelspecs_available 8123 gateway/default/jupyter/
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + retry_constant_custom 300 1 is_kernelspecs_available 8123 gateway/default/jupyter/
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retry_time=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r retry_delay=1
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + cmd=("${@:3}")
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r cmd
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local -r max_retries=300
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + local reenable_x=false
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + [[ -o xtrace ]]
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: + set +x
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: About to run 'is_kernelspecs_available 8123 gateway/default/jupyter/' with retries...
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-jupyter[3894]: 'is_kernelspecs_available 8123 gateway/default/jupyter/' attempt 1/300 failed! Sleeping 1s.
<13>Feb 20 20:45:12 startup-script[1178]: <13>Feb 20 20:45:12 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:13 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:13 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:13 startup-script[1178]: <13>Feb 20 20:45:13 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:14 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:14 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:14 startup-script[1178]: <13>Feb 20 20:45:14 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: true
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: 'is_kernelspecs_available 8123 gateway/default/jupyter/' succeeded after 3 execution(s).
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + return 0
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: ++ date +%s.%N
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + local -r end=1708461915.125472247
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + local -r runtime_s=38
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + echo 'Component jupyter took 38s to activate'
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: Component jupyter took 38s to activate
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + local -r time_file=/tmp/dataproc/components/activate/jupyter.time
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + touch /tmp/dataproc/components/activate/jupyter.time
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + cat
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh'
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/jupyter.sh
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-jupyter[3894]: + touch /tmp/dataproc/components/activate/jupyter.done
<13>Feb 20 20:45:15 startup-script[1178]: ++ echo 0
<13>Feb 20 20:45:15 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:15 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: 'systemctl start hadoop-hdfs-namenode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + start_hdfs_secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + enable_and_start_service hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + enable_service hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + retry_constant_short systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hdfs[3887]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:15 startup-script[1178]: <13>Feb 20 20:45:15 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:16 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:16 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: 'systemctl enable hadoop-hdfs-secondarynamenode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + mkdir -p /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local props
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ retry_constant_short systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ local -r max_retry_time=30
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ local -r retry_delay=1
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ cmd=("${@:3}")
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ local -r cmd
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ local -r max_retries=30
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ local reenable_x=false
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ [[ -o xtrace ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: About to run 'systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: 'systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: ++ return 0
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + props='Restart=no
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: RemainAfterExit=no'
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ hadoop-hdfs-secondarynamenode != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ Restart=no
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ Restart=no
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + start_service hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-secondarynamenode
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + retry_constant_short systemctl start hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-secondarynamenode.service
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: About to run 'systemctl start hadoop-hdfs-secondarynamenode.service' with retries...
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hdfs[3887]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:45:16 startup-script[1178]: <13>Feb 20 20:45:16 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:17 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:17 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:17 startup-script[1178]: <13>Feb 20 20:45:17 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:18 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:18 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:18 startup-script[1178]: <13>Feb 20 20:45:18 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:19 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:19 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:19 startup-script[1178]: <13>Feb 20 20:45:19 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:20 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:20 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:20 startup-script[1178]: <13>Feb 20 20:45:20 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:21 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:21 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:21 startup-script[1178]: <13>Feb 20 20:45:21 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:21 startup-script[1178]: <13>Feb 20 20:45:21 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 9083' attempt 34/300 failed! Sleeping 1s.
<13>Feb 20 20:45:22 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:22 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:22 startup-script[1178]: <13>Feb 20 20:45:22 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:23 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:23 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:23 startup-script[1178]: <13>Feb 20 20:45:23 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: 'systemctl start hadoop-hdfs-secondarynamenode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + init_hcfs_dirs
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + is_in_cluster_hdfs hdfs://project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local uri=hdfs://project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + [[ hdfs://project1-cluster-m != hdfs://* ]]
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local host_name
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + host_name=project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + host_name=project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + host_name=project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + [[ project1-cluster-m == \p\r\o\j\e\c\t\1\-\c\l\u\s\t\e\r\-\m ]]
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + init_internal_hdfs
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + wait_for_hdfs project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r hostname=project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + loginfo 'Waiting for NameNode to listen on RPC port'
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + echo 'Waiting for NameNode to listen on RPC port'
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: Waiting for NameNode to listen on RPC port
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local namenode_port_binding_timeout
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hadoop-hdfs-namenode 300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + namenode_port_binding_timeout=300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + wait_for_port hadoop-hdfs-namenode project1-cluster-m 8020 300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r name=hadoop-hdfs-namenode
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r host=project1-cluster-m
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r port=8020
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r timeout=300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r capped_timeout=300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + loginfo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.'
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + echo 'Waiting 300 seconds for service to come up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.'
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: Waiting 300 seconds for service to come up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + retry_constant_custom 300 1 nc -v -z -w 1 project1-cluster-m 8020
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r max_retry_time=300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local -r max_retries=300
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: About to run 'nc -v -z -w 1 project1-cluster-m 8020' with retries...
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: nc: connect to project1-cluster-m port 8020 (tcp) failed: Connection refused
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hdfs[3887]: 'nc -v -z -w 1 project1-cluster-m 8020' attempt 1/300 failed! Sleeping 1s.
<13>Feb 20 20:45:24 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:24 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:24 startup-script[1178]: <13>Feb 20 20:45:24 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:25 startup-script[1178]: <13>Feb 20 20:45:25 activate-component-hdfs[3887]: nc: connect to project1-cluster-m port 8020 (tcp) failed: Connection refused
<13>Feb 20 20:45:25 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:25 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:25 startup-script[1178]: <13>Feb 20 20:45:25 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:26 startup-script[1178]: <13>Feb 20 20:45:26 activate-component-hdfs[3887]: nc: connect to project1-cluster-m port 8020 (tcp) failed: Connection refused
<13>Feb 20 20:45:26 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:26 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:26 startup-script[1178]: <13>Feb 20 20:45:26 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:27 startup-script[1178]: <13>Feb 20 20:45:27 activate-component-hdfs[3887]: nc: connect to project1-cluster-m port 8020 (tcp) failed: Connection refused
<13>Feb 20 20:45:27 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:27 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:27 startup-script[1178]: <13>Feb 20 20:45:27 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:28 startup-script[1178]: <13>Feb 20 20:45:28 activate-component-hdfs[3887]: nc: connect to project1-cluster-m port 8020 (tcp) failed: Connection refused
<13>Feb 20 20:45:28 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:28 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:28 startup-script[1178]: <13>Feb 20 20:45:28 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:29 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:29 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: Connection to project1-cluster-m 8020 port [tcp/*] succeeded!
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: 'nc -v -z -w 1 project1-cluster-m 8020' succeeded after 6 execution(s).
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + loginfo 'Service up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + echo 'Service up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: Service up on host=project1-cluster-m port=8020 name=hadoop-hdfs-namenode.
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + create_hcfs_dirs true false
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local use_webhdfs=true
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local set_permission=false
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + init_webhdfs_uri
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local protocol
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local port
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + protocol=http
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ get_property_in_xml /etc/hadoop/conf/hdfs-site.xml dfs.namenode.http-address 0.0.0.0:9870
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ cut -d: -f2
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + port=9870
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local active_host
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ 1 -gt 1 ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + active_host=project1-cluster-m
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n project1-cluster-m ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + WEBHDFS_BASE_URI=http://project1-cluster-m:9870/webhdfs/v1
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -z http://project1-cluster-m:9870/webhdfs/v1 ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a dirs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a owner_groups
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a modes
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + add_user_dirs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + system_users=(hdfs mapred yarn spark pig hive hbase zookeeper solr zeppelin)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a system_users
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a real_users
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + real_users=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ getent passwd
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + users=("${system_users[@]}" "${real_users[@]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a users
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for user in "${users[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + HCFS_DIRS+=("/user/${user} ${user}:${user} 700")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local row
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a columns
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local encoded_path
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2F
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Ftmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /tmp/hadoop-yarn/staging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp/hadoop-yarn/staging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Ftmp%2Fhadoop-yarn%2Fstaging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /tmp/hadoop-yarn/staging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/tmp/hadoop-yarn/staging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /tmp/hadoop-yarn/staging/history
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/tmp/hadoop-yarn/staging/history
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Ftmp%2Fhadoop-yarn%2Fstaging%2Fhistory
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /tmp/hadoop-yarn/staging/history
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/tmp/hadoop-yarn/staging/history
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /var
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/var
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fvar
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /var
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/var
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /var/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/var/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fvar%2Ftmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /var/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/var/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fhdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/mapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/mapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fmapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/mapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/mapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/yarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/yarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fyarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/yarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/yarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/spark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/spark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fspark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/spark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/spark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/pig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/pig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fpig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/pig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/pig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/hive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fhive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/hive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/hive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/hbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/hbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fhbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/hbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/hbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/zookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/zookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fzookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/zookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/zookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/solr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/solr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fsolr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/solr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/solr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/zeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/zeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fzeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/zeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/zeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for row in "${HCFS_DIRS[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + columns=(${row})
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encode_path_for_webhdfs /user/dataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ local encoded
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: +++ perl -MURI::Escape -s -e 'print uri_escape($query);' -- -query=/user/dataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ encoded=%2Fuser%2Fdataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: ++ echo /user/dataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + encoded_path=/user/dataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + dirs+=("${encoded_path}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + owner_groups+=("${columns[1]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + modes+=("${columns[2]}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ true == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + echo 'Creating HCFS dirs with WebHDFS ...'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: Creating HCFS dirs with WebHDFS ...
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local batch_size=20
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids=()
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a pids
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i=0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids+=("$!")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( (i+1)%20==0 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i++ ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + (( i<18 ))
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + wait_all 7954 7955 7956 7957 7958 7959 7960 7961 7962 7963 7965 7966 7967 7968 7969 7970 7971 7972
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + pids=("$@")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a pids
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + wait 7954
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/hive hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/hive
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/hive?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/hbase hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/hbase
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/hbase?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/zookeeper hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/zookeeper
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/zookeeper?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/solr hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/solr
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/solr?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/zeppelin hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/zeppelin
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/zeppelin?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/dataproc hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/hdfs hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/dataproc
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/dataproc?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/dataproc?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/dataproc?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/hdfs?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/spark hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/spark
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/spark?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/yarn hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/yarn
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/yarn?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /var/tmp hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/var/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/var/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /tmp/hadoop-yarn/staging hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/tmp/hadoop-yarn/staging
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/pig hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/pig
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/pig?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /tmp hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/tmp
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/tmp?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /user/mapred hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/user/mapred
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/user/mapred?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /var hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir /tmp/hadoop-yarn/staging/history hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/var
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/tmp/hadoop-yarn/staging/history
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/var?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/tmp/hadoop-yarn/staging/history?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs_mkdir / hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r dir=/
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r owner=hdfs
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ -n hdfs ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + webhdfs 'http://project1-cluster-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -r 'uri=http://project1-cluster-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + local -a command
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command=(curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30)
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + [[ false == \t\r\u\e ]]
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + command+=("${uri}")
<13>Feb 20 20:45:29 startup-script[1178]: <13>Feb 20 20:45:29 activate-component-hdfs[3887]: + sudo -u hdfs curl -X PUT -i -fsS --retry-connrefused --max-time 60 --retry 3 --retry-max-time 30 'http://project1-cluster-m:9870/webhdfs/v1/?op=MKDIRS&user.name=hdfs'
<13>Feb 20 20:45:30 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:30 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:30 startup-script[1178]: <13>Feb 20 20:45:30 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:31 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:31 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:31 startup-script[1178]: <13>Feb 20 20:45:31 activate-component-hive-metastore[3890]: nc: connect to project1-cluster-m port 9083 (tcp) failed: Connection refused
<13>Feb 20 20:45:32 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:32 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932381&s=GGePAGF8DuKXnh3l3OB9UX6FiMVqjF3mIMudKdGaZGc="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932381&s=GGePAGF8DuKXnh3l3OB9UX6FiMVqjF3mIMudKdGaZGc="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932381&s=GGePAGF8DuKXnh3l3OB9UX6FiMVqjF3mIMudKdGaZGc="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932381&s=GGePAGF8DuKXnh3l3OB9UX6FiMVqjF3mIMudKdGaZGc="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932382&s=uyHmN/Oe3nGLVctx3fofx+j98+XxR+g/G/XHOzSuHek="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}+ for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7955
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7956
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7957
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7958
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7959
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7960
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: HTTP/1.1 200 OK
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:30 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Cache-Control: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Expires: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Date: Tue, 20 Feb 2024 20:45:32 GMT
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Pragma: no-cache
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-Content-Type-Options: nosniff
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-FRAME-OPTIONS: SAMEORIGIN
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: X-XSS-Protection: 1; mode=block
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Set-Cookie: hadoop.auth="u=hdfs&p=hdfs&t=simple&e=1708497932380&s=pJJZi7ZdqNk8OcmRyIWnB+nRLSUhNkdy41IYl0m1rmI="; Path=/; HttpOnly
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Content-Type: application/json
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: Transfer-Encoding: chunked
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: 
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: {"boolean":true}+ for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7961
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7962
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7963
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7965
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7966
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7967
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7968
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7969
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7970
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7971
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + for pid in "${pids[@]}"
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + wait 7972
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + [[ false == \f\a\l\s\e ]]
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + retry_constant_short run_hdfs_command_as_hdfs -chmod -R 1777 /
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + retry_constant_custom 30 1 run_hdfs_command_as_hdfs -chmod -R 1777 /
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hdfs[3887]: About to run 'run_hdfs_command_as_hdfs -chmod -R 1777 /' with retries...
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: Connection to project1-cluster-m 9083 port [tcp/*] succeeded!
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: 'nc -v -z -w 1 project1-cluster-m 9083' succeeded after 45 execution(s).
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + return 0
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + loginfo 'Service up on host=project1-cluster-m port=9083 name=hive-metastore.'
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + echo 'Service up on host=project1-cluster-m port=9083 name=hive-metastore.'
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: Service up on host=project1-cluster-m port=9083 name=hive-metastore.
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: ++ date +%s.%N
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + local -r end=1708461932.992725964
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + local -r runtime_s=55
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + echo 'Component hive-metastore took 55s to activate'
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: Component hive-metastore took 55s to activate
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + local -r time_file=/tmp/dataproc/components/activate/hive-metastore.time
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + touch /tmp/dataproc/components/activate/hive-metastore.time
<13>Feb 20 20:45:32 startup-script[1178]: <13>Feb 20 20:45:32 activate-component-hive-metastore[3890]: + cat
<13>Feb 20 20:45:33 startup-script[1178]: <13>Feb 20 20:45:33 activate-component-hive-metastore[3890]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:45:33 startup-script[1178]: <13>Feb 20 20:45:33 activate-component-hive-metastore[3890]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Feb 20 20:45:33 startup-script[1178]: <13>Feb 20 20:45:33 activate-component-hive-metastore[3890]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Feb 20 20:45:33 startup-script[1178]: <13>Feb 20 20:45:33 activate-component-hive-metastore[3890]: + touch /tmp/dataproc/components/activate/hive-metastore.done
<13>Feb 20 20:45:33 startup-script[1178]: ++ echo 0
<13>Feb 20 20:45:33 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:33 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:34 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:34 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:35 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:35 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: 'run_hdfs_command_as_hdfs -chmod -R 1777 /' succeeded after 1 execution(s).
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + return
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + [[ 0 == 0 ]]
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + enable_and_start_service hadoop-hdfs-datanode
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-datanode
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + enable_service hadoop-hdfs-datanode
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-datanode
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-datanode.service
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + retry_constant_short systemctl enable hadoop-hdfs-datanode.service
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-datanode.service
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: About to run 'systemctl enable hadoop-hdfs-datanode.service' with retries...
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: hadoop-hdfs-datanode.service is not a native service, redirecting to systemd-sysv-install.
<13>Feb 20 20:45:35 startup-script[1178]: <13>Feb 20 20:45:35 activate-component-hdfs[3887]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-datanode
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: 'systemctl enable hadoop-hdfs-datanode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + mkdir -p /etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local props
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ retry_constant_short systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ local -r max_retry_time=30
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ local -r retry_delay=1
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ cmd=("${@:3}")
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ local -r cmd
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ local -r max_retries=30
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ local reenable_x=false
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ [[ -o xtrace ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: About to run 'systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit' with retries...
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: 'systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ return 0
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + props='Restart=no
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: RemainAfterExit=no'
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + [[ hadoop-hdfs-datanode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + [[ hadoop-hdfs-datanode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ get_metadata_worker_count
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: ++ set +x
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r worker_count=0
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + [[ 0 != 0 ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + ln -s -f /etc/systemd/system/common/worker-restart.conf /etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + start_service hadoop-hdfs-datanode
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r service=hadoop-hdfs-datanode
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r unit=hadoop-hdfs-datanode.service
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + retry_constant_short systemctl start hadoop-hdfs-datanode.service
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + retry_constant_custom 30 1 systemctl start hadoop-hdfs-datanode.service
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r max_retry_time=30
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r retry_delay=1
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + cmd=("${@:3}")
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r cmd
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local -r max_retries=30
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + local reenable_x=false
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + [[ -o xtrace ]]
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: + set +x
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: About to run 'systemctl start hadoop-hdfs-datanode.service' with retries...
<13>Feb 20 20:45:36 startup-script[1178]: <13>Feb 20 20:45:36 activate-component-hdfs[3887]: Warning: The unit file, source configuration file or drop-ins of hadoop-hdfs-datanode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Feb 20 20:45:36 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:36 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:37 startup-script[1178]: <13>Feb 20 20:45:37 delayed_uninstall_artifacts[3856]: + uninstall_artifacts
<13>Feb 20 20:45:37 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:37 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:38 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:38 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:39 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:39 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:40 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:40 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:41 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:41 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:42 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:42 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:43 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:43 startup-script[1178]: + sleep 1
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: 'systemctl start hadoop-hdfs-datanode.service' succeeded after 1 execution(s).
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + return 0
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: ++ date +%s.%N
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + local -r end=1708461943.712669975
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + local -r runtime_s=66
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + echo 'Component hdfs took 66s to activate'
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: Component hdfs took 66s to activate
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + local -r time_file=/tmp/dataproc/components/activate/hdfs.time
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + touch /tmp/dataproc/components/activate/hdfs.time
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + cat
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + [[ 0 -ne 0 ]]
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Feb 20 20:45:43 startup-script[1178]: <13>Feb 20 20:45:43 activate-component-hdfs[3887]: + touch /tmp/dataproc/components/activate/hdfs.done
<13>Feb 20 20:45:43 startup-script[1178]: ++ echo 0
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3887.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component hdfs] pid=3887 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3887.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component hdfs] pid=3887 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3887.exitcode /tmp/dataproc/commands/3887.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3890
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3890
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component hive-metastore'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3890 cmd=[activate_component hive-metastore]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3890 cmd=[activate_component hive-metastore]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3890 cmd=[activate_component hive-metastore]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component hive-metastore'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3890.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3890.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component hive-metastore] pid=3890 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3890.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component hive-metastore] pid=3890 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3890.exitcode /tmp/dataproc/commands/3890.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3892
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3892
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component hive-server2'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3892 cmd=[activate_component hive-server2]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3892 cmd=[activate_component hive-server2]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3892 cmd=[activate_component hive-server2]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component hive-server2'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3892.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3892.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component hive-server2] pid=3892 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3892.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component hive-server2] pid=3892 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3892.exitcode /tmp/dataproc/commands/3892.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3894
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3894
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component jupyter'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3894 cmd=[activate_component jupyter]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3894 cmd=[activate_component jupyter]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3894 cmd=[activate_component jupyter]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component jupyter'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3894.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3894.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3894.done
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component jupyter] pid=3894 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component jupyter] pid=3894 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3894.exitcode /tmp/dataproc/commands/3894.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3896
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3896
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component knox'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3896 cmd=[activate_component knox]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3896 cmd=[activate_component knox]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3896 cmd=[activate_component knox]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component knox'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3896.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3896.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3896.done
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component knox] pid=3896 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component knox] pid=3896 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3896.exitcode /tmp/dataproc/commands/3896.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3898
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3898
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component mapreduce'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3898 cmd=[activate_component mapreduce]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3898 cmd=[activate_component mapreduce]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3898 cmd=[activate_component mapreduce]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component mapreduce'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3898.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3898.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component mapreduce] pid=3898 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3898.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component mapreduce] pid=3898 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3898.exitcode /tmp/dataproc/commands/3898.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3901
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3901
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component miniconda3'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3901 cmd=[activate_component miniconda3]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3901 cmd=[activate_component miniconda3]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3901 cmd=[activate_component miniconda3]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component miniconda3'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3901.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3901.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component miniconda3] pid=3901 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3901.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component miniconda3] pid=3901 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3901.exitcode /tmp/dataproc/commands/3901.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3912
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3912
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component mysql'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3912 cmd=[activate_component mysql]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3912 cmd=[activate_component mysql]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3912 cmd=[activate_component mysql]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component mysql'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3912.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3912.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3912.done
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component mysql] pid=3912 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component mysql] pid=3912 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3912.exitcode /tmp/dataproc/commands/3912.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3913
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3913
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component npd'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3913 cmd=[activate_component npd]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3913 cmd=[activate_component npd]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3913 cmd=[activate_component npd]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component npd'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3913.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3913.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component npd] pid=3913 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3913.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component npd] pid=3913 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3913.exitcode /tmp/dataproc/commands/3913.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3914
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3914
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component pig'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3914 cmd=[activate_component pig]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3914 cmd=[activate_component pig]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3914 cmd=[activate_component pig]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component pig'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3914.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3914.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component pig] pid=3914 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3914.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component pig] pid=3914 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3914.exitcode /tmp/dataproc/commands/3914.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3915
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3915
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component proxy-agent'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3915 cmd=[activate_component proxy-agent]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3915 cmd=[activate_component proxy-agent]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3915 cmd=[activate_component proxy-agent]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component proxy-agent'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3915.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3915.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component proxy-agent] pid=3915 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3915.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component proxy-agent] pid=3915 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3915.exitcode /tmp/dataproc/commands/3915.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3924
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3924
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component spark'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3924 cmd=[activate_component spark]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3924 cmd=[activate_component spark]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3924 cmd=[activate_component spark]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component spark'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3924.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3924.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component spark] pid=3924 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3924.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component spark] pid=3924 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3924.exitcode /tmp/dataproc/commands/3924.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3925
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3925
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component tez'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3925 cmd=[activate_component tez]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3925 cmd=[activate_component tez]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3925 cmd=[activate_component tez]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component tez'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3925.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3925.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component tez] pid=3925 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3925.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component tez] pid=3925 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3925.exitcode /tmp/dataproc/commands/3925.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/3926
<13>Feb 20 20:45:44 startup-script[1178]: + pid=3926
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='activate_component yarn'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=3926 cmd=[activate_component yarn]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=3926 cmd=[activate_component yarn]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=3926 cmd=[activate_component yarn]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'activate_component yarn'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/3926.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/3926.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[activate_component yarn] pid=3926 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/3926.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[activate_component yarn] pid=3926 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/3926.exitcode /tmp/dataproc/commands/3926.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/4246
<13>Feb 20 20:45:44 startup-script[1178]: + pid=4246
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='enable_and_start_service google-osconfig-agent'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=4246 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=4246 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=4246 cmd=[enable_and_start_service google-osconfig-agent]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/4246.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/4246.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[enable_and_start_service google-osconfig-agent] pid=4246 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/4246.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[enable_and_start_service google-osconfig-agent] pid=4246 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/4246.exitcode /tmp/dataproc/commands/4246.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/4707
<13>Feb 20 20:45:44 startup-script[1178]: + pid=4707
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd='setup_service google-fluentd'
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=4707 cmd=[setup_service google-fluentd]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=4707 cmd=[setup_service google-fluentd]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=4707 cmd=[setup_service google-fluentd]
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'setup_service google-fluentd'
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/4707.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/4707.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[setup_service google-fluentd] pid=4707 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/4707.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[setup_service google-fluentd] pid=4707 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/4707.exitcode /tmp/dataproc/commands/4707.running
<13>Feb 20 20:45:44 startup-script[1178]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Feb 20 20:45:44 startup-script[1178]: + local pid
<13>Feb 20 20:45:44 startup-script[1178]: ++ basename /tmp/dataproc/commands/5535
<13>Feb 20 20:45:44 startup-script[1178]: + pid=5535
<13>Feb 20 20:45:44 startup-script[1178]: + local cmd
<13>Feb 20 20:45:44 startup-script[1178]: + cmd=backup_original_configs
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'Waiting on pid=5535 cmd=[backup_original_configs]'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Waiting on pid=5535 cmd=[backup_original_configs]'
<13>Feb 20 20:45:44 startup-script[1178]: Waiting on pid=5535 cmd=[backup_original_configs]
<13>Feb 20 20:45:44 startup-script[1178]: + echo backup_original_configs
<13>Feb 20 20:45:44 startup-script[1178]: + local exitcode_file=/tmp/dataproc/commands/5535.exitcode
<13>Feb 20 20:45:44 startup-script[1178]: + [[ ! -f /tmp/dataproc/commands/5535.exitcode ]]
<13>Feb 20 20:45:44 startup-script[1178]: + local status
<13>Feb 20 20:45:44 startup-script[1178]: + status=0
<13>Feb 20 20:45:44 startup-script[1178]: + (( status != 0 ))
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'Command cmd=[backup_original_configs] pid=5535 exited with 0'
<13>Feb 20 20:45:44 startup-script[1178]: + tee /tmp/dataproc/commands/5535.done
<13>Feb 20 20:45:44 startup-script[1178]: Command cmd=[backup_original_configs] pid=5535 exited with 0
<13>Feb 20 20:45:44 startup-script[1178]: + rm /tmp/dataproc/commands/5535.exitcode /tmp/dataproc/commands/5535.running
<13>Feb 20 20:45:44 startup-script[1178]: + is_ubuntu
<13>Feb 20 20:45:44 startup-script[1178]: ++ os_id
<13>Feb 20 20:45:44 startup-script[1178]: ++ grep '^ID=' /etc/os-release
<13>Feb 20 20:45:44 startup-script[1178]: ++ xargs
<13>Feb 20 20:45:44 startup-script[1178]: ++ cut -d= -f2
<13>Feb 20 20:45:44 startup-script[1178]: + [[ debian == \u\b\u\n\t\u ]]
<13>Feb 20 20:45:44 startup-script[1178]: + loginfo 'All done'
<13>Feb 20 20:45:44 startup-script[1178]: + echo 'All done'
<13>Feb 20 20:45:44 startup-script[1178]: All done
